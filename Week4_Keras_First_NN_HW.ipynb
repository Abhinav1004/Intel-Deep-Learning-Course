{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras to Build and Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use a neural network to predict diabetes using the Pima Diabetes Dataset.  We will start by training a Random Forest to get a performance baseline.  Then we will use the Keras package to quickly build and train a neural network and compare the performance.  We will see how different network structures affect the performance, training time, and level of overfitting (or underfitting).\n",
    "\n",
    "## UCI Pima Diabetes Dataset\n",
    "\n",
    "* UCI ML Repositiory (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)\n",
    "\n",
    "\n",
    "### Attributes: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Pima Diabetes Dataset which has 8 numerical predictors and a binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminaries\n",
    "\n",
    "from __future__ import absolute_import, division, print_function  # Python 2/3 compatibility\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Import Keras objects for Deep Learning\n",
    "\n",
    "from keras.models  import Sequential, K\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "diabetes_df = pd.read_csv(current_dir+'//diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.368</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.261</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>6</td>\n",
       "      <td>115</td>\n",
       "      <td>60</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>33.7</td>\n",
       "      <td>0.245</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "163            2      100             64             23        0  29.7   \n",
       "2              8      183             64              0        0  23.3   \n",
       "764            2      122             70             27        0  36.8   \n",
       "706           10      115              0              0        0   0.0   \n",
       "664            6      115             60             39        0  33.7   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "163                     0.368   21        0  \n",
       "2                       0.672   32        1  \n",
       "764                     0.340   27        0  \n",
       "706                     0.261   30        1  \n",
       "664                     0.245   40        1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(diabetes_df.shape)\n",
    "diabetes_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes_df.iloc[:, :-1].values\n",
    "y = diabetes_df[\"Outcome\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to Train, and Test (75%, 25%)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3489583333333333, 0.6510416666666666)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.mean(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that about 35% of the patients in this dataset have diabetes, while 65% do not.  This means we can get an accuracy of 65% without any model - just declare that no one has diabetes. We will calculate the ROC-AUC score to evaluate performance of our model, and also look at the accuracy as well to see if we improved upon the 65% accuracy.\n",
    "## Exercise: Get a baseline performance using Random Forest\n",
    "To begin, and get a baseline for classifier performance:\n",
    "1. Train a Random Forest model with 200 trees on the training data.\n",
    "2. Calculate the accuracy and roc_auc_score of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train the RF Model\n",
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.760\n",
      "roc-auc is 0.831\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\n",
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc(y_test, y_pred, model_name):\n",
    "    fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(fpr, tpr, 'k-')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n",
    "    ax.grid(True)\n",
    "    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n",
    "           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n",
    "\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_rf[:, 1], 'RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Single Hidden Layer Neural Network\n",
    "\n",
    "We will use the Sequential model to quickly build a neural network.  Our first network will be a single layer network.  We have 8 variables, so we set the input shape to 8.  Let's start by having a single hidden layer with 12 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's normalize the data\n",
    "## This aids the training of neural nets by providing numerical stability\n",
    "## Random Forest does not need this as it finds a split only, as opposed to performing matrix multiplications\n",
    "\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abhinav.jha\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Define the Model \n",
    "# Input size is 8-dimensional\n",
    "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
    "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Dense(12, input_shape=(8,), activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  This is a nice tool to view the model you have created and count the parameters\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehension question:\n",
    "Why do we have 121 parameters?  Does that make sense?\n",
    "\n",
    "\n",
    "Let's fit our model for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abhinav.jha\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "576/576 [==============================] - 0s 591us/step - loss: 0.7803 - acc: 0.4913 - val_loss: 0.8193 - val_acc: 0.4635\n",
      "Epoch 2/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.7625 - acc: 0.5208 - val_loss: 0.8035 - val_acc: 0.4740\n",
      "Epoch 3/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.7463 - acc: 0.5556 - val_loss: 0.7891 - val_acc: 0.5208\n",
      "Epoch 4/200\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.7316 - acc: 0.5781 - val_loss: 0.7759 - val_acc: 0.5469\n",
      "Epoch 5/200\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.7181 - acc: 0.6163 - val_loss: 0.7638 - val_acc: 0.5677\n",
      "Epoch 6/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.7058 - acc: 0.6372 - val_loss: 0.7527 - val_acc: 0.5625\n",
      "Epoch 7/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.6946 - acc: 0.6424 - val_loss: 0.7424 - val_acc: 0.5677\n",
      "Epoch 8/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.6843 - acc: 0.6476 - val_loss: 0.7330 - val_acc: 0.5677\n",
      "Epoch 9/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.6749 - acc: 0.6632 - val_loss: 0.7242 - val_acc: 0.5990\n",
      "Epoch 10/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.6663 - acc: 0.6771 - val_loss: 0.7161 - val_acc: 0.5990\n",
      "Epoch 11/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.6583 - acc: 0.6858 - val_loss: 0.7086 - val_acc: 0.6042\n",
      "Epoch 12/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.6508 - acc: 0.6910 - val_loss: 0.7016 - val_acc: 0.6042\n",
      "Epoch 13/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.6440 - acc: 0.6979 - val_loss: 0.6950 - val_acc: 0.6094\n",
      "Epoch 14/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.6375 - acc: 0.6997 - val_loss: 0.6888 - val_acc: 0.6146\n",
      "Epoch 15/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.6314 - acc: 0.6979 - val_loss: 0.6830 - val_acc: 0.6146\n",
      "Epoch 16/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.6258 - acc: 0.7014 - val_loss: 0.6776 - val_acc: 0.6094\n",
      "Epoch 17/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.6205 - acc: 0.7049 - val_loss: 0.6725 - val_acc: 0.6094\n",
      "Epoch 18/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.6155 - acc: 0.7031 - val_loss: 0.6677 - val_acc: 0.6198\n",
      "Epoch 19/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.6108 - acc: 0.7049 - val_loss: 0.6631 - val_acc: 0.6250\n",
      "Epoch 20/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.6063 - acc: 0.7066 - val_loss: 0.6588 - val_acc: 0.6250\n",
      "Epoch 21/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.6020 - acc: 0.7083 - val_loss: 0.6547 - val_acc: 0.6198\n",
      "Epoch 22/200\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5980 - acc: 0.7066 - val_loss: 0.6508 - val_acc: 0.6250\n",
      "Epoch 23/200\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5942 - acc: 0.7118 - val_loss: 0.6471 - val_acc: 0.6250\n",
      "Epoch 24/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5906 - acc: 0.7118 - val_loss: 0.6436 - val_acc: 0.6302\n",
      "Epoch 25/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5872 - acc: 0.7135 - val_loss: 0.6402 - val_acc: 0.6302\n",
      "Epoch 26/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5839 - acc: 0.7170 - val_loss: 0.6370 - val_acc: 0.6354\n",
      "Epoch 27/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5807 - acc: 0.7170 - val_loss: 0.6340 - val_acc: 0.6354\n",
      "Epoch 28/200\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.5778 - acc: 0.7170 - val_loss: 0.6310 - val_acc: 0.6302\n",
      "Epoch 29/200\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.5749 - acc: 0.7188 - val_loss: 0.6282 - val_acc: 0.6250\n",
      "Epoch 30/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5720 - acc: 0.7205 - val_loss: 0.6255 - val_acc: 0.6250\n",
      "Epoch 31/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5694 - acc: 0.7257 - val_loss: 0.6229 - val_acc: 0.6250\n",
      "Epoch 32/200\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5668 - acc: 0.7240 - val_loss: 0.6204 - val_acc: 0.6406\n",
      "Epoch 33/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5643 - acc: 0.7257 - val_loss: 0.6180 - val_acc: 0.6458\n",
      "Epoch 34/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5619 - acc: 0.7274 - val_loss: 0.6156 - val_acc: 0.6510\n",
      "Epoch 35/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5596 - acc: 0.7326 - val_loss: 0.6134 - val_acc: 0.6562\n",
      "Epoch 36/200\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5574 - acc: 0.7361 - val_loss: 0.6112 - val_acc: 0.6562\n",
      "Epoch 37/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5552 - acc: 0.7361 - val_loss: 0.6092 - val_acc: 0.6562\n",
      "Epoch 38/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.5531 - acc: 0.7378 - val_loss: 0.6072 - val_acc: 0.6562\n",
      "Epoch 39/200\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5511 - acc: 0.7361 - val_loss: 0.6052 - val_acc: 0.6562\n",
      "Epoch 40/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5491 - acc: 0.7361 - val_loss: 0.6034 - val_acc: 0.6562\n",
      "Epoch 41/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5473 - acc: 0.7361 - val_loss: 0.6016 - val_acc: 0.6562\n",
      "Epoch 42/200\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.5454 - acc: 0.7326 - val_loss: 0.5999 - val_acc: 0.6562\n",
      "Epoch 43/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5437 - acc: 0.7344 - val_loss: 0.5982 - val_acc: 0.6667\n",
      "Epoch 44/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5419 - acc: 0.7361 - val_loss: 0.5966 - val_acc: 0.6667\n",
      "Epoch 45/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5403 - acc: 0.7413 - val_loss: 0.5950 - val_acc: 0.6719\n",
      "Epoch 46/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5387 - acc: 0.7413 - val_loss: 0.5935 - val_acc: 0.6719\n",
      "Epoch 47/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5372 - acc: 0.7465 - val_loss: 0.5920 - val_acc: 0.6719\n",
      "Epoch 48/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5356 - acc: 0.7465 - val_loss: 0.5906 - val_acc: 0.6771\n",
      "Epoch 49/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5342 - acc: 0.7465 - val_loss: 0.5892 - val_acc: 0.6771\n",
      "Epoch 50/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5327 - acc: 0.7465 - val_loss: 0.5878 - val_acc: 0.6771\n",
      "Epoch 51/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.5313 - acc: 0.7448 - val_loss: 0.5865 - val_acc: 0.6771\n",
      "Epoch 52/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.5300 - acc: 0.7465 - val_loss: 0.5853 - val_acc: 0.6875\n",
      "Epoch 53/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.5286 - acc: 0.7465 - val_loss: 0.5840 - val_acc: 0.6875\n",
      "Epoch 54/200\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5273 - acc: 0.7465 - val_loss: 0.5828 - val_acc: 0.6927\n",
      "Epoch 55/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5261 - acc: 0.7448 - val_loss: 0.5816 - val_acc: 0.6927\n",
      "Epoch 56/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5248 - acc: 0.7448 - val_loss: 0.5805 - val_acc: 0.6979\n",
      "Epoch 57/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5236 - acc: 0.7483 - val_loss: 0.5794 - val_acc: 0.6979\n",
      "Epoch 58/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5224 - acc: 0.7483 - val_loss: 0.5783 - val_acc: 0.7031\n",
      "Epoch 59/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5212 - acc: 0.7483 - val_loss: 0.5772 - val_acc: 0.7031\n",
      "Epoch 60/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5201 - acc: 0.7500 - val_loss: 0.5762 - val_acc: 0.7031\n",
      "Epoch 61/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5190 - acc: 0.7500 - val_loss: 0.5751 - val_acc: 0.7031\n",
      "Epoch 62/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5178 - acc: 0.7500 - val_loss: 0.5741 - val_acc: 0.7031\n",
      "Epoch 63/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5167 - acc: 0.7517 - val_loss: 0.5731 - val_acc: 0.7083\n",
      "Epoch 64/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5157 - acc: 0.7517 - val_loss: 0.5721 - val_acc: 0.7083\n",
      "Epoch 65/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.5146 - acc: 0.7517 - val_loss: 0.5712 - val_acc: 0.7031\n",
      "Epoch 66/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.5136 - acc: 0.7552 - val_loss: 0.5703 - val_acc: 0.6979\n",
      "Epoch 67/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.5126 - acc: 0.7569 - val_loss: 0.5694 - val_acc: 0.6979\n",
      "Epoch 68/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5116 - acc: 0.7552 - val_loss: 0.5685 - val_acc: 0.6979\n",
      "Epoch 69/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5107 - acc: 0.7622 - val_loss: 0.5677 - val_acc: 0.6979\n",
      "Epoch 70/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5097 - acc: 0.7674 - val_loss: 0.5668 - val_acc: 0.6979\n",
      "Epoch 71/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.5088 - acc: 0.7674 - val_loss: 0.5660 - val_acc: 0.6979\n",
      "Epoch 72/200\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.5079 - acc: 0.7674 - val_loss: 0.5652 - val_acc: 0.6979\n",
      "Epoch 73/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5070 - acc: 0.7691 - val_loss: 0.5645 - val_acc: 0.6979\n",
      "Epoch 74/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5062 - acc: 0.7726 - val_loss: 0.5637 - val_acc: 0.6979\n",
      "Epoch 75/200\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5053 - acc: 0.7708 - val_loss: 0.5630 - val_acc: 0.6979\n",
      "Epoch 76/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5045 - acc: 0.7708 - val_loss: 0.5623 - val_acc: 0.6979\n",
      "Epoch 77/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.5037 - acc: 0.7708 - val_loss: 0.5616 - val_acc: 0.6979\n",
      "Epoch 78/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5029 - acc: 0.7708 - val_loss: 0.5609 - val_acc: 0.7031\n",
      "Epoch 79/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5021 - acc: 0.7691 - val_loss: 0.5602 - val_acc: 0.7083\n",
      "Epoch 80/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5014 - acc: 0.7691 - val_loss: 0.5596 - val_acc: 0.7083\n",
      "Epoch 81/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5006 - acc: 0.7656 - val_loss: 0.5589 - val_acc: 0.7083\n",
      "Epoch 82/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4999 - acc: 0.7691 - val_loss: 0.5583 - val_acc: 0.7083\n",
      "Epoch 83/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4992 - acc: 0.7691 - val_loss: 0.5577 - val_acc: 0.7083\n",
      "Epoch 84/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4985 - acc: 0.7656 - val_loss: 0.5571 - val_acc: 0.7083\n",
      "Epoch 85/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4978 - acc: 0.7674 - val_loss: 0.5565 - val_acc: 0.7083\n",
      "Epoch 86/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4971 - acc: 0.7674 - val_loss: 0.5560 - val_acc: 0.7083\n",
      "Epoch 87/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4964 - acc: 0.7674 - val_loss: 0.5554 - val_acc: 0.7083\n",
      "Epoch 88/200\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4958 - acc: 0.7656 - val_loss: 0.5549 - val_acc: 0.7083\n",
      "Epoch 89/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4952 - acc: 0.7656 - val_loss: 0.5544 - val_acc: 0.7083\n",
      "Epoch 90/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4946 - acc: 0.7674 - val_loss: 0.5539 - val_acc: 0.7135\n",
      "Epoch 91/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4939 - acc: 0.7674 - val_loss: 0.5533 - val_acc: 0.7135\n",
      "Epoch 92/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4934 - acc: 0.7674 - val_loss: 0.5528 - val_acc: 0.7188\n",
      "Epoch 93/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4928 - acc: 0.7691 - val_loss: 0.5524 - val_acc: 0.7240\n",
      "Epoch 94/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4922 - acc: 0.7691 - val_loss: 0.5519 - val_acc: 0.7240\n",
      "Epoch 95/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4916 - acc: 0.7674 - val_loss: 0.5514 - val_acc: 0.7240\n",
      "Epoch 96/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4910 - acc: 0.7674 - val_loss: 0.5510 - val_acc: 0.7240\n",
      "Epoch 97/200\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4905 - acc: 0.7708 - val_loss: 0.5505 - val_acc: 0.7240\n",
      "Epoch 98/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4899 - acc: 0.7708 - val_loss: 0.5501 - val_acc: 0.7240\n",
      "Epoch 99/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4894 - acc: 0.7691 - val_loss: 0.5496 - val_acc: 0.7240\n",
      "Epoch 100/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4889 - acc: 0.7691 - val_loss: 0.5492 - val_acc: 0.7240\n",
      "Epoch 101/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4884 - acc: 0.7691 - val_loss: 0.5488 - val_acc: 0.7240\n",
      "Epoch 102/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4879 - acc: 0.7708 - val_loss: 0.5484 - val_acc: 0.7240\n",
      "Epoch 103/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4874 - acc: 0.7691 - val_loss: 0.5480 - val_acc: 0.7188\n",
      "Epoch 104/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4869 - acc: 0.7708 - val_loss: 0.5475 - val_acc: 0.7188\n",
      "Epoch 105/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4864 - acc: 0.7691 - val_loss: 0.5471 - val_acc: 0.7188\n",
      "Epoch 106/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4859 - acc: 0.7743 - val_loss: 0.5467 - val_acc: 0.7188\n",
      "Epoch 107/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4855 - acc: 0.7726 - val_loss: 0.5464 - val_acc: 0.7188\n",
      "Epoch 108/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4850 - acc: 0.7726 - val_loss: 0.5460 - val_acc: 0.7188\n",
      "Epoch 109/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4846 - acc: 0.7743 - val_loss: 0.5456 - val_acc: 0.7188\n",
      "Epoch 110/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4841 - acc: 0.7726 - val_loss: 0.5453 - val_acc: 0.7188\n",
      "Epoch 111/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4837 - acc: 0.7726 - val_loss: 0.5449 - val_acc: 0.7188\n",
      "Epoch 112/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4832 - acc: 0.7726 - val_loss: 0.5445 - val_acc: 0.7188\n",
      "Epoch 113/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4828 - acc: 0.7743 - val_loss: 0.5442 - val_acc: 0.7135\n",
      "Epoch 114/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4824 - acc: 0.7743 - val_loss: 0.5439 - val_acc: 0.7135\n",
      "Epoch 115/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4820 - acc: 0.7726 - val_loss: 0.5435 - val_acc: 0.7135\n",
      "Epoch 116/200\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4816 - acc: 0.7708 - val_loss: 0.5432 - val_acc: 0.7135\n",
      "Epoch 117/200\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4812 - acc: 0.7708 - val_loss: 0.5429 - val_acc: 0.7135\n",
      "Epoch 118/200\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4808 - acc: 0.7726 - val_loss: 0.5426 - val_acc: 0.7188\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 33us/step - loss: 0.4804 - acc: 0.7726 - val_loss: 0.5423 - val_acc: 0.7188\n",
      "Epoch 120/200\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4800 - acc: 0.7726 - val_loss: 0.5420 - val_acc: 0.7188\n",
      "Epoch 121/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4796 - acc: 0.7726 - val_loss: 0.5417 - val_acc: 0.7188\n",
      "Epoch 122/200\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4793 - acc: 0.7726 - val_loss: 0.5414 - val_acc: 0.7188\n",
      "Epoch 123/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4789 - acc: 0.7726 - val_loss: 0.5411 - val_acc: 0.7188\n",
      "Epoch 124/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4785 - acc: 0.7726 - val_loss: 0.5408 - val_acc: 0.7188\n",
      "Epoch 125/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4781 - acc: 0.7726 - val_loss: 0.5405 - val_acc: 0.7188\n",
      "Epoch 126/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4778 - acc: 0.7743 - val_loss: 0.5403 - val_acc: 0.7188\n",
      "Epoch 127/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4775 - acc: 0.7743 - val_loss: 0.5400 - val_acc: 0.7188\n",
      "Epoch 128/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4771 - acc: 0.7743 - val_loss: 0.5397 - val_acc: 0.7188\n",
      "Epoch 129/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4768 - acc: 0.7743 - val_loss: 0.5395 - val_acc: 0.7240\n",
      "Epoch 130/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4764 - acc: 0.7760 - val_loss: 0.5392 - val_acc: 0.7240\n",
      "Epoch 131/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4761 - acc: 0.7743 - val_loss: 0.5390 - val_acc: 0.7240\n",
      "Epoch 132/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4758 - acc: 0.7743 - val_loss: 0.5387 - val_acc: 0.7240\n",
      "Epoch 133/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4755 - acc: 0.7726 - val_loss: 0.5385 - val_acc: 0.7240\n",
      "Epoch 134/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4752 - acc: 0.7708 - val_loss: 0.5382 - val_acc: 0.7240\n",
      "Epoch 135/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4749 - acc: 0.7743 - val_loss: 0.5380 - val_acc: 0.7240\n",
      "Epoch 136/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4746 - acc: 0.7743 - val_loss: 0.5378 - val_acc: 0.7240\n",
      "Epoch 137/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4743 - acc: 0.7743 - val_loss: 0.5376 - val_acc: 0.7240\n",
      "Epoch 138/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4740 - acc: 0.7760 - val_loss: 0.5374 - val_acc: 0.7240\n",
      "Epoch 139/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4737 - acc: 0.7743 - val_loss: 0.5372 - val_acc: 0.7240\n",
      "Epoch 140/200\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4734 - acc: 0.7743 - val_loss: 0.5369 - val_acc: 0.7240\n",
      "Epoch 141/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4731 - acc: 0.7743 - val_loss: 0.5367 - val_acc: 0.7240\n",
      "Epoch 142/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4728 - acc: 0.7743 - val_loss: 0.5365 - val_acc: 0.7240\n",
      "Epoch 143/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4726 - acc: 0.7743 - val_loss: 0.5363 - val_acc: 0.7240\n",
      "Epoch 144/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4723 - acc: 0.7743 - val_loss: 0.5361 - val_acc: 0.7240\n",
      "Epoch 145/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4720 - acc: 0.7743 - val_loss: 0.5359 - val_acc: 0.7188\n",
      "Epoch 146/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4717 - acc: 0.7743 - val_loss: 0.5357 - val_acc: 0.7188\n",
      "Epoch 147/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4715 - acc: 0.7743 - val_loss: 0.5355 - val_acc: 0.7188\n",
      "Epoch 148/200\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4712 - acc: 0.7743 - val_loss: 0.5353 - val_acc: 0.7188\n",
      "Epoch 149/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4709 - acc: 0.7743 - val_loss: 0.5351 - val_acc: 0.7188\n",
      "Epoch 150/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4707 - acc: 0.7743 - val_loss: 0.5349 - val_acc: 0.7188\n",
      "Epoch 151/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4704 - acc: 0.7743 - val_loss: 0.5347 - val_acc: 0.7188\n",
      "Epoch 152/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4702 - acc: 0.7743 - val_loss: 0.5345 - val_acc: 0.7188\n",
      "Epoch 153/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4699 - acc: 0.7743 - val_loss: 0.5343 - val_acc: 0.7240\n",
      "Epoch 154/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4697 - acc: 0.7778 - val_loss: 0.5341 - val_acc: 0.7240\n",
      "Epoch 155/200\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4695 - acc: 0.7778 - val_loss: 0.5339 - val_acc: 0.7188\n",
      "Epoch 156/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4692 - acc: 0.7778 - val_loss: 0.5337 - val_acc: 0.7188\n",
      "Epoch 157/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4690 - acc: 0.7778 - val_loss: 0.5335 - val_acc: 0.7188\n",
      "Epoch 158/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4687 - acc: 0.7778 - val_loss: 0.5334 - val_acc: 0.7188\n",
      "Epoch 159/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4685 - acc: 0.7778 - val_loss: 0.5332 - val_acc: 0.7188\n",
      "Epoch 160/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4683 - acc: 0.7778 - val_loss: 0.5330 - val_acc: 0.7188\n",
      "Epoch 161/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4681 - acc: 0.7778 - val_loss: 0.5328 - val_acc: 0.7188\n",
      "Epoch 162/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4679 - acc: 0.7778 - val_loss: 0.5327 - val_acc: 0.7188\n",
      "Epoch 163/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4677 - acc: 0.7778 - val_loss: 0.5325 - val_acc: 0.7188\n",
      "Epoch 164/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4674 - acc: 0.7778 - val_loss: 0.5323 - val_acc: 0.7188\n",
      "Epoch 165/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4673 - acc: 0.7778 - val_loss: 0.5321 - val_acc: 0.7240\n",
      "Epoch 166/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4671 - acc: 0.7778 - val_loss: 0.5320 - val_acc: 0.7240\n",
      "Epoch 167/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4668 - acc: 0.7778 - val_loss: 0.5318 - val_acc: 0.7240\n",
      "Epoch 168/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4666 - acc: 0.7778 - val_loss: 0.5317 - val_acc: 0.7240\n",
      "Epoch 169/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4664 - acc: 0.7778 - val_loss: 0.5315 - val_acc: 0.7240\n",
      "Epoch 170/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4663 - acc: 0.7778 - val_loss: 0.5313 - val_acc: 0.7240\n",
      "Epoch 171/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4661 - acc: 0.7778 - val_loss: 0.5312 - val_acc: 0.7240\n",
      "Epoch 172/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4658 - acc: 0.7778 - val_loss: 0.5310 - val_acc: 0.7240\n",
      "Epoch 173/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4657 - acc: 0.7778 - val_loss: 0.5309 - val_acc: 0.7240\n",
      "Epoch 174/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4655 - acc: 0.7778 - val_loss: 0.5307 - val_acc: 0.7292\n",
      "Epoch 175/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4653 - acc: 0.7778 - val_loss: 0.5306 - val_acc: 0.7292\n",
      "Epoch 176/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4651 - acc: 0.7760 - val_loss: 0.5305 - val_acc: 0.7292\n",
      "Epoch 177/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4649 - acc: 0.7760 - val_loss: 0.5303 - val_acc: 0.7292\n",
      "Epoch 178/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4647 - acc: 0.7743 - val_loss: 0.5302 - val_acc: 0.7292\n",
      "Epoch 179/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4646 - acc: 0.7743 - val_loss: 0.5301 - val_acc: 0.7292\n",
      "Epoch 180/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4644 - acc: 0.7760 - val_loss: 0.5299 - val_acc: 0.7292\n",
      "Epoch 181/200\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4642 - acc: 0.7743 - val_loss: 0.5298 - val_acc: 0.7292\n",
      "Epoch 182/200\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4641 - acc: 0.7778 - val_loss: 0.5297 - val_acc: 0.7292\n",
      "Epoch 183/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4639 - acc: 0.7778 - val_loss: 0.5295 - val_acc: 0.7292\n",
      "Epoch 184/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4637 - acc: 0.7778 - val_loss: 0.5294 - val_acc: 0.7292\n",
      "Epoch 185/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4635 - acc: 0.7795 - val_loss: 0.5293 - val_acc: 0.7292\n",
      "Epoch 186/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4634 - acc: 0.7795 - val_loss: 0.5292 - val_acc: 0.7292\n",
      "Epoch 187/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4632 - acc: 0.7795 - val_loss: 0.5291 - val_acc: 0.7292\n",
      "Epoch 188/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4631 - acc: 0.7795 - val_loss: 0.5289 - val_acc: 0.7292\n",
      "Epoch 189/200\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4629 - acc: 0.7795 - val_loss: 0.5288 - val_acc: 0.7292\n",
      "Epoch 190/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4627 - acc: 0.7795 - val_loss: 0.5287 - val_acc: 0.7292\n",
      "Epoch 191/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4626 - acc: 0.7795 - val_loss: 0.5286 - val_acc: 0.7292\n",
      "Epoch 192/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4624 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7292\n",
      "Epoch 193/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4623 - acc: 0.7778 - val_loss: 0.5284 - val_acc: 0.7292\n",
      "Epoch 194/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4622 - acc: 0.7778 - val_loss: 0.5283 - val_acc: 0.7292\n",
      "Epoch 195/200\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4620 - acc: 0.7778 - val_loss: 0.5282 - val_acc: 0.7292\n",
      "Epoch 196/200\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4619 - acc: 0.7760 - val_loss: 0.5280 - val_acc: 0.7292\n",
      "Epoch 197/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4617 - acc: 0.7778 - val_loss: 0.5279 - val_acc: 0.7292\n",
      "Epoch 198/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4616 - acc: 0.7743 - val_loss: 0.5278 - val_acc: 0.7292\n",
      "Epoch 199/200\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4614 - acc: 0.7760 - val_loss: 0.5277 - val_acc: 0.7292\n",
      "Epoch 200/200\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4613 - acc: 0.7760 - val_loss: 0.5276 - val_acc: 0.7292\n"
     ]
    }
   ],
   "source": [
    "# Fit(Train) the Model\n",
    "\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "# Roc-Auc is not available in Keras as an off the shelf metric yet, so we will skip it here.\n",
    "\n",
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n",
    "# the fit function returns the run history. \n",
    "# It is very convenient, as it contains information about the model fit, iterations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Like we did for the Random Forest, we generate two kinds of predictions\n",
    "#  One is a hard decision, the other is a probabilitistic score.\n",
    "\n",
    "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the outputs to get a feel for how keras apis work.\n",
    "y_pred_class_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53114927],\n",
       "       [0.64171726],\n",
       "       [0.2958378 ],\n",
       "       [0.30220217],\n",
       "       [0.14479771],\n",
       "       [0.62375987],\n",
       "       [0.0551585 ],\n",
       "       [0.24550319],\n",
       "       [0.8385385 ],\n",
       "       [0.23618695]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.729\n",
      "roc-auc is 0.788\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model performance and plot the roc curve\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_1, 'NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some variation in exact numbers due to randomness, but you should get results in the same ballpark as the Random Forest - between 75% and 85% accuracy, between .8 and .9 for AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `run_hist_1` object that was created, specifically its `history` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_1.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training loss and the validation loss over the different epochs and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x285e2094748>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the losses are still going down on both the training set and the validation set.  This suggests that the model might benefit from further training.  Let's train the model a little more and see what happens. Note that it will pick up from where it left off. Train for 1000 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/1000\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4611 - acc: 0.7760 - val_loss: 0.5275 - val_acc: 0.7292\n",
      "Epoch 2/1000\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4610 - acc: 0.7760 - val_loss: 0.5274 - val_acc: 0.7292\n",
      "Epoch 3/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4609 - acc: 0.7760 - val_loss: 0.5273 - val_acc: 0.7292\n",
      "Epoch 4/1000\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4608 - acc: 0.7760 - val_loss: 0.5272 - val_acc: 0.7292\n",
      "Epoch 5/1000\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4606 - acc: 0.7760 - val_loss: 0.5271 - val_acc: 0.7396\n",
      "Epoch 6/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4605 - acc: 0.7760 - val_loss: 0.5270 - val_acc: 0.7396\n",
      "Epoch 7/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4603 - acc: 0.7760 - val_loss: 0.5269 - val_acc: 0.7396\n",
      "Epoch 8/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4602 - acc: 0.7760 - val_loss: 0.5268 - val_acc: 0.7396\n",
      "Epoch 9/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4600 - acc: 0.7760 - val_loss: 0.5267 - val_acc: 0.7396\n",
      "Epoch 10/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4599 - acc: 0.7760 - val_loss: 0.5266 - val_acc: 0.7396\n",
      "Epoch 11/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4598 - acc: 0.7760 - val_loss: 0.5265 - val_acc: 0.7396\n",
      "Epoch 12/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4597 - acc: 0.7760 - val_loss: 0.5264 - val_acc: 0.7396\n",
      "Epoch 13/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4595 - acc: 0.7760 - val_loss: 0.5264 - val_acc: 0.7396\n",
      "Epoch 14/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4595 - acc: 0.7760 - val_loss: 0.5263 - val_acc: 0.7396\n",
      "Epoch 15/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4593 - acc: 0.7760 - val_loss: 0.5262 - val_acc: 0.7396\n",
      "Epoch 16/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4592 - acc: 0.7760 - val_loss: 0.5261 - val_acc: 0.7396\n",
      "Epoch 17/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4590 - acc: 0.7760 - val_loss: 0.5260 - val_acc: 0.7396\n",
      "Epoch 18/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4589 - acc: 0.7760 - val_loss: 0.5259 - val_acc: 0.7396\n",
      "Epoch 19/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4588 - acc: 0.7760 - val_loss: 0.5259 - val_acc: 0.7396\n",
      "Epoch 20/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4587 - acc: 0.7760 - val_loss: 0.5258 - val_acc: 0.7396\n",
      "Epoch 21/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4586 - acc: 0.7743 - val_loss: 0.5257 - val_acc: 0.7396\n",
      "Epoch 22/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4585 - acc: 0.7743 - val_loss: 0.5256 - val_acc: 0.7396\n",
      "Epoch 23/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4583 - acc: 0.7743 - val_loss: 0.5255 - val_acc: 0.7396\n",
      "Epoch 24/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4583 - acc: 0.7743 - val_loss: 0.5255 - val_acc: 0.7396\n",
      "Epoch 25/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4581 - acc: 0.7743 - val_loss: 0.5254 - val_acc: 0.7396\n",
      "Epoch 26/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4580 - acc: 0.7726 - val_loss: 0.5253 - val_acc: 0.7396\n",
      "Epoch 27/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4579 - acc: 0.7726 - val_loss: 0.5252 - val_acc: 0.7396\n",
      "Epoch 28/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4578 - acc: 0.7708 - val_loss: 0.5251 - val_acc: 0.7396\n",
      "Epoch 29/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4577 - acc: 0.7708 - val_loss: 0.5251 - val_acc: 0.7396\n",
      "Epoch 30/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4576 - acc: 0.7708 - val_loss: 0.5250 - val_acc: 0.7396\n",
      "Epoch 31/1000\n",
      "576/576 [==============================] - 0s 121us/step - loss: 0.4574 - acc: 0.7708 - val_loss: 0.5249 - val_acc: 0.7396\n",
      "Epoch 32/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4573 - acc: 0.7708 - val_loss: 0.5249 - val_acc: 0.7396\n",
      "Epoch 33/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4572 - acc: 0.7708 - val_loss: 0.5248 - val_acc: 0.7396\n",
      "Epoch 34/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4571 - acc: 0.7708 - val_loss: 0.5247 - val_acc: 0.7448\n",
      "Epoch 35/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4570 - acc: 0.7708 - val_loss: 0.5247 - val_acc: 0.7448\n",
      "Epoch 36/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4569 - acc: 0.7708 - val_loss: 0.5246 - val_acc: 0.7448\n",
      "Epoch 37/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4568 - acc: 0.7726 - val_loss: 0.5245 - val_acc: 0.7448\n",
      "Epoch 38/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4567 - acc: 0.7726 - val_loss: 0.5245 - val_acc: 0.7448\n",
      "Epoch 39/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4566 - acc: 0.7726 - val_loss: 0.5244 - val_acc: 0.7448\n",
      "Epoch 40/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4565 - acc: 0.7708 - val_loss: 0.5244 - val_acc: 0.7448\n",
      "Epoch 41/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4564 - acc: 0.7708 - val_loss: 0.5243 - val_acc: 0.7448\n",
      "Epoch 42/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4563 - acc: 0.7708 - val_loss: 0.5242 - val_acc: 0.7448\n",
      "Epoch 43/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4562 - acc: 0.7708 - val_loss: 0.5242 - val_acc: 0.7448\n",
      "Epoch 44/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4560 - acc: 0.7708 - val_loss: 0.5241 - val_acc: 0.7448\n",
      "Epoch 45/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4560 - acc: 0.7708 - val_loss: 0.5241 - val_acc: 0.7448\n",
      "Epoch 46/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4559 - acc: 0.7708 - val_loss: 0.5240 - val_acc: 0.7448\n",
      "Epoch 47/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4558 - acc: 0.7708 - val_loss: 0.5239 - val_acc: 0.7448\n",
      "Epoch 48/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4557 - acc: 0.7708 - val_loss: 0.5239 - val_acc: 0.7448\n",
      "Epoch 49/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4556 - acc: 0.7708 - val_loss: 0.5238 - val_acc: 0.7448\n",
      "Epoch 50/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4554 - acc: 0.7708 - val_loss: 0.5238 - val_acc: 0.7448\n",
      "Epoch 51/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4554 - acc: 0.7708 - val_loss: 0.5237 - val_acc: 0.7448\n",
      "Epoch 52/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4553 - acc: 0.7726 - val_loss: 0.5237 - val_acc: 0.7448\n",
      "Epoch 53/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4552 - acc: 0.7726 - val_loss: 0.5236 - val_acc: 0.7448\n",
      "Epoch 54/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4551 - acc: 0.7691 - val_loss: 0.5236 - val_acc: 0.7448\n",
      "Epoch 55/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4550 - acc: 0.7726 - val_loss: 0.5235 - val_acc: 0.7448\n",
      "Epoch 56/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4549 - acc: 0.7708 - val_loss: 0.5235 - val_acc: 0.7448\n",
      "Epoch 57/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4548 - acc: 0.7708 - val_loss: 0.5234 - val_acc: 0.7448\n",
      "Epoch 58/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4547 - acc: 0.7708 - val_loss: 0.5234 - val_acc: 0.7448\n",
      "Epoch 59/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4546 - acc: 0.7708 - val_loss: 0.5233 - val_acc: 0.7448\n",
      "Epoch 60/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4545 - acc: 0.7708 - val_loss: 0.5233 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4544 - acc: 0.7708 - val_loss: 0.5232 - val_acc: 0.7448\n",
      "Epoch 62/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4543 - acc: 0.7708 - val_loss: 0.5232 - val_acc: 0.7448\n",
      "Epoch 63/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4542 - acc: 0.7708 - val_loss: 0.5231 - val_acc: 0.7448\n",
      "Epoch 64/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4541 - acc: 0.7708 - val_loss: 0.5231 - val_acc: 0.7448\n",
      "Epoch 65/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4540 - acc: 0.7708 - val_loss: 0.5230 - val_acc: 0.7448\n",
      "Epoch 66/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4539 - acc: 0.7708 - val_loss: 0.5230 - val_acc: 0.7448\n",
      "Epoch 67/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4538 - acc: 0.7708 - val_loss: 0.5229 - val_acc: 0.7448\n",
      "Epoch 68/1000\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4537 - acc: 0.7708 - val_loss: 0.5229 - val_acc: 0.7448\n",
      "Epoch 69/1000\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4536 - acc: 0.7708 - val_loss: 0.5229 - val_acc: 0.7448\n",
      "Epoch 70/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4536 - acc: 0.7708 - val_loss: 0.5228 - val_acc: 0.7448\n",
      "Epoch 71/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4535 - acc: 0.7726 - val_loss: 0.5228 - val_acc: 0.7448\n",
      "Epoch 72/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4534 - acc: 0.7726 - val_loss: 0.5228 - val_acc: 0.7448\n",
      "Epoch 73/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4533 - acc: 0.7708 - val_loss: 0.5227 - val_acc: 0.7448\n",
      "Epoch 74/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4532 - acc: 0.7726 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 75/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4531 - acc: 0.7726 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 76/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4530 - acc: 0.7726 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 77/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4529 - acc: 0.7726 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 78/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4528 - acc: 0.7726 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 79/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4528 - acc: 0.7726 - val_loss: 0.5225 - val_acc: 0.7500\n",
      "Epoch 80/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4527 - acc: 0.7726 - val_loss: 0.5225 - val_acc: 0.7500\n",
      "Epoch 81/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4526 - acc: 0.7726 - val_loss: 0.5225 - val_acc: 0.7500\n",
      "Epoch 82/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4525 - acc: 0.7726 - val_loss: 0.5224 - val_acc: 0.7500\n",
      "Epoch 83/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4524 - acc: 0.7726 - val_loss: 0.5224 - val_acc: 0.7500\n",
      "Epoch 84/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4523 - acc: 0.7726 - val_loss: 0.5224 - val_acc: 0.7500\n",
      "Epoch 85/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4522 - acc: 0.7726 - val_loss: 0.5223 - val_acc: 0.7500\n",
      "Epoch 86/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4521 - acc: 0.7726 - val_loss: 0.5223 - val_acc: 0.7500\n",
      "Epoch 87/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4520 - acc: 0.7726 - val_loss: 0.5222 - val_acc: 0.7500\n",
      "Epoch 88/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4519 - acc: 0.7726 - val_loss: 0.5222 - val_acc: 0.7500\n",
      "Epoch 89/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4519 - acc: 0.7726 - val_loss: 0.5222 - val_acc: 0.7500\n",
      "Epoch 90/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4518 - acc: 0.7726 - val_loss: 0.5221 - val_acc: 0.7500\n",
      "Epoch 91/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4517 - acc: 0.7726 - val_loss: 0.5221 - val_acc: 0.7500\n",
      "Epoch 92/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4516 - acc: 0.7726 - val_loss: 0.5221 - val_acc: 0.7448\n",
      "Epoch 93/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4515 - acc: 0.7726 - val_loss: 0.5221 - val_acc: 0.7448\n",
      "Epoch 94/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4514 - acc: 0.7726 - val_loss: 0.5220 - val_acc: 0.7448\n",
      "Epoch 95/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4513 - acc: 0.7726 - val_loss: 0.5220 - val_acc: 0.7448\n",
      "Epoch 96/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4513 - acc: 0.7726 - val_loss: 0.5220 - val_acc: 0.7448\n",
      "Epoch 97/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4512 - acc: 0.7726 - val_loss: 0.5220 - val_acc: 0.7448\n",
      "Epoch 98/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4511 - acc: 0.7726 - val_loss: 0.5219 - val_acc: 0.7448\n",
      "Epoch 99/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4510 - acc: 0.7726 - val_loss: 0.5219 - val_acc: 0.7448\n",
      "Epoch 100/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4509 - acc: 0.7726 - val_loss: 0.5219 - val_acc: 0.7448\n",
      "Epoch 101/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4509 - acc: 0.7726 - val_loss: 0.5219 - val_acc: 0.7448\n",
      "Epoch 102/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4508 - acc: 0.7726 - val_loss: 0.5218 - val_acc: 0.7448\n",
      "Epoch 103/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4507 - acc: 0.7726 - val_loss: 0.5218 - val_acc: 0.7448\n",
      "Epoch 104/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4506 - acc: 0.7726 - val_loss: 0.5218 - val_acc: 0.7448\n",
      "Epoch 105/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4505 - acc: 0.7726 - val_loss: 0.5217 - val_acc: 0.7448\n",
      "Epoch 106/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4504 - acc: 0.7726 - val_loss: 0.5217 - val_acc: 0.7448\n",
      "Epoch 107/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4504 - acc: 0.7726 - val_loss: 0.5217 - val_acc: 0.7448\n",
      "Epoch 108/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4503 - acc: 0.7743 - val_loss: 0.5217 - val_acc: 0.7448\n",
      "Epoch 109/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4502 - acc: 0.7743 - val_loss: 0.5216 - val_acc: 0.7448\n",
      "Epoch 110/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4501 - acc: 0.7743 - val_loss: 0.5216 - val_acc: 0.7448\n",
      "Epoch 111/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4501 - acc: 0.7743 - val_loss: 0.5216 - val_acc: 0.7448\n",
      "Epoch 112/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4499 - acc: 0.7743 - val_loss: 0.5215 - val_acc: 0.7448\n",
      "Epoch 113/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4499 - acc: 0.7743 - val_loss: 0.5215 - val_acc: 0.7448\n",
      "Epoch 114/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4498 - acc: 0.7743 - val_loss: 0.5215 - val_acc: 0.7448\n",
      "Epoch 115/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4497 - acc: 0.7743 - val_loss: 0.5214 - val_acc: 0.7448\n",
      "Epoch 116/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4496 - acc: 0.7743 - val_loss: 0.5214 - val_acc: 0.7448\n",
      "Epoch 117/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4496 - acc: 0.7743 - val_loss: 0.5213 - val_acc: 0.7448\n",
      "Epoch 118/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4495 - acc: 0.7743 - val_loss: 0.5213 - val_acc: 0.7448\n",
      "Epoch 119/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4494 - acc: 0.7743 - val_loss: 0.5213 - val_acc: 0.7448\n",
      "Epoch 120/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4493 - acc: 0.7743 - val_loss: 0.5212 - val_acc: 0.7448\n",
      "Epoch 121/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 31us/step - loss: 0.4493 - acc: 0.7743 - val_loss: 0.5212 - val_acc: 0.7448\n",
      "Epoch 122/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4492 - acc: 0.7743 - val_loss: 0.5211 - val_acc: 0.7448\n",
      "Epoch 123/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4491 - acc: 0.7743 - val_loss: 0.5211 - val_acc: 0.7448\n",
      "Epoch 124/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4490 - acc: 0.7743 - val_loss: 0.5210 - val_acc: 0.7448\n",
      "Epoch 125/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4489 - acc: 0.7743 - val_loss: 0.5210 - val_acc: 0.7448\n",
      "Epoch 126/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4489 - acc: 0.7743 - val_loss: 0.5210 - val_acc: 0.7448\n",
      "Epoch 127/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4488 - acc: 0.7743 - val_loss: 0.5209 - val_acc: 0.7448\n",
      "Epoch 128/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4487 - acc: 0.7743 - val_loss: 0.5209 - val_acc: 0.7448\n",
      "Epoch 129/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4487 - acc: 0.7743 - val_loss: 0.5208 - val_acc: 0.7448\n",
      "Epoch 130/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4486 - acc: 0.7743 - val_loss: 0.5208 - val_acc: 0.7448\n",
      "Epoch 131/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4485 - acc: 0.7743 - val_loss: 0.5208 - val_acc: 0.7448\n",
      "Epoch 132/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4485 - acc: 0.7743 - val_loss: 0.5207 - val_acc: 0.7448\n",
      "Epoch 133/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4484 - acc: 0.7743 - val_loss: 0.5207 - val_acc: 0.7448\n",
      "Epoch 134/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4483 - acc: 0.7743 - val_loss: 0.5207 - val_acc: 0.7448\n",
      "Epoch 135/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4483 - acc: 0.7743 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 136/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4482 - acc: 0.7743 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 137/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4481 - acc: 0.7743 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 138/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4480 - acc: 0.7743 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 139/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4480 - acc: 0.7743 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 140/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4479 - acc: 0.7743 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 141/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4478 - acc: 0.7743 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 142/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4478 - acc: 0.7743 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 143/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4477 - acc: 0.7743 - val_loss: 0.5203 - val_acc: 0.7500\n",
      "Epoch 144/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4476 - acc: 0.7743 - val_loss: 0.5203 - val_acc: 0.7500\n",
      "Epoch 145/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4476 - acc: 0.7743 - val_loss: 0.5203 - val_acc: 0.7500\n",
      "Epoch 146/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4475 - acc: 0.7743 - val_loss: 0.5202 - val_acc: 0.7500\n",
      "Epoch 147/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4475 - acc: 0.7726 - val_loss: 0.5202 - val_acc: 0.7500\n",
      "Epoch 148/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4474 - acc: 0.7726 - val_loss: 0.5202 - val_acc: 0.7500\n",
      "Epoch 149/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4473 - acc: 0.7726 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 150/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4473 - acc: 0.7726 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 151/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4472 - acc: 0.7726 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 152/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4471 - acc: 0.7726 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 153/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4471 - acc: 0.7726 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 154/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4471 - acc: 0.7743 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 155/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4470 - acc: 0.7743 - val_loss: 0.5199 - val_acc: 0.7500\n",
      "Epoch 156/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4469 - acc: 0.7726 - val_loss: 0.5199 - val_acc: 0.7500\n",
      "Epoch 157/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4469 - acc: 0.7743 - val_loss: 0.5199 - val_acc: 0.7500\n",
      "Epoch 158/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4468 - acc: 0.7743 - val_loss: 0.5199 - val_acc: 0.7500\n",
      "Epoch 159/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4467 - acc: 0.7726 - val_loss: 0.5198 - val_acc: 0.7500\n",
      "Epoch 160/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4467 - acc: 0.7743 - val_loss: 0.5198 - val_acc: 0.7500\n",
      "Epoch 161/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4466 - acc: 0.7743 - val_loss: 0.5198 - val_acc: 0.7500\n",
      "Epoch 162/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4466 - acc: 0.7743 - val_loss: 0.5198 - val_acc: 0.7500\n",
      "Epoch 163/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4465 - acc: 0.7743 - val_loss: 0.5197 - val_acc: 0.7500\n",
      "Epoch 164/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4465 - acc: 0.7743 - val_loss: 0.5197 - val_acc: 0.7500\n",
      "Epoch 165/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4464 - acc: 0.7743 - val_loss: 0.5197 - val_acc: 0.7500\n",
      "Epoch 166/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4464 - acc: 0.7743 - val_loss: 0.5197 - val_acc: 0.7500\n",
      "Epoch 167/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4463 - acc: 0.7743 - val_loss: 0.5196 - val_acc: 0.7500\n",
      "Epoch 168/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4462 - acc: 0.7743 - val_loss: 0.5196 - val_acc: 0.7500\n",
      "Epoch 169/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4462 - acc: 0.7743 - val_loss: 0.5196 - val_acc: 0.7500\n",
      "Epoch 170/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4461 - acc: 0.7743 - val_loss: 0.5195 - val_acc: 0.7500\n",
      "Epoch 171/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4461 - acc: 0.7743 - val_loss: 0.5195 - val_acc: 0.7500\n",
      "Epoch 172/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4461 - acc: 0.7743 - val_loss: 0.5195 - val_acc: 0.7500\n",
      "Epoch 173/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4460 - acc: 0.7743 - val_loss: 0.5195 - val_acc: 0.7500\n",
      "Epoch 174/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4460 - acc: 0.7743 - val_loss: 0.5194 - val_acc: 0.7500\n",
      "Epoch 175/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4459 - acc: 0.7743 - val_loss: 0.5194 - val_acc: 0.7500\n",
      "Epoch 176/1000\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4458 - acc: 0.7743 - val_loss: 0.5194 - val_acc: 0.7500\n",
      "Epoch 177/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4458 - acc: 0.7743 - val_loss: 0.5194 - val_acc: 0.7500\n",
      "Epoch 178/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4457 - acc: 0.7743 - val_loss: 0.5193 - val_acc: 0.7500\n",
      "Epoch 179/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4457 - acc: 0.7743 - val_loss: 0.5193 - val_acc: 0.7500\n",
      "Epoch 180/1000\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4456 - acc: 0.7743 - val_loss: 0.5193 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4456 - acc: 0.7760 - val_loss: 0.5193 - val_acc: 0.7500\n",
      "Epoch 182/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4455 - acc: 0.7743 - val_loss: 0.5192 - val_acc: 0.7500\n",
      "Epoch 183/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4455 - acc: 0.7760 - val_loss: 0.5192 - val_acc: 0.7500\n",
      "Epoch 184/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4454 - acc: 0.7743 - val_loss: 0.5192 - val_acc: 0.7500\n",
      "Epoch 185/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4454 - acc: 0.7778 - val_loss: 0.5192 - val_acc: 0.7500\n",
      "Epoch 186/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4453 - acc: 0.7760 - val_loss: 0.5191 - val_acc: 0.7500\n",
      "Epoch 187/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4453 - acc: 0.7778 - val_loss: 0.5191 - val_acc: 0.7500\n",
      "Epoch 188/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4452 - acc: 0.7778 - val_loss: 0.5191 - val_acc: 0.7500\n",
      "Epoch 189/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4451 - acc: 0.7760 - val_loss: 0.5191 - val_acc: 0.7500\n",
      "Epoch 190/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4451 - acc: 0.7795 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 191/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4450 - acc: 0.7778 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 192/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4450 - acc: 0.7778 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 193/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4449 - acc: 0.7795 - val_loss: 0.5190 - val_acc: 0.7500\n",
      "Epoch 194/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4449 - acc: 0.7795 - val_loss: 0.5189 - val_acc: 0.7500\n",
      "Epoch 195/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4448 - acc: 0.7795 - val_loss: 0.5189 - val_acc: 0.7500\n",
      "Epoch 196/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4448 - acc: 0.7795 - val_loss: 0.5189 - val_acc: 0.7500\n",
      "Epoch 197/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4448 - acc: 0.7812 - val_loss: 0.5189 - val_acc: 0.7500\n",
      "Epoch 198/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4447 - acc: 0.7812 - val_loss: 0.5188 - val_acc: 0.7500\n",
      "Epoch 199/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4446 - acc: 0.7812 - val_loss: 0.5188 - val_acc: 0.7500\n",
      "Epoch 200/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4446 - acc: 0.7812 - val_loss: 0.5188 - val_acc: 0.7500\n",
      "Epoch 201/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4445 - acc: 0.7812 - val_loss: 0.5188 - val_acc: 0.7500\n",
      "Epoch 202/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4445 - acc: 0.7812 - val_loss: 0.5187 - val_acc: 0.7500\n",
      "Epoch 203/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4444 - acc: 0.7812 - val_loss: 0.5187 - val_acc: 0.7500\n",
      "Epoch 204/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4444 - acc: 0.7812 - val_loss: 0.5187 - val_acc: 0.7500\n",
      "Epoch 205/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4443 - acc: 0.7812 - val_loss: 0.5187 - val_acc: 0.7500\n",
      "Epoch 206/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4443 - acc: 0.7812 - val_loss: 0.5186 - val_acc: 0.7552\n",
      "Epoch 207/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4442 - acc: 0.7812 - val_loss: 0.5186 - val_acc: 0.7552\n",
      "Epoch 208/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4442 - acc: 0.7812 - val_loss: 0.5186 - val_acc: 0.7552\n",
      "Epoch 209/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4441 - acc: 0.7812 - val_loss: 0.5186 - val_acc: 0.7552\n",
      "Epoch 210/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4441 - acc: 0.7812 - val_loss: 0.5186 - val_acc: 0.7552\n",
      "Epoch 211/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4440 - acc: 0.7812 - val_loss: 0.5185 - val_acc: 0.7552\n",
      "Epoch 212/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4440 - acc: 0.7812 - val_loss: 0.5185 - val_acc: 0.7552\n",
      "Epoch 213/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4439 - acc: 0.7812 - val_loss: 0.5185 - val_acc: 0.7552\n",
      "Epoch 214/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4439 - acc: 0.7812 - val_loss: 0.5185 - val_acc: 0.7552\n",
      "Epoch 215/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4438 - acc: 0.7812 - val_loss: 0.5184 - val_acc: 0.7604\n",
      "Epoch 216/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4438 - acc: 0.7812 - val_loss: 0.5184 - val_acc: 0.7604\n",
      "Epoch 217/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4437 - acc: 0.7812 - val_loss: 0.5184 - val_acc: 0.7604\n",
      "Epoch 218/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4437 - acc: 0.7812 - val_loss: 0.5183 - val_acc: 0.7604\n",
      "Epoch 219/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4436 - acc: 0.7812 - val_loss: 0.5183 - val_acc: 0.7604\n",
      "Epoch 220/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4436 - acc: 0.7812 - val_loss: 0.5183 - val_acc: 0.7604\n",
      "Epoch 221/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4435 - acc: 0.7812 - val_loss: 0.5183 - val_acc: 0.7604\n",
      "Epoch 222/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4435 - acc: 0.7812 - val_loss: 0.5182 - val_acc: 0.7604\n",
      "Epoch 223/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4435 - acc: 0.7812 - val_loss: 0.5182 - val_acc: 0.7604\n",
      "Epoch 224/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4434 - acc: 0.7812 - val_loss: 0.5182 - val_acc: 0.7604\n",
      "Epoch 225/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4434 - acc: 0.7812 - val_loss: 0.5182 - val_acc: 0.7604\n",
      "Epoch 226/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4433 - acc: 0.7812 - val_loss: 0.5181 - val_acc: 0.7604\n",
      "Epoch 227/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4433 - acc: 0.7812 - val_loss: 0.5181 - val_acc: 0.7604\n",
      "Epoch 228/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4433 - acc: 0.7830 - val_loss: 0.5181 - val_acc: 0.7604\n",
      "Epoch 229/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4432 - acc: 0.7812 - val_loss: 0.5181 - val_acc: 0.7604\n",
      "Epoch 230/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4431 - acc: 0.7812 - val_loss: 0.5180 - val_acc: 0.7604\n",
      "Epoch 231/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4431 - acc: 0.7812 - val_loss: 0.5180 - val_acc: 0.7604\n",
      "Epoch 232/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4431 - acc: 0.7812 - val_loss: 0.5180 - val_acc: 0.7604\n",
      "Epoch 233/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4430 - acc: 0.7812 - val_loss: 0.5179 - val_acc: 0.7604\n",
      "Epoch 234/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4430 - acc: 0.7812 - val_loss: 0.5179 - val_acc: 0.7604\n",
      "Epoch 235/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4429 - acc: 0.7812 - val_loss: 0.5179 - val_acc: 0.7604\n",
      "Epoch 236/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4429 - acc: 0.7830 - val_loss: 0.5179 - val_acc: 0.7604\n",
      "Epoch 237/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4428 - acc: 0.7812 - val_loss: 0.5178 - val_acc: 0.7604\n",
      "Epoch 238/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4428 - acc: 0.7812 - val_loss: 0.5178 - val_acc: 0.7604\n",
      "Epoch 239/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4428 - acc: 0.7812 - val_loss: 0.5178 - val_acc: 0.7604\n",
      "Epoch 240/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4427 - acc: 0.7812 - val_loss: 0.5178 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4427 - acc: 0.7812 - val_loss: 0.5177 - val_acc: 0.7604\n",
      "Epoch 242/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4427 - acc: 0.7812 - val_loss: 0.5177 - val_acc: 0.7604\n",
      "Epoch 243/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4426 - acc: 0.7830 - val_loss: 0.5177 - val_acc: 0.7604\n",
      "Epoch 244/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4425 - acc: 0.7830 - val_loss: 0.5177 - val_acc: 0.7604\n",
      "Epoch 245/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4425 - acc: 0.7812 - val_loss: 0.5177 - val_acc: 0.7604\n",
      "Epoch 246/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4425 - acc: 0.7830 - val_loss: 0.5176 - val_acc: 0.7604\n",
      "Epoch 247/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4424 - acc: 0.7830 - val_loss: 0.5176 - val_acc: 0.7604\n",
      "Epoch 248/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4424 - acc: 0.7830 - val_loss: 0.5176 - val_acc: 0.7604\n",
      "Epoch 249/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4423 - acc: 0.7812 - val_loss: 0.5175 - val_acc: 0.7604\n",
      "Epoch 250/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4423 - acc: 0.7830 - val_loss: 0.5175 - val_acc: 0.7604\n",
      "Epoch 251/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4423 - acc: 0.7830 - val_loss: 0.5175 - val_acc: 0.7604\n",
      "Epoch 252/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4422 - acc: 0.7830 - val_loss: 0.5175 - val_acc: 0.7604\n",
      "Epoch 253/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4422 - acc: 0.7830 - val_loss: 0.5174 - val_acc: 0.7604\n",
      "Epoch 254/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4421 - acc: 0.7830 - val_loss: 0.5174 - val_acc: 0.7604\n",
      "Epoch 255/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4421 - acc: 0.7830 - val_loss: 0.5174 - val_acc: 0.7604\n",
      "Epoch 256/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4421 - acc: 0.7830 - val_loss: 0.5174 - val_acc: 0.7604\n",
      "Epoch 257/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4420 - acc: 0.7830 - val_loss: 0.5173 - val_acc: 0.7604\n",
      "Epoch 258/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4420 - acc: 0.7830 - val_loss: 0.5173 - val_acc: 0.7604\n",
      "Epoch 259/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4419 - acc: 0.7830 - val_loss: 0.5173 - val_acc: 0.7604\n",
      "Epoch 260/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4419 - acc: 0.7830 - val_loss: 0.5173 - val_acc: 0.7604\n",
      "Epoch 261/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4418 - acc: 0.7830 - val_loss: 0.5172 - val_acc: 0.7604\n",
      "Epoch 262/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4418 - acc: 0.7830 - val_loss: 0.5172 - val_acc: 0.7604\n",
      "Epoch 263/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4418 - acc: 0.7830 - val_loss: 0.5172 - val_acc: 0.7604\n",
      "Epoch 264/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4418 - acc: 0.7830 - val_loss: 0.5172 - val_acc: 0.7604\n",
      "Epoch 265/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4417 - acc: 0.7830 - val_loss: 0.5171 - val_acc: 0.7604\n",
      "Epoch 266/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4417 - acc: 0.7830 - val_loss: 0.5171 - val_acc: 0.7604\n",
      "Epoch 267/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4416 - acc: 0.7830 - val_loss: 0.5171 - val_acc: 0.7604\n",
      "Epoch 268/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4416 - acc: 0.7830 - val_loss: 0.5171 - val_acc: 0.7604\n",
      "Epoch 269/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4416 - acc: 0.7812 - val_loss: 0.5170 - val_acc: 0.7604\n",
      "Epoch 270/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4415 - acc: 0.7830 - val_loss: 0.5170 - val_acc: 0.7604\n",
      "Epoch 271/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4415 - acc: 0.7812 - val_loss: 0.5170 - val_acc: 0.7604\n",
      "Epoch 272/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4415 - acc: 0.7812 - val_loss: 0.5170 - val_acc: 0.7604\n",
      "Epoch 273/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4414 - acc: 0.7830 - val_loss: 0.5169 - val_acc: 0.7604\n",
      "Epoch 274/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4414 - acc: 0.7812 - val_loss: 0.5169 - val_acc: 0.7604\n",
      "Epoch 275/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4414 - acc: 0.7812 - val_loss: 0.5169 - val_acc: 0.7604\n",
      "Epoch 276/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4413 - acc: 0.7812 - val_loss: 0.5168 - val_acc: 0.7604\n",
      "Epoch 277/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4413 - acc: 0.7812 - val_loss: 0.5168 - val_acc: 0.7604\n",
      "Epoch 278/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4412 - acc: 0.7812 - val_loss: 0.5168 - val_acc: 0.7604\n",
      "Epoch 279/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4412 - acc: 0.7812 - val_loss: 0.5168 - val_acc: 0.7604\n",
      "Epoch 280/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4412 - acc: 0.7812 - val_loss: 0.5168 - val_acc: 0.7604\n",
      "Epoch 281/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4411 - acc: 0.7812 - val_loss: 0.5167 - val_acc: 0.7604\n",
      "Epoch 282/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4411 - acc: 0.7812 - val_loss: 0.5167 - val_acc: 0.7604\n",
      "Epoch 283/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4410 - acc: 0.7812 - val_loss: 0.5167 - val_acc: 0.7604\n",
      "Epoch 284/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4410 - acc: 0.7812 - val_loss: 0.5167 - val_acc: 0.7604\n",
      "Epoch 285/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4410 - acc: 0.7812 - val_loss: 0.5166 - val_acc: 0.7604\n",
      "Epoch 286/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4410 - acc: 0.7812 - val_loss: 0.5166 - val_acc: 0.7604\n",
      "Epoch 287/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4409 - acc: 0.7812 - val_loss: 0.5166 - val_acc: 0.7604\n",
      "Epoch 288/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4409 - acc: 0.7812 - val_loss: 0.5166 - val_acc: 0.7604\n",
      "Epoch 289/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4409 - acc: 0.7812 - val_loss: 0.5165 - val_acc: 0.7604\n",
      "Epoch 290/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4408 - acc: 0.7812 - val_loss: 0.5165 - val_acc: 0.7604\n",
      "Epoch 291/1000\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4408 - acc: 0.7812 - val_loss: 0.5165 - val_acc: 0.7604\n",
      "Epoch 292/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4408 - acc: 0.7812 - val_loss: 0.5165 - val_acc: 0.7604\n",
      "Epoch 293/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4408 - acc: 0.7812 - val_loss: 0.5164 - val_acc: 0.7604\n",
      "Epoch 294/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4407 - acc: 0.7812 - val_loss: 0.5164 - val_acc: 0.7604\n",
      "Epoch 295/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4407 - acc: 0.7812 - val_loss: 0.5164 - val_acc: 0.7604\n",
      "Epoch 296/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4406 - acc: 0.7812 - val_loss: 0.5164 - val_acc: 0.7604\n",
      "Epoch 297/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4406 - acc: 0.7812 - val_loss: 0.5163 - val_acc: 0.7604\n",
      "Epoch 298/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4406 - acc: 0.7812 - val_loss: 0.5163 - val_acc: 0.7604\n",
      "Epoch 299/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4405 - acc: 0.7812 - val_loss: 0.5163 - val_acc: 0.7604\n",
      "Epoch 300/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4405 - acc: 0.7812 - val_loss: 0.5163 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4405 - acc: 0.7812 - val_loss: 0.5162 - val_acc: 0.7604\n",
      "Epoch 302/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4404 - acc: 0.7812 - val_loss: 0.5162 - val_acc: 0.7604\n",
      "Epoch 303/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4404 - acc: 0.7812 - val_loss: 0.5162 - val_acc: 0.7604\n",
      "Epoch 304/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4403 - acc: 0.7812 - val_loss: 0.5161 - val_acc: 0.7604\n",
      "Epoch 305/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4403 - acc: 0.7812 - val_loss: 0.5161 - val_acc: 0.7604\n",
      "Epoch 306/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4403 - acc: 0.7812 - val_loss: 0.5161 - val_acc: 0.7604\n",
      "Epoch 307/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4402 - acc: 0.7812 - val_loss: 0.5160 - val_acc: 0.7604\n",
      "Epoch 308/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4402 - acc: 0.7812 - val_loss: 0.5160 - val_acc: 0.7604\n",
      "Epoch 309/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4402 - acc: 0.7812 - val_loss: 0.5160 - val_acc: 0.7604\n",
      "Epoch 310/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4401 - acc: 0.7812 - val_loss: 0.5160 - val_acc: 0.7604\n",
      "Epoch 311/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4401 - acc: 0.7795 - val_loss: 0.5160 - val_acc: 0.7604\n",
      "Epoch 312/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4401 - acc: 0.7812 - val_loss: 0.5159 - val_acc: 0.7604\n",
      "Epoch 313/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4400 - acc: 0.7795 - val_loss: 0.5159 - val_acc: 0.7604\n",
      "Epoch 314/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4400 - acc: 0.7812 - val_loss: 0.5159 - val_acc: 0.7604\n",
      "Epoch 315/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4400 - acc: 0.7812 - val_loss: 0.5158 - val_acc: 0.7552\n",
      "Epoch 316/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4399 - acc: 0.7795 - val_loss: 0.5158 - val_acc: 0.7552\n",
      "Epoch 317/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4399 - acc: 0.7812 - val_loss: 0.5158 - val_acc: 0.7552\n",
      "Epoch 318/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4398 - acc: 0.7795 - val_loss: 0.5158 - val_acc: 0.7552\n",
      "Epoch 319/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4398 - acc: 0.7795 - val_loss: 0.5157 - val_acc: 0.7552\n",
      "Epoch 320/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4398 - acc: 0.7795 - val_loss: 0.5157 - val_acc: 0.7552\n",
      "Epoch 321/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4398 - acc: 0.7795 - val_loss: 0.5157 - val_acc: 0.7552\n",
      "Epoch 322/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4397 - acc: 0.7795 - val_loss: 0.5157 - val_acc: 0.7552\n",
      "Epoch 323/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4397 - acc: 0.7795 - val_loss: 0.5157 - val_acc: 0.7552\n",
      "Epoch 324/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4397 - acc: 0.7795 - val_loss: 0.5156 - val_acc: 0.7552\n",
      "Epoch 325/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4396 - acc: 0.7795 - val_loss: 0.5156 - val_acc: 0.7552\n",
      "Epoch 326/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4396 - acc: 0.7795 - val_loss: 0.5156 - val_acc: 0.7552\n",
      "Epoch 327/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4396 - acc: 0.7795 - val_loss: 0.5156 - val_acc: 0.7552\n",
      "Epoch 328/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4395 - acc: 0.7795 - val_loss: 0.5155 - val_acc: 0.7552\n",
      "Epoch 329/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4395 - acc: 0.7795 - val_loss: 0.5155 - val_acc: 0.7552\n",
      "Epoch 330/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4395 - acc: 0.7795 - val_loss: 0.5155 - val_acc: 0.7552\n",
      "Epoch 331/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4394 - acc: 0.7795 - val_loss: 0.5155 - val_acc: 0.7552\n",
      "Epoch 332/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4394 - acc: 0.7795 - val_loss: 0.5155 - val_acc: 0.7552\n",
      "Epoch 333/1000\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4394 - acc: 0.7795 - val_loss: 0.5154 - val_acc: 0.7552\n",
      "Epoch 334/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4393 - acc: 0.7795 - val_loss: 0.5154 - val_acc: 0.7552\n",
      "Epoch 335/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4393 - acc: 0.7795 - val_loss: 0.5154 - val_acc: 0.7500\n",
      "Epoch 336/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4393 - acc: 0.7795 - val_loss: 0.5154 - val_acc: 0.7500\n",
      "Epoch 337/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4392 - acc: 0.7795 - val_loss: 0.5154 - val_acc: 0.7500\n",
      "Epoch 338/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4392 - acc: 0.7795 - val_loss: 0.5153 - val_acc: 0.7500\n",
      "Epoch 339/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4392 - acc: 0.7795 - val_loss: 0.5153 - val_acc: 0.7448\n",
      "Epoch 340/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4391 - acc: 0.7795 - val_loss: 0.5153 - val_acc: 0.7448\n",
      "Epoch 341/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4391 - acc: 0.7795 - val_loss: 0.5153 - val_acc: 0.7448\n",
      "Epoch 342/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4391 - acc: 0.7795 - val_loss: 0.5152 - val_acc: 0.7448\n",
      "Epoch 343/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4390 - acc: 0.7795 - val_loss: 0.5152 - val_acc: 0.7448\n",
      "Epoch 344/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4390 - acc: 0.7795 - val_loss: 0.5152 - val_acc: 0.7448\n",
      "Epoch 345/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4389 - acc: 0.7795 - val_loss: 0.5152 - val_acc: 0.7448\n",
      "Epoch 346/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4389 - acc: 0.7795 - val_loss: 0.5151 - val_acc: 0.7448\n",
      "Epoch 347/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4389 - acc: 0.7795 - val_loss: 0.5151 - val_acc: 0.7448\n",
      "Epoch 348/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4389 - acc: 0.7795 - val_loss: 0.5151 - val_acc: 0.7448\n",
      "Epoch 349/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4389 - acc: 0.7795 - val_loss: 0.5151 - val_acc: 0.7448\n",
      "Epoch 350/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4388 - acc: 0.7795 - val_loss: 0.5150 - val_acc: 0.7448\n",
      "Epoch 351/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4388 - acc: 0.7795 - val_loss: 0.5150 - val_acc: 0.7448\n",
      "Epoch 352/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4387 - acc: 0.7778 - val_loss: 0.5150 - val_acc: 0.7448\n",
      "Epoch 353/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4387 - acc: 0.7778 - val_loss: 0.5149 - val_acc: 0.7448\n",
      "Epoch 354/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4387 - acc: 0.7795 - val_loss: 0.5149 - val_acc: 0.7448\n",
      "Epoch 355/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4386 - acc: 0.7795 - val_loss: 0.5149 - val_acc: 0.7448\n",
      "Epoch 356/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4386 - acc: 0.7778 - val_loss: 0.5149 - val_acc: 0.7448\n",
      "Epoch 357/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4385 - acc: 0.7778 - val_loss: 0.5148 - val_acc: 0.7448\n",
      "Epoch 358/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4385 - acc: 0.7778 - val_loss: 0.5148 - val_acc: 0.7448\n",
      "Epoch 359/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4385 - acc: 0.7778 - val_loss: 0.5148 - val_acc: 0.7448\n",
      "Epoch 360/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4384 - acc: 0.7778 - val_loss: 0.5148 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4384 - acc: 0.7778 - val_loss: 0.5147 - val_acc: 0.7448\n",
      "Epoch 362/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4384 - acc: 0.7778 - val_loss: 0.5147 - val_acc: 0.7448\n",
      "Epoch 363/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4383 - acc: 0.7778 - val_loss: 0.5147 - val_acc: 0.7448\n",
      "Epoch 364/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4383 - acc: 0.7778 - val_loss: 0.5146 - val_acc: 0.7448\n",
      "Epoch 365/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4383 - acc: 0.7778 - val_loss: 0.5146 - val_acc: 0.7448\n",
      "Epoch 366/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4382 - acc: 0.7778 - val_loss: 0.5146 - val_acc: 0.7448\n",
      "Epoch 367/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4382 - acc: 0.7778 - val_loss: 0.5146 - val_acc: 0.7448\n",
      "Epoch 368/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4382 - acc: 0.7778 - val_loss: 0.5145 - val_acc: 0.7448\n",
      "Epoch 369/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4381 - acc: 0.7778 - val_loss: 0.5145 - val_acc: 0.7448\n",
      "Epoch 370/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4381 - acc: 0.7778 - val_loss: 0.5145 - val_acc: 0.7448\n",
      "Epoch 371/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4381 - acc: 0.7778 - val_loss: 0.5144 - val_acc: 0.7448\n",
      "Epoch 372/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4380 - acc: 0.7778 - val_loss: 0.5144 - val_acc: 0.7448\n",
      "Epoch 373/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4380 - acc: 0.7778 - val_loss: 0.5144 - val_acc: 0.7448\n",
      "Epoch 374/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4379 - acc: 0.7778 - val_loss: 0.5144 - val_acc: 0.7448\n",
      "Epoch 375/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4379 - acc: 0.7778 - val_loss: 0.5143 - val_acc: 0.7448\n",
      "Epoch 376/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4379 - acc: 0.7778 - val_loss: 0.5143 - val_acc: 0.7448\n",
      "Epoch 377/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4378 - acc: 0.7778 - val_loss: 0.5143 - val_acc: 0.7448\n",
      "Epoch 378/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4378 - acc: 0.7778 - val_loss: 0.5143 - val_acc: 0.7448\n",
      "Epoch 379/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4377 - acc: 0.7778 - val_loss: 0.5142 - val_acc: 0.7448\n",
      "Epoch 380/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4377 - acc: 0.7778 - val_loss: 0.5142 - val_acc: 0.7448\n",
      "Epoch 381/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4377 - acc: 0.7778 - val_loss: 0.5142 - val_acc: 0.7448\n",
      "Epoch 382/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4377 - acc: 0.7778 - val_loss: 0.5142 - val_acc: 0.7448\n",
      "Epoch 383/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4376 - acc: 0.7778 - val_loss: 0.5141 - val_acc: 0.7448\n",
      "Epoch 384/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4376 - acc: 0.7778 - val_loss: 0.5141 - val_acc: 0.7448\n",
      "Epoch 385/1000\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4376 - acc: 0.7778 - val_loss: 0.5141 - val_acc: 0.7448\n",
      "Epoch 386/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4375 - acc: 0.7778 - val_loss: 0.5141 - val_acc: 0.7448\n",
      "Epoch 387/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4375 - acc: 0.7778 - val_loss: 0.5141 - val_acc: 0.7448\n",
      "Epoch 388/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4375 - acc: 0.7778 - val_loss: 0.5140 - val_acc: 0.7448\n",
      "Epoch 389/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4374 - acc: 0.7778 - val_loss: 0.5140 - val_acc: 0.7448\n",
      "Epoch 390/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4374 - acc: 0.7778 - val_loss: 0.5140 - val_acc: 0.7448\n",
      "Epoch 391/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4373 - acc: 0.7778 - val_loss: 0.5140 - val_acc: 0.7448\n",
      "Epoch 392/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4373 - acc: 0.7778 - val_loss: 0.5140 - val_acc: 0.7448\n",
      "Epoch 393/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4373 - acc: 0.7778 - val_loss: 0.5139 - val_acc: 0.7448\n",
      "Epoch 394/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4372 - acc: 0.7778 - val_loss: 0.5139 - val_acc: 0.7448\n",
      "Epoch 395/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4372 - acc: 0.7778 - val_loss: 0.5139 - val_acc: 0.7448\n",
      "Epoch 396/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4372 - acc: 0.7778 - val_loss: 0.5139 - val_acc: 0.7448\n",
      "Epoch 397/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4371 - acc: 0.7778 - val_loss: 0.5139 - val_acc: 0.7448\n",
      "Epoch 398/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4371 - acc: 0.7778 - val_loss: 0.5139 - val_acc: 0.7448\n",
      "Epoch 399/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4371 - acc: 0.7778 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 400/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4371 - acc: 0.7778 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 401/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4370 - acc: 0.7778 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 402/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4370 - acc: 0.7778 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 403/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4370 - acc: 0.7778 - val_loss: 0.5138 - val_acc: 0.7500\n",
      "Epoch 404/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4369 - acc: 0.7778 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 405/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4369 - acc: 0.7778 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 406/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4368 - acc: 0.7778 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 407/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4368 - acc: 0.7778 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 408/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4368 - acc: 0.7778 - val_loss: 0.5137 - val_acc: 0.7500\n",
      "Epoch 409/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4367 - acc: 0.7778 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 410/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4367 - acc: 0.7778 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 411/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4367 - acc: 0.7778 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 412/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4367 - acc: 0.7778 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 413/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4366 - acc: 0.7778 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 414/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4366 - acc: 0.7778 - val_loss: 0.5136 - val_acc: 0.7500\n",
      "Epoch 415/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4365 - acc: 0.7778 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 416/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4365 - acc: 0.7778 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 417/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4365 - acc: 0.7778 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 418/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4364 - acc: 0.7778 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 419/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4364 - acc: 0.7778 - val_loss: 0.5135 - val_acc: 0.7500\n",
      "Epoch 420/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4364 - acc: 0.7778 - val_loss: 0.5135 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 421/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4363 - acc: 0.7778 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 422/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4363 - acc: 0.7778 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 423/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4363 - acc: 0.7778 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 424/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4363 - acc: 0.7778 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 425/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4362 - acc: 0.7778 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 426/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4362 - acc: 0.7778 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 427/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4362 - acc: 0.7778 - val_loss: 0.5133 - val_acc: 0.7500\n",
      "Epoch 428/1000\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4361 - acc: 0.7778 - val_loss: 0.5133 - val_acc: 0.7500\n",
      "Epoch 429/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4361 - acc: 0.7778 - val_loss: 0.5133 - val_acc: 0.7448\n",
      "Epoch 430/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4360 - acc: 0.7778 - val_loss: 0.5133 - val_acc: 0.7448\n",
      "Epoch 431/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4360 - acc: 0.7778 - val_loss: 0.5133 - val_acc: 0.7448\n",
      "Epoch 432/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4360 - acc: 0.7778 - val_loss: 0.5133 - val_acc: 0.7448\n",
      "Epoch 433/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4360 - acc: 0.7778 - val_loss: 0.5133 - val_acc: 0.7448\n",
      "Epoch 434/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4359 - acc: 0.7778 - val_loss: 0.5133 - val_acc: 0.7448\n",
      "Epoch 435/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4359 - acc: 0.7778 - val_loss: 0.5132 - val_acc: 0.7448\n",
      "Epoch 436/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4359 - acc: 0.7778 - val_loss: 0.5132 - val_acc: 0.7448\n",
      "Epoch 437/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4358 - acc: 0.7778 - val_loss: 0.5132 - val_acc: 0.7448\n",
      "Epoch 438/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4358 - acc: 0.7778 - val_loss: 0.5132 - val_acc: 0.7448\n",
      "Epoch 439/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4358 - acc: 0.7778 - val_loss: 0.5132 - val_acc: 0.7448\n",
      "Epoch 440/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4357 - acc: 0.7778 - val_loss: 0.5132 - val_acc: 0.7448\n",
      "Epoch 441/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4357 - acc: 0.7778 - val_loss: 0.5131 - val_acc: 0.7448\n",
      "Epoch 442/1000\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4357 - acc: 0.7778 - val_loss: 0.5131 - val_acc: 0.7448\n",
      "Epoch 443/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4356 - acc: 0.7778 - val_loss: 0.5131 - val_acc: 0.7448\n",
      "Epoch 444/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4356 - acc: 0.7778 - val_loss: 0.5131 - val_acc: 0.7448\n",
      "Epoch 445/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4356 - acc: 0.7778 - val_loss: 0.5131 - val_acc: 0.7448\n",
      "Epoch 446/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4356 - acc: 0.7778 - val_loss: 0.5131 - val_acc: 0.7448\n",
      "Epoch 447/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4355 - acc: 0.7778 - val_loss: 0.5130 - val_acc: 0.7448\n",
      "Epoch 448/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4355 - acc: 0.7778 - val_loss: 0.5130 - val_acc: 0.7448\n",
      "Epoch 449/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4354 - acc: 0.7778 - val_loss: 0.5130 - val_acc: 0.7448\n",
      "Epoch 450/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4354 - acc: 0.7778 - val_loss: 0.5130 - val_acc: 0.7448\n",
      "Epoch 451/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4354 - acc: 0.7778 - val_loss: 0.5130 - val_acc: 0.7448\n",
      "Epoch 452/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4354 - acc: 0.7778 - val_loss: 0.5130 - val_acc: 0.7448\n",
      "Epoch 453/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4353 - acc: 0.7778 - val_loss: 0.5129 - val_acc: 0.7448\n",
      "Epoch 454/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4353 - acc: 0.7778 - val_loss: 0.5129 - val_acc: 0.7448\n",
      "Epoch 455/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4352 - acc: 0.7778 - val_loss: 0.5129 - val_acc: 0.7448\n",
      "Epoch 456/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4352 - acc: 0.7778 - val_loss: 0.5129 - val_acc: 0.7448\n",
      "Epoch 457/1000\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4352 - acc: 0.7778 - val_loss: 0.5129 - val_acc: 0.7448\n",
      "Epoch 458/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4351 - acc: 0.7778 - val_loss: 0.5128 - val_acc: 0.7448\n",
      "Epoch 459/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4351 - acc: 0.7778 - val_loss: 0.5128 - val_acc: 0.7448\n",
      "Epoch 460/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4351 - acc: 0.7778 - val_loss: 0.5128 - val_acc: 0.7448\n",
      "Epoch 461/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4350 - acc: 0.7778 - val_loss: 0.5128 - val_acc: 0.7448\n",
      "Epoch 462/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4350 - acc: 0.7778 - val_loss: 0.5128 - val_acc: 0.7448\n",
      "Epoch 463/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4349 - acc: 0.7778 - val_loss: 0.5128 - val_acc: 0.7448\n",
      "Epoch 464/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4349 - acc: 0.7778 - val_loss: 0.5128 - val_acc: 0.7448\n",
      "Epoch 465/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4349 - acc: 0.7778 - val_loss: 0.5127 - val_acc: 0.7448\n",
      "Epoch 466/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4349 - acc: 0.7778 - val_loss: 0.5127 - val_acc: 0.7448\n",
      "Epoch 467/1000\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4348 - acc: 0.7778 - val_loss: 0.5127 - val_acc: 0.7448\n",
      "Epoch 468/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4348 - acc: 0.7778 - val_loss: 0.5127 - val_acc: 0.7448\n",
      "Epoch 469/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4348 - acc: 0.7778 - val_loss: 0.5127 - val_acc: 0.7448\n",
      "Epoch 470/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4347 - acc: 0.7778 - val_loss: 0.5127 - val_acc: 0.7448\n",
      "Epoch 471/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4347 - acc: 0.7778 - val_loss: 0.5127 - val_acc: 0.7448\n",
      "Epoch 472/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4347 - acc: 0.7778 - val_loss: 0.5126 - val_acc: 0.7448\n",
      "Epoch 473/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4346 - acc: 0.7778 - val_loss: 0.5126 - val_acc: 0.7448\n",
      "Epoch 474/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4346 - acc: 0.7778 - val_loss: 0.5126 - val_acc: 0.7448\n",
      "Epoch 475/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4345 - acc: 0.7778 - val_loss: 0.5126 - val_acc: 0.7448\n",
      "Epoch 476/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4345 - acc: 0.7778 - val_loss: 0.5126 - val_acc: 0.7448\n",
      "Epoch 477/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4345 - acc: 0.7778 - val_loss: 0.5126 - val_acc: 0.7448\n",
      "Epoch 478/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4345 - acc: 0.7778 - val_loss: 0.5126 - val_acc: 0.7448\n",
      "Epoch 479/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4344 - acc: 0.7778 - val_loss: 0.5126 - val_acc: 0.7448\n",
      "Epoch 480/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4344 - acc: 0.7778 - val_loss: 0.5126 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4344 - acc: 0.7778 - val_loss: 0.5125 - val_acc: 0.7448\n",
      "Epoch 482/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4343 - acc: 0.7778 - val_loss: 0.5125 - val_acc: 0.7448\n",
      "Epoch 483/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4343 - acc: 0.7778 - val_loss: 0.5125 - val_acc: 0.7448\n",
      "Epoch 484/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4343 - acc: 0.7778 - val_loss: 0.5125 - val_acc: 0.7448\n",
      "Epoch 485/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4342 - acc: 0.7778 - val_loss: 0.5125 - val_acc: 0.7448\n",
      "Epoch 486/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4342 - acc: 0.7778 - val_loss: 0.5125 - val_acc: 0.7448\n",
      "Epoch 487/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4342 - acc: 0.7778 - val_loss: 0.5125 - val_acc: 0.7448\n",
      "Epoch 488/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4341 - acc: 0.7778 - val_loss: 0.5125 - val_acc: 0.7448\n",
      "Epoch 489/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4341 - acc: 0.7778 - val_loss: 0.5125 - val_acc: 0.7448\n",
      "Epoch 490/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4340 - acc: 0.7778 - val_loss: 0.5125 - val_acc: 0.7448\n",
      "Epoch 491/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4340 - acc: 0.7778 - val_loss: 0.5125 - val_acc: 0.7448\n",
      "Epoch 492/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4340 - acc: 0.7778 - val_loss: 0.5124 - val_acc: 0.7448\n",
      "Epoch 493/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4340 - acc: 0.7778 - val_loss: 0.5124 - val_acc: 0.7448\n",
      "Epoch 494/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4340 - acc: 0.7778 - val_loss: 0.5124 - val_acc: 0.7448\n",
      "Epoch 495/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4339 - acc: 0.7778 - val_loss: 0.5124 - val_acc: 0.7448\n",
      "Epoch 496/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4339 - acc: 0.7778 - val_loss: 0.5124 - val_acc: 0.7448\n",
      "Epoch 497/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4338 - acc: 0.7778 - val_loss: 0.5124 - val_acc: 0.7448\n",
      "Epoch 498/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4338 - acc: 0.7760 - val_loss: 0.5124 - val_acc: 0.7448\n",
      "Epoch 499/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4338 - acc: 0.7760 - val_loss: 0.5124 - val_acc: 0.7448\n",
      "Epoch 500/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4337 - acc: 0.7760 - val_loss: 0.5124 - val_acc: 0.7448\n",
      "Epoch 501/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4337 - acc: 0.7760 - val_loss: 0.5124 - val_acc: 0.7448\n",
      "Epoch 502/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4337 - acc: 0.7778 - val_loss: 0.5124 - val_acc: 0.7448\n",
      "Epoch 503/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4337 - acc: 0.7760 - val_loss: 0.5123 - val_acc: 0.7448\n",
      "Epoch 504/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4337 - acc: 0.7778 - val_loss: 0.5123 - val_acc: 0.7448\n",
      "Epoch 505/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4336 - acc: 0.7760 - val_loss: 0.5123 - val_acc: 0.7448\n",
      "Epoch 506/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4336 - acc: 0.7760 - val_loss: 0.5123 - val_acc: 0.7448\n",
      "Epoch 507/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4336 - acc: 0.7760 - val_loss: 0.5123 - val_acc: 0.7448\n",
      "Epoch 508/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4335 - acc: 0.7760 - val_loss: 0.5123 - val_acc: 0.7448\n",
      "Epoch 509/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4335 - acc: 0.7760 - val_loss: 0.5123 - val_acc: 0.7448\n",
      "Epoch 510/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4335 - acc: 0.7760 - val_loss: 0.5123 - val_acc: 0.7448\n",
      "Epoch 511/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4334 - acc: 0.7760 - val_loss: 0.5123 - val_acc: 0.7448\n",
      "Epoch 512/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4334 - acc: 0.7760 - val_loss: 0.5123 - val_acc: 0.7448\n",
      "Epoch 513/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4334 - acc: 0.7760 - val_loss: 0.5123 - val_acc: 0.7448\n",
      "Epoch 514/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4334 - acc: 0.7760 - val_loss: 0.5122 - val_acc: 0.7448\n",
      "Epoch 515/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4333 - acc: 0.7760 - val_loss: 0.5122 - val_acc: 0.7448\n",
      "Epoch 516/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4333 - acc: 0.7760 - val_loss: 0.5122 - val_acc: 0.7448\n",
      "Epoch 517/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4333 - acc: 0.7760 - val_loss: 0.5122 - val_acc: 0.7448\n",
      "Epoch 518/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4332 - acc: 0.7760 - val_loss: 0.5122 - val_acc: 0.7448\n",
      "Epoch 519/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4332 - acc: 0.7760 - val_loss: 0.5122 - val_acc: 0.7448\n",
      "Epoch 520/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4332 - acc: 0.7760 - val_loss: 0.5122 - val_acc: 0.7448\n",
      "Epoch 521/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4332 - acc: 0.7760 - val_loss: 0.5122 - val_acc: 0.7448\n",
      "Epoch 522/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4331 - acc: 0.7760 - val_loss: 0.5122 - val_acc: 0.7448\n",
      "Epoch 523/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4331 - acc: 0.7760 - val_loss: 0.5122 - val_acc: 0.7448\n",
      "Epoch 524/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4331 - acc: 0.7760 - val_loss: 0.5122 - val_acc: 0.7448\n",
      "Epoch 525/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4330 - acc: 0.7760 - val_loss: 0.5122 - val_acc: 0.7448\n",
      "Epoch 526/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4330 - acc: 0.7760 - val_loss: 0.5121 - val_acc: 0.7448\n",
      "Epoch 527/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4330 - acc: 0.7760 - val_loss: 0.5121 - val_acc: 0.7448\n",
      "Epoch 528/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4330 - acc: 0.7760 - val_loss: 0.5121 - val_acc: 0.7448\n",
      "Epoch 529/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4330 - acc: 0.7760 - val_loss: 0.5121 - val_acc: 0.7448\n",
      "Epoch 530/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4329 - acc: 0.7760 - val_loss: 0.5121 - val_acc: 0.7448\n",
      "Epoch 531/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4329 - acc: 0.7760 - val_loss: 0.5121 - val_acc: 0.7448\n",
      "Epoch 532/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4329 - acc: 0.7760 - val_loss: 0.5121 - val_acc: 0.7448\n",
      "Epoch 533/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4328 - acc: 0.7760 - val_loss: 0.5121 - val_acc: 0.7500\n",
      "Epoch 534/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4328 - acc: 0.7760 - val_loss: 0.5120 - val_acc: 0.7500\n",
      "Epoch 535/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4328 - acc: 0.7760 - val_loss: 0.5120 - val_acc: 0.7500\n",
      "Epoch 536/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4328 - acc: 0.7760 - val_loss: 0.5120 - val_acc: 0.7500\n",
      "Epoch 537/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4327 - acc: 0.7760 - val_loss: 0.5120 - val_acc: 0.7500\n",
      "Epoch 538/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4327 - acc: 0.7760 - val_loss: 0.5120 - val_acc: 0.7500\n",
      "Epoch 539/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4327 - acc: 0.7760 - val_loss: 0.5120 - val_acc: 0.7500\n",
      "Epoch 540/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4326 - acc: 0.7760 - val_loss: 0.5120 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 541/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4326 - acc: 0.7760 - val_loss: 0.5120 - val_acc: 0.7500\n",
      "Epoch 542/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4326 - acc: 0.7760 - val_loss: 0.5119 - val_acc: 0.7500\n",
      "Epoch 543/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4326 - acc: 0.7760 - val_loss: 0.5119 - val_acc: 0.7500\n",
      "Epoch 544/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4325 - acc: 0.7760 - val_loss: 0.5119 - val_acc: 0.7500\n",
      "Epoch 545/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4325 - acc: 0.7760 - val_loss: 0.5119 - val_acc: 0.7500\n",
      "Epoch 546/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4324 - acc: 0.7760 - val_loss: 0.5119 - val_acc: 0.7500\n",
      "Epoch 547/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4324 - acc: 0.7760 - val_loss: 0.5119 - val_acc: 0.7500\n",
      "Epoch 548/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4324 - acc: 0.7760 - val_loss: 0.5118 - val_acc: 0.7500\n",
      "Epoch 549/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4324 - acc: 0.7760 - val_loss: 0.5118 - val_acc: 0.7500\n",
      "Epoch 550/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4323 - acc: 0.7760 - val_loss: 0.5118 - val_acc: 0.7500\n",
      "Epoch 551/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4323 - acc: 0.7760 - val_loss: 0.5118 - val_acc: 0.7500\n",
      "Epoch 552/1000\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4323 - acc: 0.7760 - val_loss: 0.5118 - val_acc: 0.7500\n",
      "Epoch 553/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4323 - acc: 0.7760 - val_loss: 0.5118 - val_acc: 0.7500\n",
      "Epoch 554/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4323 - acc: 0.7760 - val_loss: 0.5118 - val_acc: 0.7500\n",
      "Epoch 555/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4322 - acc: 0.7760 - val_loss: 0.5118 - val_acc: 0.7500\n",
      "Epoch 556/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4322 - acc: 0.7760 - val_loss: 0.5117 - val_acc: 0.7500\n",
      "Epoch 557/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4321 - acc: 0.7760 - val_loss: 0.5117 - val_acc: 0.7500\n",
      "Epoch 558/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4321 - acc: 0.7760 - val_loss: 0.5117 - val_acc: 0.7500\n",
      "Epoch 559/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4321 - acc: 0.7760 - val_loss: 0.5117 - val_acc: 0.7500\n",
      "Epoch 560/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4321 - acc: 0.7760 - val_loss: 0.5117 - val_acc: 0.7500\n",
      "Epoch 561/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4321 - acc: 0.7760 - val_loss: 0.5117 - val_acc: 0.7500\n",
      "Epoch 562/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4320 - acc: 0.7760 - val_loss: 0.5116 - val_acc: 0.7500\n",
      "Epoch 563/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4320 - acc: 0.7760 - val_loss: 0.5116 - val_acc: 0.7500\n",
      "Epoch 564/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4320 - acc: 0.7760 - val_loss: 0.5116 - val_acc: 0.7500\n",
      "Epoch 565/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4319 - acc: 0.7760 - val_loss: 0.5116 - val_acc: 0.7500\n",
      "Epoch 566/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4319 - acc: 0.7760 - val_loss: 0.5116 - val_acc: 0.7500\n",
      "Epoch 567/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4319 - acc: 0.7760 - val_loss: 0.5116 - val_acc: 0.7500\n",
      "Epoch 568/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4319 - acc: 0.7760 - val_loss: 0.5116 - val_acc: 0.7500\n",
      "Epoch 569/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4318 - acc: 0.7760 - val_loss: 0.5116 - val_acc: 0.7500\n",
      "Epoch 570/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4318 - acc: 0.7760 - val_loss: 0.5115 - val_acc: 0.7500\n",
      "Epoch 571/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4318 - acc: 0.7760 - val_loss: 0.5115 - val_acc: 0.7500\n",
      "Epoch 572/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4317 - acc: 0.7760 - val_loss: 0.5115 - val_acc: 0.7500\n",
      "Epoch 573/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4317 - acc: 0.7760 - val_loss: 0.5115 - val_acc: 0.7500\n",
      "Epoch 574/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4317 - acc: 0.7760 - val_loss: 0.5115 - val_acc: 0.7500\n",
      "Epoch 575/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4317 - acc: 0.7760 - val_loss: 0.5115 - val_acc: 0.7500\n",
      "Epoch 576/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4316 - acc: 0.7760 - val_loss: 0.5115 - val_acc: 0.7500\n",
      "Epoch 577/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4316 - acc: 0.7760 - val_loss: 0.5114 - val_acc: 0.7500\n",
      "Epoch 578/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4316 - acc: 0.7760 - val_loss: 0.5114 - val_acc: 0.7500\n",
      "Epoch 579/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4316 - acc: 0.7760 - val_loss: 0.5114 - val_acc: 0.7500\n",
      "Epoch 580/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4315 - acc: 0.7760 - val_loss: 0.5114 - val_acc: 0.7500\n",
      "Epoch 581/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4315 - acc: 0.7760 - val_loss: 0.5114 - val_acc: 0.7500\n",
      "Epoch 582/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4315 - acc: 0.7760 - val_loss: 0.5114 - val_acc: 0.7500\n",
      "Epoch 583/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4314 - acc: 0.7760 - val_loss: 0.5114 - val_acc: 0.7500\n",
      "Epoch 584/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4314 - acc: 0.7760 - val_loss: 0.5113 - val_acc: 0.7500\n",
      "Epoch 585/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4314 - acc: 0.7760 - val_loss: 0.5113 - val_acc: 0.7500\n",
      "Epoch 586/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4314 - acc: 0.7760 - val_loss: 0.5113 - val_acc: 0.7500\n",
      "Epoch 587/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4314 - acc: 0.7760 - val_loss: 0.5113 - val_acc: 0.7500\n",
      "Epoch 588/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4313 - acc: 0.7760 - val_loss: 0.5113 - val_acc: 0.7500\n",
      "Epoch 589/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4313 - acc: 0.7760 - val_loss: 0.5113 - val_acc: 0.7500\n",
      "Epoch 590/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4313 - acc: 0.7760 - val_loss: 0.5113 - val_acc: 0.7500\n",
      "Epoch 591/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4312 - acc: 0.7760 - val_loss: 0.5113 - val_acc: 0.7500\n",
      "Epoch 592/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4312 - acc: 0.7760 - val_loss: 0.5113 - val_acc: 0.7500\n",
      "Epoch 593/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4312 - acc: 0.7760 - val_loss: 0.5113 - val_acc: 0.7500\n",
      "Epoch 594/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4311 - acc: 0.7760 - val_loss: 0.5113 - val_acc: 0.7500\n",
      "Epoch 595/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4312 - acc: 0.7760 - val_loss: 0.5113 - val_acc: 0.7500\n",
      "Epoch 596/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4311 - acc: 0.7760 - val_loss: 0.5112 - val_acc: 0.7500\n",
      "Epoch 597/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4311 - acc: 0.7760 - val_loss: 0.5112 - val_acc: 0.7500\n",
      "Epoch 598/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4310 - acc: 0.7760 - val_loss: 0.5112 - val_acc: 0.7500\n",
      "Epoch 599/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4310 - acc: 0.7760 - val_loss: 0.5112 - val_acc: 0.7500\n",
      "Epoch 600/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4310 - acc: 0.7760 - val_loss: 0.5112 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 601/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4310 - acc: 0.7760 - val_loss: 0.5112 - val_acc: 0.7500\n",
      "Epoch 602/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4310 - acc: 0.7760 - val_loss: 0.5112 - val_acc: 0.7500\n",
      "Epoch 603/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4309 - acc: 0.7760 - val_loss: 0.5112 - val_acc: 0.7500\n",
      "Epoch 604/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4309 - acc: 0.7760 - val_loss: 0.5111 - val_acc: 0.7500\n",
      "Epoch 605/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4309 - acc: 0.7760 - val_loss: 0.5111 - val_acc: 0.7500\n",
      "Epoch 606/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4308 - acc: 0.7760 - val_loss: 0.5111 - val_acc: 0.7500\n",
      "Epoch 607/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4308 - acc: 0.7760 - val_loss: 0.5111 - val_acc: 0.7500\n",
      "Epoch 608/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4308 - acc: 0.7760 - val_loss: 0.5111 - val_acc: 0.7500\n",
      "Epoch 609/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4308 - acc: 0.7760 - val_loss: 0.5111 - val_acc: 0.7500\n",
      "Epoch 610/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4308 - acc: 0.7760 - val_loss: 0.5111 - val_acc: 0.7500\n",
      "Epoch 611/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4307 - acc: 0.7760 - val_loss: 0.5111 - val_acc: 0.7500\n",
      "Epoch 612/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4308 - acc: 0.7760 - val_loss: 0.5110 - val_acc: 0.7500\n",
      "Epoch 613/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4307 - acc: 0.7760 - val_loss: 0.5110 - val_acc: 0.7500\n",
      "Epoch 614/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4307 - acc: 0.7760 - val_loss: 0.5110 - val_acc: 0.7500\n",
      "Epoch 615/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4306 - acc: 0.7760 - val_loss: 0.5110 - val_acc: 0.7448\n",
      "Epoch 616/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4306 - acc: 0.7743 - val_loss: 0.5110 - val_acc: 0.7448\n",
      "Epoch 617/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4306 - acc: 0.7760 - val_loss: 0.5110 - val_acc: 0.7448\n",
      "Epoch 618/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4306 - acc: 0.7743 - val_loss: 0.5110 - val_acc: 0.7448\n",
      "Epoch 619/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4305 - acc: 0.7760 - val_loss: 0.5110 - val_acc: 0.7448\n",
      "Epoch 620/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4305 - acc: 0.7760 - val_loss: 0.5110 - val_acc: 0.7448\n",
      "Epoch 621/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4305 - acc: 0.7760 - val_loss: 0.5110 - val_acc: 0.7448\n",
      "Epoch 622/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4305 - acc: 0.7760 - val_loss: 0.5110 - val_acc: 0.7448\n",
      "Epoch 623/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4304 - acc: 0.7760 - val_loss: 0.5110 - val_acc: 0.7448\n",
      "Epoch 624/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4304 - acc: 0.7760 - val_loss: 0.5110 - val_acc: 0.7448\n",
      "Epoch 625/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4304 - acc: 0.7743 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 626/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4304 - acc: 0.7760 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 627/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4304 - acc: 0.7760 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 628/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4303 - acc: 0.7760 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 629/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4303 - acc: 0.7760 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 630/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4303 - acc: 0.7760 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 631/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4303 - acc: 0.7743 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 632/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4302 - acc: 0.7760 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 633/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4302 - acc: 0.7760 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 634/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4302 - acc: 0.7760 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 635/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4302 - acc: 0.7760 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 636/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4301 - acc: 0.7760 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 637/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4301 - acc: 0.7743 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 638/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4301 - acc: 0.7778 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 639/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4301 - acc: 0.7760 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 640/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4301 - acc: 0.7778 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 641/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4300 - acc: 0.7760 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 642/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4300 - acc: 0.7778 - val_loss: 0.5109 - val_acc: 0.7448\n",
      "Epoch 643/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4300 - acc: 0.7760 - val_loss: 0.5108 - val_acc: 0.7448\n",
      "Epoch 644/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4299 - acc: 0.7778 - val_loss: 0.5108 - val_acc: 0.7448\n",
      "Epoch 645/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4299 - acc: 0.7760 - val_loss: 0.5108 - val_acc: 0.7448\n",
      "Epoch 646/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4299 - acc: 0.7760 - val_loss: 0.5108 - val_acc: 0.7448\n",
      "Epoch 647/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4299 - acc: 0.7778 - val_loss: 0.5108 - val_acc: 0.7448\n",
      "Epoch 648/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4299 - acc: 0.7760 - val_loss: 0.5108 - val_acc: 0.7448\n",
      "Epoch 649/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4298 - acc: 0.7778 - val_loss: 0.5108 - val_acc: 0.7448\n",
      "Epoch 650/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4298 - acc: 0.7778 - val_loss: 0.5108 - val_acc: 0.7448\n",
      "Epoch 651/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4298 - acc: 0.7760 - val_loss: 0.5108 - val_acc: 0.7448\n",
      "Epoch 652/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4297 - acc: 0.7778 - val_loss: 0.5108 - val_acc: 0.7448\n",
      "Epoch 653/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4297 - acc: 0.7778 - val_loss: 0.5108 - val_acc: 0.7448\n",
      "Epoch 654/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4297 - acc: 0.7778 - val_loss: 0.5108 - val_acc: 0.7448\n",
      "Epoch 655/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4297 - acc: 0.7778 - val_loss: 0.5108 - val_acc: 0.7448\n",
      "Epoch 656/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4296 - acc: 0.7778 - val_loss: 0.5108 - val_acc: 0.7448\n",
      "Epoch 657/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4297 - acc: 0.7778 - val_loss: 0.5108 - val_acc: 0.7448\n",
      "Epoch 658/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4296 - acc: 0.7778 - val_loss: 0.5107 - val_acc: 0.7448\n",
      "Epoch 659/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4296 - acc: 0.7795 - val_loss: 0.5107 - val_acc: 0.7448\n",
      "Epoch 660/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4296 - acc: 0.7778 - val_loss: 0.5107 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 661/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4295 - acc: 0.7778 - val_loss: 0.5107 - val_acc: 0.7448\n",
      "Epoch 662/1000\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4295 - acc: 0.7778 - val_loss: 0.5107 - val_acc: 0.7500\n",
      "Epoch 663/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4295 - acc: 0.7795 - val_loss: 0.5107 - val_acc: 0.7500\n",
      "Epoch 664/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4295 - acc: 0.7778 - val_loss: 0.5107 - val_acc: 0.7500\n",
      "Epoch 665/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4295 - acc: 0.7795 - val_loss: 0.5107 - val_acc: 0.7500\n",
      "Epoch 666/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4294 - acc: 0.7778 - val_loss: 0.5107 - val_acc: 0.7500\n",
      "Epoch 667/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4294 - acc: 0.7795 - val_loss: 0.5107 - val_acc: 0.7500\n",
      "Epoch 668/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4223 - acc: 0.718 - 0s 29us/step - loss: 0.4294 - acc: 0.7795 - val_loss: 0.5107 - val_acc: 0.7500\n",
      "Epoch 669/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4293 - acc: 0.7795 - val_loss: 0.5107 - val_acc: 0.7552\n",
      "Epoch 670/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4293 - acc: 0.7795 - val_loss: 0.5106 - val_acc: 0.7552\n",
      "Epoch 671/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4293 - acc: 0.7795 - val_loss: 0.5106 - val_acc: 0.7552\n",
      "Epoch 672/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4293 - acc: 0.7795 - val_loss: 0.5106 - val_acc: 0.7552\n",
      "Epoch 673/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4293 - acc: 0.7795 - val_loss: 0.5106 - val_acc: 0.7552\n",
      "Epoch 674/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4292 - acc: 0.7795 - val_loss: 0.5106 - val_acc: 0.7552\n",
      "Epoch 675/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4292 - acc: 0.7795 - val_loss: 0.5106 - val_acc: 0.7552\n",
      "Epoch 676/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4292 - acc: 0.7795 - val_loss: 0.5106 - val_acc: 0.7552\n",
      "Epoch 677/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4292 - acc: 0.7795 - val_loss: 0.5106 - val_acc: 0.7552\n",
      "Epoch 678/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4291 - acc: 0.7795 - val_loss: 0.5105 - val_acc: 0.7552\n",
      "Epoch 679/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4291 - acc: 0.7795 - val_loss: 0.5105 - val_acc: 0.7552\n",
      "Epoch 680/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4291 - acc: 0.7795 - val_loss: 0.5105 - val_acc: 0.7552\n",
      "Epoch 681/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4291 - acc: 0.7795 - val_loss: 0.5105 - val_acc: 0.7552\n",
      "Epoch 682/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4290 - acc: 0.7795 - val_loss: 0.5105 - val_acc: 0.7552\n",
      "Epoch 683/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4290 - acc: 0.7795 - val_loss: 0.5105 - val_acc: 0.7552\n",
      "Epoch 684/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4290 - acc: 0.7795 - val_loss: 0.5105 - val_acc: 0.7604\n",
      "Epoch 685/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4290 - acc: 0.7795 - val_loss: 0.5105 - val_acc: 0.7604\n",
      "Epoch 686/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4289 - acc: 0.7795 - val_loss: 0.5104 - val_acc: 0.7604\n",
      "Epoch 687/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4289 - acc: 0.7795 - val_loss: 0.5104 - val_acc: 0.7604\n",
      "Epoch 688/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4289 - acc: 0.7795 - val_loss: 0.5104 - val_acc: 0.7604\n",
      "Epoch 689/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4289 - acc: 0.7795 - val_loss: 0.5104 - val_acc: 0.7604\n",
      "Epoch 690/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4288 - acc: 0.7795 - val_loss: 0.5104 - val_acc: 0.7604\n",
      "Epoch 691/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4288 - acc: 0.7795 - val_loss: 0.5104 - val_acc: 0.7604\n",
      "Epoch 692/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4288 - acc: 0.7795 - val_loss: 0.5104 - val_acc: 0.7604\n",
      "Epoch 693/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4288 - acc: 0.7795 - val_loss: 0.5104 - val_acc: 0.7604\n",
      "Epoch 694/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4287 - acc: 0.7795 - val_loss: 0.5103 - val_acc: 0.7604\n",
      "Epoch 695/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4287 - acc: 0.7795 - val_loss: 0.5103 - val_acc: 0.7604\n",
      "Epoch 696/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4287 - acc: 0.7795 - val_loss: 0.5103 - val_acc: 0.7708\n",
      "Epoch 697/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4287 - acc: 0.7795 - val_loss: 0.5103 - val_acc: 0.7708\n",
      "Epoch 698/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4287 - acc: 0.7795 - val_loss: 0.5103 - val_acc: 0.7708\n",
      "Epoch 699/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4286 - acc: 0.7795 - val_loss: 0.5103 - val_acc: 0.7708\n",
      "Epoch 700/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4286 - acc: 0.7795 - val_loss: 0.5103 - val_acc: 0.7708\n",
      "Epoch 701/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4286 - acc: 0.7795 - val_loss: 0.5103 - val_acc: 0.7708\n",
      "Epoch 702/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4285 - acc: 0.7795 - val_loss: 0.5103 - val_acc: 0.7760\n",
      "Epoch 703/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4285 - acc: 0.7795 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 704/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4285 - acc: 0.7795 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 705/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4285 - acc: 0.7795 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 706/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4284 - acc: 0.7795 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 707/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4284 - acc: 0.7795 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 708/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4284 - acc: 0.7795 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 709/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4284 - acc: 0.7812 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 710/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4283 - acc: 0.7795 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 711/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4283 - acc: 0.7795 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 712/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4283 - acc: 0.7795 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 713/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4283 - acc: 0.7812 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 714/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4282 - acc: 0.7812 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 715/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4283 - acc: 0.7830 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 716/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4282 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 717/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4282 - acc: 0.7812 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 718/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4281 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 719/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4281 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 720/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4281 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 721/1000\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4281 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 722/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4281 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 723/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4281 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 724/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4280 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 725/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4280 - acc: 0.7812 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 726/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4280 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 727/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4279 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 728/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4279 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 729/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4279 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 730/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4279 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 731/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4279 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 732/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4278 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 733/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4278 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 734/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4278 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 735/1000\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4278 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 736/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4278 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 737/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4277 - acc: 0.7830 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 738/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4277 - acc: 0.7830 - val_loss: 0.5100 - val_acc: 0.7760\n",
      "Epoch 739/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4277 - acc: 0.7830 - val_loss: 0.5100 - val_acc: 0.7760\n",
      "Epoch 740/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4277 - acc: 0.7830 - val_loss: 0.5100 - val_acc: 0.7760\n",
      "Epoch 741/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4276 - acc: 0.7847 - val_loss: 0.5100 - val_acc: 0.7760\n",
      "Epoch 742/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4276 - acc: 0.7830 - val_loss: 0.5100 - val_acc: 0.7760\n",
      "Epoch 743/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4276 - acc: 0.7847 - val_loss: 0.5100 - val_acc: 0.7760\n",
      "Epoch 744/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4275 - acc: 0.7830 - val_loss: 0.5100 - val_acc: 0.7760\n",
      "Epoch 745/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4275 - acc: 0.7830 - val_loss: 0.5100 - val_acc: 0.7812\n",
      "Epoch 746/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4275 - acc: 0.7830 - val_loss: 0.5100 - val_acc: 0.7812\n",
      "Epoch 747/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4275 - acc: 0.7847 - val_loss: 0.5100 - val_acc: 0.7812\n",
      "Epoch 748/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4275 - acc: 0.7830 - val_loss: 0.5100 - val_acc: 0.7812\n",
      "Epoch 749/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4274 - acc: 0.7847 - val_loss: 0.5100 - val_acc: 0.7812\n",
      "Epoch 750/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4274 - acc: 0.7830 - val_loss: 0.5100 - val_acc: 0.7812\n",
      "Epoch 751/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4274 - acc: 0.7830 - val_loss: 0.5100 - val_acc: 0.7812\n",
      "Epoch 752/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4274 - acc: 0.7847 - val_loss: 0.5100 - val_acc: 0.7812\n",
      "Epoch 753/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4273 - acc: 0.7830 - val_loss: 0.5099 - val_acc: 0.7812\n",
      "Epoch 754/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4273 - acc: 0.7847 - val_loss: 0.5099 - val_acc: 0.7812\n",
      "Epoch 755/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4273 - acc: 0.7830 - val_loss: 0.5099 - val_acc: 0.7812\n",
      "Epoch 756/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4272 - acc: 0.7830 - val_loss: 0.5099 - val_acc: 0.7812\n",
      "Epoch 757/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4272 - acc: 0.7847 - val_loss: 0.5099 - val_acc: 0.7812\n",
      "Epoch 758/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4272 - acc: 0.7847 - val_loss: 0.5099 - val_acc: 0.7812\n",
      "Epoch 759/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4272 - acc: 0.7847 - val_loss: 0.5099 - val_acc: 0.7812\n",
      "Epoch 760/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4271 - acc: 0.7847 - val_loss: 0.5099 - val_acc: 0.7812\n",
      "Epoch 761/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4271 - acc: 0.7847 - val_loss: 0.5099 - val_acc: 0.7812\n",
      "Epoch 762/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4271 - acc: 0.7847 - val_loss: 0.5099 - val_acc: 0.7812\n",
      "Epoch 763/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4271 - acc: 0.7847 - val_loss: 0.5099 - val_acc: 0.7812\n",
      "Epoch 764/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4270 - acc: 0.7865 - val_loss: 0.5099 - val_acc: 0.7812\n",
      "Epoch 765/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4270 - acc: 0.7830 - val_loss: 0.5099 - val_acc: 0.7812\n",
      "Epoch 766/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4270 - acc: 0.7865 - val_loss: 0.5098 - val_acc: 0.7812\n",
      "Epoch 767/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4270 - acc: 0.7865 - val_loss: 0.5098 - val_acc: 0.7812\n",
      "Epoch 768/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4269 - acc: 0.7847 - val_loss: 0.5098 - val_acc: 0.7812\n",
      "Epoch 769/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4269 - acc: 0.7865 - val_loss: 0.5098 - val_acc: 0.7812\n",
      "Epoch 770/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4269 - acc: 0.7865 - val_loss: 0.5098 - val_acc: 0.7812\n",
      "Epoch 771/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4269 - acc: 0.7865 - val_loss: 0.5098 - val_acc: 0.7812\n",
      "Epoch 772/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4268 - acc: 0.7865 - val_loss: 0.5098 - val_acc: 0.7812\n",
      "Epoch 773/1000\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4268 - acc: 0.7865 - val_loss: 0.5098 - val_acc: 0.7812\n",
      "Epoch 774/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4268 - acc: 0.7847 - val_loss: 0.5098 - val_acc: 0.7812\n",
      "Epoch 775/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4268 - acc: 0.7847 - val_loss: 0.5098 - val_acc: 0.7812\n",
      "Epoch 776/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4268 - acc: 0.7865 - val_loss: 0.5098 - val_acc: 0.7812\n",
      "Epoch 777/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4267 - acc: 0.7847 - val_loss: 0.5098 - val_acc: 0.7812\n",
      "Epoch 778/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4267 - acc: 0.7847 - val_loss: 0.5098 - val_acc: 0.7812\n",
      "Epoch 779/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4267 - acc: 0.7865 - val_loss: 0.5098 - val_acc: 0.7812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 780/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4267 - acc: 0.7882 - val_loss: 0.5098 - val_acc: 0.7812\n",
      "Epoch 781/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4266 - acc: 0.7882 - val_loss: 0.5098 - val_acc: 0.7812\n",
      "Epoch 782/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4266 - acc: 0.7882 - val_loss: 0.5098 - val_acc: 0.7812\n",
      "Epoch 783/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4266 - acc: 0.7865 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 784/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4266 - acc: 0.7882 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 785/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4265 - acc: 0.7865 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 786/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4265 - acc: 0.7882 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 787/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4265 - acc: 0.7865 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 788/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4265 - acc: 0.7882 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 789/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4265 - acc: 0.7865 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 790/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4264 - acc: 0.7865 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 791/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4264 - acc: 0.7865 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 792/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4264 - acc: 0.7882 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 793/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4264 - acc: 0.7882 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 794/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4263 - acc: 0.7865 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 795/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4263 - acc: 0.7865 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 796/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4263 - acc: 0.7865 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 797/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4263 - acc: 0.7865 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 798/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4263 - acc: 0.7882 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 799/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4263 - acc: 0.7865 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 800/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4262 - acc: 0.7865 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 801/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4262 - acc: 0.7882 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 802/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4262 - acc: 0.7865 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 803/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4261 - acc: 0.7865 - val_loss: 0.5097 - val_acc: 0.7812\n",
      "Epoch 804/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4261 - acc: 0.7865 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 805/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4261 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 806/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4261 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 807/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3035 - acc: 0.906 - 0s 29us/step - loss: 0.4261 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 808/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4260 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 809/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4260 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 810/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4260 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 811/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4260 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 812/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4259 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 813/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4259 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 814/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4259 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 815/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4259 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 816/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4258 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 817/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4258 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 818/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4258 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 819/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4258 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 820/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4258 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 821/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4257 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 822/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4257 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 823/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4257 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 824/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4257 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 825/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4256 - acc: 0.7882 - val_loss: 0.5096 - val_acc: 0.7812\n",
      "Epoch 826/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4256 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 827/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4256 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 828/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4256 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 829/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4255 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 830/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4255 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 831/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4255 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 832/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4254 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 833/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4255 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 834/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4254 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 835/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4254 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 836/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4254 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 837/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4254 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 838/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4254 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 839/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4253 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 840/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4253 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 841/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4252 - acc: 0.7882 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 842/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4252 - acc: 0.7882 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 843/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4252 - acc: 0.7882 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 844/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4252 - acc: 0.7882 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 845/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4251 - acc: 0.7882 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 846/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4252 - acc: 0.7882 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 847/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4251 - acc: 0.7882 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 848/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4251 - acc: 0.7882 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 849/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4251 - acc: 0.7899 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 850/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4250 - acc: 0.7882 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 851/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4250 - acc: 0.7899 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 852/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4250 - acc: 0.7882 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 853/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4250 - acc: 0.7899 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 854/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4249 - acc: 0.7899 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 855/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4250 - acc: 0.7899 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 856/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4249 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 857/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4249 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 858/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4249 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 859/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4248 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 860/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4248 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 861/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4248 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 862/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4248 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 863/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4247 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 864/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4247 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 865/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4247 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 866/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4247 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 867/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4246 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 868/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4246 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 869/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4246 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 870/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4246 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 871/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4245 - acc: 0.7899 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 872/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4245 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 873/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4245 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 874/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4245 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 875/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4245 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 876/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4245 - acc: 0.7899 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 877/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4244 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 878/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4244 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 879/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4244 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 880/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4244 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 881/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4243 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 882/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4243 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 883/1000\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4243 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 884/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4243 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 885/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4242 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 886/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4242 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 887/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4242 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 888/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4242 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 889/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4242 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 890/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4241 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 891/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4241 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 892/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4241 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 893/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4241 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 894/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4241 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 895/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4240 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 896/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4240 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 897/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4240 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 898/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4239 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 899/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4239 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 900/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4239 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 901/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4239 - acc: 0.7917 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 902/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4239 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 903/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4238 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 904/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4238 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 905/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4238 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 906/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4237 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 907/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4237 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 908/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4237 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 909/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4237 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 910/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4236 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 911/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4237 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 912/1000\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4236 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 913/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4236 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 914/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4236 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7812\n",
      "Epoch 915/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4235 - acc: 0.7917 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 916/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4235 - acc: 0.7917 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 917/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4235 - acc: 0.7917 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 918/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4235 - acc: 0.7917 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 919/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4234 - acc: 0.7917 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 920/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4234 - acc: 0.7917 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 921/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4234 - acc: 0.7917 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 922/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4234 - acc: 0.7934 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 923/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4233 - acc: 0.7934 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 924/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4233 - acc: 0.7934 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 925/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4233 - acc: 0.7951 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 926/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4233 - acc: 0.7934 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 927/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4233 - acc: 0.7951 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 928/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4233 - acc: 0.7934 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 929/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4232 - acc: 0.7934 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 930/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4232 - acc: 0.7951 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 931/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4232 - acc: 0.7951 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 932/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4231 - acc: 0.7951 - val_loss: 0.5094 - val_acc: 0.7812\n",
      "Epoch 933/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4231 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 934/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4231 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 935/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4231 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 936/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4231 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 937/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4230 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7812\n",
      "Epoch 938/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4230 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7760\n",
      "Epoch 939/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4230 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7760\n",
      "Epoch 940/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4230 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7760\n",
      "Epoch 941/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4230 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7760\n",
      "Epoch 942/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4229 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7760\n",
      "Epoch 943/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4229 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7760\n",
      "Epoch 944/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4229 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7760\n",
      "Epoch 945/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4229 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7760\n",
      "Epoch 946/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4228 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7760\n",
      "Epoch 947/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4228 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7760\n",
      "Epoch 948/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4228 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7760\n",
      "Epoch 949/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4228 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7760\n",
      "Epoch 950/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4227 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7760\n",
      "Epoch 951/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4227 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7760\n",
      "Epoch 952/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4227 - acc: 0.7951 - val_loss: 0.5095 - val_acc: 0.7760\n",
      "Epoch 953/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4227 - acc: 0.7951 - val_loss: 0.5096 - val_acc: 0.7760\n",
      "Epoch 954/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4226 - acc: 0.7951 - val_loss: 0.5096 - val_acc: 0.7760\n",
      "Epoch 955/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4226 - acc: 0.7951 - val_loss: 0.5096 - val_acc: 0.7760\n",
      "Epoch 956/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4226 - acc: 0.7951 - val_loss: 0.5096 - val_acc: 0.7760\n",
      "Epoch 957/1000\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4226 - acc: 0.7951 - val_loss: 0.5096 - val_acc: 0.7760\n",
      "Epoch 958/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4225 - acc: 0.7951 - val_loss: 0.5096 - val_acc: 0.7760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 959/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4225 - acc: 0.7951 - val_loss: 0.5096 - val_acc: 0.7760\n",
      "Epoch 960/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4225 - acc: 0.7951 - val_loss: 0.5096 - val_acc: 0.7760\n",
      "Epoch 961/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4225 - acc: 0.7951 - val_loss: 0.5096 - val_acc: 0.7760\n",
      "Epoch 962/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4225 - acc: 0.7951 - val_loss: 0.5096 - val_acc: 0.7760\n",
      "Epoch 963/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4225 - acc: 0.7951 - val_loss: 0.5096 - val_acc: 0.7760\n",
      "Epoch 964/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4224 - acc: 0.7951 - val_loss: 0.5096 - val_acc: 0.7760\n",
      "Epoch 965/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4224 - acc: 0.7934 - val_loss: 0.5096 - val_acc: 0.7760\n",
      "Epoch 966/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4224 - acc: 0.7951 - val_loss: 0.5097 - val_acc: 0.7760\n",
      "Epoch 967/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4224 - acc: 0.7934 - val_loss: 0.5097 - val_acc: 0.7760\n",
      "Epoch 968/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4223 - acc: 0.7934 - val_loss: 0.5097 - val_acc: 0.7760\n",
      "Epoch 969/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4223 - acc: 0.7934 - val_loss: 0.5097 - val_acc: 0.7760\n",
      "Epoch 970/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4223 - acc: 0.7934 - val_loss: 0.5097 - val_acc: 0.7760\n",
      "Epoch 971/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4223 - acc: 0.7934 - val_loss: 0.5097 - val_acc: 0.7760\n",
      "Epoch 972/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4223 - acc: 0.7934 - val_loss: 0.5097 - val_acc: 0.7708\n",
      "Epoch 973/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4222 - acc: 0.7934 - val_loss: 0.5097 - val_acc: 0.7708\n",
      "Epoch 974/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4222 - acc: 0.7934 - val_loss: 0.5097 - val_acc: 0.7708\n",
      "Epoch 975/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4221 - acc: 0.7934 - val_loss: 0.5097 - val_acc: 0.7708\n",
      "Epoch 976/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4222 - acc: 0.7934 - val_loss: 0.5097 - val_acc: 0.7708\n",
      "Epoch 977/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4221 - acc: 0.7934 - val_loss: 0.5097 - val_acc: 0.7708\n",
      "Epoch 978/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4221 - acc: 0.7934 - val_loss: 0.5097 - val_acc: 0.7708\n",
      "Epoch 979/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4221 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 980/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4220 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 981/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4220 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 982/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4220 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 983/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4220 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 984/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4220 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 985/1000\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4219 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 986/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4219 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 987/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4219 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 988/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4218 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 989/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4218 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 990/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4218 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 991/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4218 - acc: 0.7917 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 992/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4218 - acc: 0.7917 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 993/1000\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4217 - acc: 0.7917 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 994/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4217 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 995/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4217 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7708\n",
      "Epoch 996/1000\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4216 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7708\n",
      "Epoch 997/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4216 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7708\n",
      "Epoch 998/1000\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4216 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7708\n",
      "Epoch 999/1000\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4215 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7708\n",
      "Epoch 1000/1000\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4215 - acc: 0.7934 - val_loss: 0.5098 - val_acc: 0.7708\n"
     ]
    }
   ],
   "source": [
    "## Note that when we call \"fit\" again, it picks up where it left off\n",
    "run_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x285e1c32a58>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(run_hist_1.history[\"loss\"])\n",
    "m = len(run_hist_1b.history['loss'])\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this graph begins where the other left off.  While the training loss is still going down, it looks like the validation loss has stabilized (or even gotten worse!).  This suggests that our network will not benefit from further training.  What is the appropriate number of epochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Now it's your turn.  Do the following in the cells below:\n",
    "- Build a model with two hidden layers, each with 6 nodes\n",
    "- Use the \"relu\" activation function for the hidden layers, and \"sigmoid\" for the final layer\n",
    "- Use a learning rate of .003 and train for 1500 epochs\n",
    "- Graph the trajectory of the loss functions, accuracy on both train and test set\n",
    "- Plot the roc curve for the predictions\n",
    "\n",
    "Experiment with different learning rates, numbers of epochs, and network structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here to with layer 1,2 having activation relu and layer 3 with activation sigmoid\n",
    "model_2 = Sequential([\n",
    "    Dense(6, input_shape=(8,), activation=\"relu\"),\n",
    "    Dense(6,input_shape = (6,),activation='relu'),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 103\n",
      "Trainable params: 103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/1500\n",
      "576/576 [==============================] - 0s 688us/step - loss: 0.7997 - acc: 0.3507 - val_loss: 0.7916 - val_acc: 0.3698\n",
      "Epoch 2/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.7815 - acc: 0.3576 - val_loss: 0.7764 - val_acc: 0.3750\n",
      "Epoch 3/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.7651 - acc: 0.3628 - val_loss: 0.7625 - val_acc: 0.3802\n",
      "Epoch 4/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.7501 - acc: 0.3767 - val_loss: 0.7500 - val_acc: 0.3906\n",
      "Epoch 5/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.7365 - acc: 0.4028 - val_loss: 0.7385 - val_acc: 0.4062\n",
      "Epoch 6/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.7240 - acc: 0.4392 - val_loss: 0.7280 - val_acc: 0.4427\n",
      "Epoch 7/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.7126 - acc: 0.4740 - val_loss: 0.7184 - val_acc: 0.4635\n",
      "Epoch 8/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.7020 - acc: 0.5139 - val_loss: 0.7095 - val_acc: 0.5365\n",
      "Epoch 9/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.6922 - acc: 0.5590 - val_loss: 0.7014 - val_acc: 0.5521\n",
      "Epoch 10/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.6832 - acc: 0.6007 - val_loss: 0.6939 - val_acc: 0.5625\n",
      "Epoch 11/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.6749 - acc: 0.6337 - val_loss: 0.6870 - val_acc: 0.5677\n",
      "Epoch 12/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.6671 - acc: 0.6615 - val_loss: 0.6806 - val_acc: 0.5677\n",
      "Epoch 13/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.6599 - acc: 0.6649 - val_loss: 0.6746 - val_acc: 0.6094\n",
      "Epoch 14/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.6531 - acc: 0.6823 - val_loss: 0.6691 - val_acc: 0.6094\n",
      "Epoch 15/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.6468 - acc: 0.6875 - val_loss: 0.6639 - val_acc: 0.6354\n",
      "Epoch 16/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.6409 - acc: 0.6927 - val_loss: 0.6591 - val_acc: 0.6510\n",
      "Epoch 17/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.6354 - acc: 0.6927 - val_loss: 0.6546 - val_acc: 0.6615\n",
      "Epoch 18/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.6302 - acc: 0.6875 - val_loss: 0.6504 - val_acc: 0.6510\n",
      "Epoch 19/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.6253 - acc: 0.6892 - val_loss: 0.6465 - val_acc: 0.6562\n",
      "Epoch 20/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.6206 - acc: 0.6858 - val_loss: 0.6427 - val_acc: 0.6562\n",
      "Epoch 21/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.6163 - acc: 0.6875 - val_loss: 0.6393 - val_acc: 0.6458\n",
      "Epoch 22/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.6122 - acc: 0.6910 - val_loss: 0.6360 - val_acc: 0.6354\n",
      "Epoch 23/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.6083 - acc: 0.6927 - val_loss: 0.6329 - val_acc: 0.6406\n",
      "Epoch 24/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.6046 - acc: 0.6944 - val_loss: 0.6299 - val_acc: 0.6406\n",
      "Epoch 25/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.6011 - acc: 0.6979 - val_loss: 0.6272 - val_acc: 0.6510\n",
      "Epoch 26/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5977 - acc: 0.6944 - val_loss: 0.6246 - val_acc: 0.6562\n",
      "Epoch 27/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5946 - acc: 0.6892 - val_loss: 0.6221 - val_acc: 0.6615\n",
      "Epoch 28/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5916 - acc: 0.6962 - val_loss: 0.6198 - val_acc: 0.6667\n",
      "Epoch 29/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5887 - acc: 0.6997 - val_loss: 0.6176 - val_acc: 0.6667\n",
      "Epoch 30/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5860 - acc: 0.6997 - val_loss: 0.6155 - val_acc: 0.6719\n",
      "Epoch 31/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5834 - acc: 0.7014 - val_loss: 0.6135 - val_acc: 0.6667\n",
      "Epoch 32/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5809 - acc: 0.7014 - val_loss: 0.6116 - val_acc: 0.6771\n",
      "Epoch 33/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5785 - acc: 0.7083 - val_loss: 0.6098 - val_acc: 0.6771\n",
      "Epoch 34/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5762 - acc: 0.7083 - val_loss: 0.6081 - val_acc: 0.6927\n",
      "Epoch 35/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5740 - acc: 0.7101 - val_loss: 0.6064 - val_acc: 0.6927\n",
      "Epoch 36/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5719 - acc: 0.7118 - val_loss: 0.6048 - val_acc: 0.6927\n",
      "Epoch 37/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.5699 - acc: 0.7101 - val_loss: 0.6033 - val_acc: 0.6927\n",
      "Epoch 38/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5679 - acc: 0.7101 - val_loss: 0.6018 - val_acc: 0.6927\n",
      "Epoch 39/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5660 - acc: 0.7101 - val_loss: 0.6004 - val_acc: 0.6875\n",
      "Epoch 40/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5642 - acc: 0.7118 - val_loss: 0.5991 - val_acc: 0.6875\n",
      "Epoch 41/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5624 - acc: 0.7135 - val_loss: 0.5978 - val_acc: 0.6875\n",
      "Epoch 42/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5607 - acc: 0.7118 - val_loss: 0.5965 - val_acc: 0.6979\n",
      "Epoch 43/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5590 - acc: 0.7118 - val_loss: 0.5954 - val_acc: 0.6979\n",
      "Epoch 44/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5575 - acc: 0.7118 - val_loss: 0.5942 - val_acc: 0.6979\n",
      "Epoch 45/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5559 - acc: 0.7135 - val_loss: 0.5931 - val_acc: 0.6979\n",
      "Epoch 46/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.5544 - acc: 0.7135 - val_loss: 0.5921 - val_acc: 0.6979\n",
      "Epoch 47/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5530 - acc: 0.7135 - val_loss: 0.5910 - val_acc: 0.6979\n",
      "Epoch 48/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5517 - acc: 0.7118 - val_loss: 0.5901 - val_acc: 0.6979\n",
      "Epoch 49/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5503 - acc: 0.7135 - val_loss: 0.5891 - val_acc: 0.6979\n",
      "Epoch 50/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.5490 - acc: 0.7135 - val_loss: 0.5882 - val_acc: 0.6979\n",
      "Epoch 51/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5478 - acc: 0.7135 - val_loss: 0.5873 - val_acc: 0.6979\n",
      "Epoch 52/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5466 - acc: 0.7153 - val_loss: 0.5865 - val_acc: 0.6979\n",
      "Epoch 53/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5453 - acc: 0.7153 - val_loss: 0.5856 - val_acc: 0.6979\n",
      "Epoch 54/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5442 - acc: 0.7153 - val_loss: 0.5848 - val_acc: 0.7031\n",
      "Epoch 55/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5430 - acc: 0.7170 - val_loss: 0.5840 - val_acc: 0.6979\n",
      "Epoch 56/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5418 - acc: 0.7188 - val_loss: 0.5832 - val_acc: 0.6979\n",
      "Epoch 57/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5407 - acc: 0.7205 - val_loss: 0.5824 - val_acc: 0.6979\n",
      "Epoch 58/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5396 - acc: 0.7188 - val_loss: 0.5816 - val_acc: 0.6979\n",
      "Epoch 59/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5386 - acc: 0.7205 - val_loss: 0.5808 - val_acc: 0.7031\n",
      "Epoch 60/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5375 - acc: 0.7205 - val_loss: 0.5801 - val_acc: 0.7083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5365 - acc: 0.7205 - val_loss: 0.5793 - val_acc: 0.7083\n",
      "Epoch 62/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5355 - acc: 0.7222 - val_loss: 0.5786 - val_acc: 0.7135\n",
      "Epoch 63/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5345 - acc: 0.7240 - val_loss: 0.5779 - val_acc: 0.7135\n",
      "Epoch 64/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5336 - acc: 0.7222 - val_loss: 0.5773 - val_acc: 0.7188\n",
      "Epoch 65/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5326 - acc: 0.7222 - val_loss: 0.5766 - val_acc: 0.7188\n",
      "Epoch 66/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5317 - acc: 0.7222 - val_loss: 0.5759 - val_acc: 0.7188\n",
      "Epoch 67/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5308 - acc: 0.7222 - val_loss: 0.5753 - val_acc: 0.7188\n",
      "Epoch 68/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5300 - acc: 0.7257 - val_loss: 0.5746 - val_acc: 0.7188\n",
      "Epoch 69/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5291 - acc: 0.7257 - val_loss: 0.5740 - val_acc: 0.7188\n",
      "Epoch 70/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5283 - acc: 0.7274 - val_loss: 0.5734 - val_acc: 0.7240\n",
      "Epoch 71/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5275 - acc: 0.7274 - val_loss: 0.5728 - val_acc: 0.7240\n",
      "Epoch 72/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5267 - acc: 0.7257 - val_loss: 0.5722 - val_acc: 0.7240\n",
      "Epoch 73/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5259 - acc: 0.7274 - val_loss: 0.5717 - val_acc: 0.7240\n",
      "Epoch 74/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5251 - acc: 0.7257 - val_loss: 0.5711 - val_acc: 0.7240\n",
      "Epoch 75/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5243 - acc: 0.7274 - val_loss: 0.5705 - val_acc: 0.7240\n",
      "Epoch 76/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5235 - acc: 0.7292 - val_loss: 0.5699 - val_acc: 0.7240\n",
      "Epoch 77/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5228 - acc: 0.7274 - val_loss: 0.5694 - val_acc: 0.7240\n",
      "Epoch 78/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5220 - acc: 0.7292 - val_loss: 0.5689 - val_acc: 0.7240\n",
      "Epoch 79/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5213 - acc: 0.7292 - val_loss: 0.5683 - val_acc: 0.7240\n",
      "Epoch 80/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5206 - acc: 0.7292 - val_loss: 0.5678 - val_acc: 0.7188\n",
      "Epoch 81/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5322 - acc: 0.750 - 0s 29us/step - loss: 0.5198 - acc: 0.7309 - val_loss: 0.5672 - val_acc: 0.7188\n",
      "Epoch 82/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5191 - acc: 0.7344 - val_loss: 0.5667 - val_acc: 0.7188\n",
      "Epoch 83/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5184 - acc: 0.7326 - val_loss: 0.5661 - val_acc: 0.7188\n",
      "Epoch 84/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5177 - acc: 0.7326 - val_loss: 0.5656 - val_acc: 0.7188\n",
      "Epoch 85/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5171 - acc: 0.7326 - val_loss: 0.5650 - val_acc: 0.7188\n",
      "Epoch 86/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5164 - acc: 0.7326 - val_loss: 0.5645 - val_acc: 0.7188\n",
      "Epoch 87/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5157 - acc: 0.7326 - val_loss: 0.5639 - val_acc: 0.7188\n",
      "Epoch 88/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5150 - acc: 0.7326 - val_loss: 0.5634 - val_acc: 0.7240\n",
      "Epoch 89/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5144 - acc: 0.7344 - val_loss: 0.5629 - val_acc: 0.7240\n",
      "Epoch 90/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5138 - acc: 0.7344 - val_loss: 0.5623 - val_acc: 0.7240\n",
      "Epoch 91/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5131 - acc: 0.7344 - val_loss: 0.5618 - val_acc: 0.7240\n",
      "Epoch 92/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5125 - acc: 0.7344 - val_loss: 0.5613 - val_acc: 0.7240\n",
      "Epoch 93/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5119 - acc: 0.7344 - val_loss: 0.5608 - val_acc: 0.7292\n",
      "Epoch 94/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5113 - acc: 0.7344 - val_loss: 0.5603 - val_acc: 0.7292\n",
      "Epoch 95/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5107 - acc: 0.7344 - val_loss: 0.5598 - val_acc: 0.7292\n",
      "Epoch 96/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5101 - acc: 0.7344 - val_loss: 0.5593 - val_acc: 0.7292\n",
      "Epoch 97/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5095 - acc: 0.7361 - val_loss: 0.5588 - val_acc: 0.7292\n",
      "Epoch 98/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5089 - acc: 0.7344 - val_loss: 0.5583 - val_acc: 0.7292\n",
      "Epoch 99/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5083 - acc: 0.7344 - val_loss: 0.5578 - val_acc: 0.7344\n",
      "Epoch 100/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5077 - acc: 0.7361 - val_loss: 0.5573 - val_acc: 0.7344\n",
      "Epoch 101/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5071 - acc: 0.7344 - val_loss: 0.5569 - val_acc: 0.7344\n",
      "Epoch 102/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5066 - acc: 0.7344 - val_loss: 0.5564 - val_acc: 0.7344\n",
      "Epoch 103/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5061 - acc: 0.7326 - val_loss: 0.5560 - val_acc: 0.7396\n",
      "Epoch 104/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5055 - acc: 0.7326 - val_loss: 0.5556 - val_acc: 0.7396\n",
      "Epoch 105/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5050 - acc: 0.7344 - val_loss: 0.5552 - val_acc: 0.7396\n",
      "Epoch 106/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5044 - acc: 0.7361 - val_loss: 0.5548 - val_acc: 0.7396\n",
      "Epoch 107/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5039 - acc: 0.7378 - val_loss: 0.5544 - val_acc: 0.7396\n",
      "Epoch 108/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.5034 - acc: 0.7378 - val_loss: 0.5540 - val_acc: 0.7396\n",
      "Epoch 109/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5029 - acc: 0.7378 - val_loss: 0.5536 - val_acc: 0.7396\n",
      "Epoch 110/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5024 - acc: 0.7396 - val_loss: 0.5532 - val_acc: 0.7396\n",
      "Epoch 111/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.5019 - acc: 0.7396 - val_loss: 0.5528 - val_acc: 0.7396\n",
      "Epoch 112/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5014 - acc: 0.7396 - val_loss: 0.5525 - val_acc: 0.7396\n",
      "Epoch 113/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5009 - acc: 0.7413 - val_loss: 0.5521 - val_acc: 0.7396\n",
      "Epoch 114/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.5005 - acc: 0.7413 - val_loss: 0.5518 - val_acc: 0.7396\n",
      "Epoch 115/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4729 - acc: 0.750 - 0s 31us/step - loss: 0.5000 - acc: 0.7431 - val_loss: 0.5514 - val_acc: 0.7448\n",
      "Epoch 116/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4995 - acc: 0.7431 - val_loss: 0.5511 - val_acc: 0.7500\n",
      "Epoch 117/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4991 - acc: 0.7413 - val_loss: 0.5508 - val_acc: 0.7500\n",
      "Epoch 118/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4986 - acc: 0.7413 - val_loss: 0.5505 - val_acc: 0.7552\n",
      "Epoch 119/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4982 - acc: 0.7413 - val_loss: 0.5502 - val_acc: 0.7552\n",
      "Epoch 120/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4977 - acc: 0.7448 - val_loss: 0.5499 - val_acc: 0.7552\n",
      "Epoch 121/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4973 - acc: 0.7448 - val_loss: 0.5496 - val_acc: 0.7552\n",
      "Epoch 122/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4969 - acc: 0.7448 - val_loss: 0.5493 - val_acc: 0.7500\n",
      "Epoch 123/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4965 - acc: 0.7465 - val_loss: 0.5490 - val_acc: 0.7500\n",
      "Epoch 124/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4961 - acc: 0.7483 - val_loss: 0.5487 - val_acc: 0.7500\n",
      "Epoch 125/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4957 - acc: 0.7483 - val_loss: 0.5484 - val_acc: 0.7500\n",
      "Epoch 126/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4952 - acc: 0.7500 - val_loss: 0.5481 - val_acc: 0.7500\n",
      "Epoch 127/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4948 - acc: 0.7483 - val_loss: 0.5478 - val_acc: 0.7500\n",
      "Epoch 128/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4944 - acc: 0.7483 - val_loss: 0.5475 - val_acc: 0.7500\n",
      "Epoch 129/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4940 - acc: 0.7500 - val_loss: 0.5472 - val_acc: 0.7500\n",
      "Epoch 130/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4936 - acc: 0.7465 - val_loss: 0.5469 - val_acc: 0.7500\n",
      "Epoch 131/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4932 - acc: 0.7465 - val_loss: 0.5466 - val_acc: 0.7500\n",
      "Epoch 132/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4928 - acc: 0.7465 - val_loss: 0.5464 - val_acc: 0.7500\n",
      "Epoch 133/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4924 - acc: 0.7483 - val_loss: 0.5461 - val_acc: 0.7500\n",
      "Epoch 134/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4920 - acc: 0.7500 - val_loss: 0.5458 - val_acc: 0.7500\n",
      "Epoch 135/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4916 - acc: 0.7500 - val_loss: 0.5456 - val_acc: 0.7552\n",
      "Epoch 136/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4912 - acc: 0.7500 - val_loss: 0.5453 - val_acc: 0.7500\n",
      "Epoch 137/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4907 - acc: 0.7500 - val_loss: 0.5450 - val_acc: 0.7500\n",
      "Epoch 138/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4903 - acc: 0.7483 - val_loss: 0.5447 - val_acc: 0.7500\n",
      "Epoch 139/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4899 - acc: 0.7483 - val_loss: 0.5444 - val_acc: 0.7500\n",
      "Epoch 140/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4894 - acc: 0.7465 - val_loss: 0.5440 - val_acc: 0.7500\n",
      "Epoch 141/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4889 - acc: 0.7465 - val_loss: 0.5437 - val_acc: 0.7500\n",
      "Epoch 142/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4885 - acc: 0.7465 - val_loss: 0.5433 - val_acc: 0.7500\n",
      "Epoch 143/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4880 - acc: 0.7448 - val_loss: 0.5430 - val_acc: 0.7500\n",
      "Epoch 144/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4876 - acc: 0.7448 - val_loss: 0.5427 - val_acc: 0.7500\n",
      "Epoch 145/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4872 - acc: 0.7448 - val_loss: 0.5424 - val_acc: 0.7500\n",
      "Epoch 146/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4868 - acc: 0.7431 - val_loss: 0.5421 - val_acc: 0.7500\n",
      "Epoch 147/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4864 - acc: 0.7431 - val_loss: 0.5418 - val_acc: 0.7500\n",
      "Epoch 148/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4860 - acc: 0.7448 - val_loss: 0.5415 - val_acc: 0.7500\n",
      "Epoch 149/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4856 - acc: 0.7413 - val_loss: 0.5412 - val_acc: 0.7500\n",
      "Epoch 150/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4852 - acc: 0.7413 - val_loss: 0.5409 - val_acc: 0.7500\n",
      "Epoch 151/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4848 - acc: 0.7448 - val_loss: 0.5407 - val_acc: 0.7500\n",
      "Epoch 152/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4844 - acc: 0.7413 - val_loss: 0.5404 - val_acc: 0.7552\n",
      "Epoch 153/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4840 - acc: 0.7413 - val_loss: 0.5401 - val_acc: 0.7552\n",
      "Epoch 154/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4837 - acc: 0.7413 - val_loss: 0.5399 - val_acc: 0.7552\n",
      "Epoch 155/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4833 - acc: 0.7396 - val_loss: 0.5396 - val_acc: 0.7552\n",
      "Epoch 156/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4829 - acc: 0.7431 - val_loss: 0.5393 - val_acc: 0.7552\n",
      "Epoch 157/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4826 - acc: 0.7413 - val_loss: 0.5391 - val_acc: 0.7552\n",
      "Epoch 158/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4823 - acc: 0.7413 - val_loss: 0.5389 - val_acc: 0.7552\n",
      "Epoch 159/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4819 - acc: 0.7448 - val_loss: 0.5386 - val_acc: 0.7552\n",
      "Epoch 160/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4816 - acc: 0.7431 - val_loss: 0.5384 - val_acc: 0.7552\n",
      "Epoch 161/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4812 - acc: 0.7431 - val_loss: 0.5382 - val_acc: 0.7552\n",
      "Epoch 162/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4809 - acc: 0.7431 - val_loss: 0.5380 - val_acc: 0.7552\n",
      "Epoch 163/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4806 - acc: 0.7431 - val_loss: 0.5378 - val_acc: 0.7552\n",
      "Epoch 164/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4803 - acc: 0.7465 - val_loss: 0.5376 - val_acc: 0.7552\n",
      "Epoch 165/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4800 - acc: 0.7448 - val_loss: 0.5374 - val_acc: 0.7552\n",
      "Epoch 166/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4797 - acc: 0.7448 - val_loss: 0.5372 - val_acc: 0.7500\n",
      "Epoch 167/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4794 - acc: 0.7483 - val_loss: 0.5371 - val_acc: 0.7500\n",
      "Epoch 168/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4791 - acc: 0.7483 - val_loss: 0.5369 - val_acc: 0.7500\n",
      "Epoch 169/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4788 - acc: 0.7483 - val_loss: 0.5367 - val_acc: 0.7500\n",
      "Epoch 170/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4785 - acc: 0.7483 - val_loss: 0.5365 - val_acc: 0.7500\n",
      "Epoch 171/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4782 - acc: 0.7517 - val_loss: 0.5363 - val_acc: 0.7500\n",
      "Epoch 172/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4779 - acc: 0.7552 - val_loss: 0.5361 - val_acc: 0.7500\n",
      "Epoch 173/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4776 - acc: 0.7569 - val_loss: 0.5360 - val_acc: 0.7500\n",
      "Epoch 174/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4774 - acc: 0.7569 - val_loss: 0.5358 - val_acc: 0.7500\n",
      "Epoch 175/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4771 - acc: 0.7569 - val_loss: 0.5356 - val_acc: 0.7500\n",
      "Epoch 176/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4769 - acc: 0.7569 - val_loss: 0.5355 - val_acc: 0.7500\n",
      "Epoch 177/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4766 - acc: 0.7587 - val_loss: 0.5354 - val_acc: 0.7500\n",
      "Epoch 178/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4763 - acc: 0.7587 - val_loss: 0.5352 - val_acc: 0.7500\n",
      "Epoch 179/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4761 - acc: 0.7587 - val_loss: 0.5351 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4758 - acc: 0.7604 - val_loss: 0.5350 - val_acc: 0.7500\n",
      "Epoch 181/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4756 - acc: 0.7587 - val_loss: 0.5348 - val_acc: 0.7500\n",
      "Epoch 182/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4753 - acc: 0.7604 - val_loss: 0.5347 - val_acc: 0.7500\n",
      "Epoch 183/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4750 - acc: 0.7604 - val_loss: 0.5346 - val_acc: 0.7500\n",
      "Epoch 184/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4748 - acc: 0.7604 - val_loss: 0.5345 - val_acc: 0.7500\n",
      "Epoch 185/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4746 - acc: 0.7639 - val_loss: 0.5344 - val_acc: 0.7500\n",
      "Epoch 186/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4743 - acc: 0.7639 - val_loss: 0.5343 - val_acc: 0.7500\n",
      "Epoch 187/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4741 - acc: 0.7639 - val_loss: 0.5342 - val_acc: 0.7500\n",
      "Epoch 188/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4738 - acc: 0.7639 - val_loss: 0.5341 - val_acc: 0.7448\n",
      "Epoch 189/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4736 - acc: 0.7639 - val_loss: 0.5340 - val_acc: 0.7448\n",
      "Epoch 190/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4734 - acc: 0.7639 - val_loss: 0.5339 - val_acc: 0.7448\n",
      "Epoch 191/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4732 - acc: 0.7639 - val_loss: 0.5338 - val_acc: 0.7448\n",
      "Epoch 192/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4729 - acc: 0.7639 - val_loss: 0.5337 - val_acc: 0.7448\n",
      "Epoch 193/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4727 - acc: 0.7639 - val_loss: 0.5337 - val_acc: 0.7448\n",
      "Epoch 194/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4725 - acc: 0.7656 - val_loss: 0.5336 - val_acc: 0.7448\n",
      "Epoch 195/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4722 - acc: 0.7656 - val_loss: 0.5334 - val_acc: 0.7448\n",
      "Epoch 196/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4720 - acc: 0.7656 - val_loss: 0.5333 - val_acc: 0.7500\n",
      "Epoch 197/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4718 - acc: 0.7656 - val_loss: 0.5332 - val_acc: 0.7500\n",
      "Epoch 198/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4715 - acc: 0.7656 - val_loss: 0.5331 - val_acc: 0.7500\n",
      "Epoch 199/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4713 - acc: 0.7656 - val_loss: 0.5330 - val_acc: 0.7500\n",
      "Epoch 200/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4712 - acc: 0.7674 - val_loss: 0.5329 - val_acc: 0.7448\n",
      "Epoch 201/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4709 - acc: 0.7674 - val_loss: 0.5328 - val_acc: 0.7448\n",
      "Epoch 202/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4707 - acc: 0.7691 - val_loss: 0.5327 - val_acc: 0.7448\n",
      "Epoch 203/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4705 - acc: 0.7691 - val_loss: 0.5326 - val_acc: 0.7448\n",
      "Epoch 204/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4703 - acc: 0.7691 - val_loss: 0.5325 - val_acc: 0.7448\n",
      "Epoch 205/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4700 - acc: 0.7691 - val_loss: 0.5324 - val_acc: 0.7448\n",
      "Epoch 206/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4698 - acc: 0.7743 - val_loss: 0.5323 - val_acc: 0.7448\n",
      "Epoch 207/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4697 - acc: 0.7743 - val_loss: 0.5322 - val_acc: 0.7448\n",
      "Epoch 208/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4695 - acc: 0.7743 - val_loss: 0.5321 - val_acc: 0.7448\n",
      "Epoch 209/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4692 - acc: 0.7743 - val_loss: 0.5320 - val_acc: 0.7448\n",
      "Epoch 210/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4690 - acc: 0.7743 - val_loss: 0.5319 - val_acc: 0.7448\n",
      "Epoch 211/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4689 - acc: 0.7743 - val_loss: 0.5319 - val_acc: 0.7448\n",
      "Epoch 212/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4686 - acc: 0.7760 - val_loss: 0.5318 - val_acc: 0.7448\n",
      "Epoch 213/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4684 - acc: 0.7760 - val_loss: 0.5317 - val_acc: 0.7448\n",
      "Epoch 214/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4683 - acc: 0.7743 - val_loss: 0.5316 - val_acc: 0.7448\n",
      "Epoch 215/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4680 - acc: 0.7743 - val_loss: 0.5315 - val_acc: 0.7500\n",
      "Epoch 216/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4678 - acc: 0.7743 - val_loss: 0.5313 - val_acc: 0.7500\n",
      "Epoch 217/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4676 - acc: 0.7743 - val_loss: 0.5312 - val_acc: 0.7500\n",
      "Epoch 218/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4674 - acc: 0.7743 - val_loss: 0.5311 - val_acc: 0.7500\n",
      "Epoch 219/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4673 - acc: 0.7743 - val_loss: 0.5310 - val_acc: 0.7500\n",
      "Epoch 220/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4671 - acc: 0.7760 - val_loss: 0.5310 - val_acc: 0.7500\n",
      "Epoch 221/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4669 - acc: 0.7760 - val_loss: 0.5309 - val_acc: 0.7500\n",
      "Epoch 222/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4667 - acc: 0.7760 - val_loss: 0.5308 - val_acc: 0.7500\n",
      "Epoch 223/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4665 - acc: 0.7760 - val_loss: 0.5307 - val_acc: 0.7500\n",
      "Epoch 224/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4663 - acc: 0.7760 - val_loss: 0.5306 - val_acc: 0.7500\n",
      "Epoch 225/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4662 - acc: 0.7760 - val_loss: 0.5305 - val_acc: 0.7500\n",
      "Epoch 226/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4660 - acc: 0.7778 - val_loss: 0.5304 - val_acc: 0.7500\n",
      "Epoch 227/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4659 - acc: 0.7778 - val_loss: 0.5303 - val_acc: 0.7500\n",
      "Epoch 228/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4657 - acc: 0.7778 - val_loss: 0.5303 - val_acc: 0.7500\n",
      "Epoch 229/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4655 - acc: 0.7778 - val_loss: 0.5302 - val_acc: 0.7500\n",
      "Epoch 230/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4653 - acc: 0.7778 - val_loss: 0.5301 - val_acc: 0.7500\n",
      "Epoch 231/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4652 - acc: 0.7760 - val_loss: 0.5300 - val_acc: 0.7500\n",
      "Epoch 232/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4650 - acc: 0.7778 - val_loss: 0.5299 - val_acc: 0.7500\n",
      "Epoch 233/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4648 - acc: 0.7760 - val_loss: 0.5299 - val_acc: 0.7500\n",
      "Epoch 234/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4647 - acc: 0.7760 - val_loss: 0.5298 - val_acc: 0.7552\n",
      "Epoch 235/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4645 - acc: 0.7743 - val_loss: 0.5297 - val_acc: 0.7552\n",
      "Epoch 236/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4643 - acc: 0.7778 - val_loss: 0.5296 - val_acc: 0.7552\n",
      "Epoch 237/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4642 - acc: 0.7778 - val_loss: 0.5296 - val_acc: 0.7552\n",
      "Epoch 238/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4640 - acc: 0.7760 - val_loss: 0.5295 - val_acc: 0.7552\n",
      "Epoch 239/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4639 - acc: 0.7760 - val_loss: 0.5294 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4637 - acc: 0.7760 - val_loss: 0.5294 - val_acc: 0.7552\n",
      "Epoch 241/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4636 - acc: 0.7760 - val_loss: 0.5293 - val_acc: 0.7552\n",
      "Epoch 242/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4634 - acc: 0.7743 - val_loss: 0.5292 - val_acc: 0.7552\n",
      "Epoch 243/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4633 - acc: 0.7743 - val_loss: 0.5291 - val_acc: 0.7552\n",
      "Epoch 244/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4631 - acc: 0.7743 - val_loss: 0.5291 - val_acc: 0.7552\n",
      "Epoch 245/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4630 - acc: 0.7743 - val_loss: 0.5290 - val_acc: 0.7552\n",
      "Epoch 246/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4628 - acc: 0.7760 - val_loss: 0.5289 - val_acc: 0.7552\n",
      "Epoch 247/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4627 - acc: 0.7760 - val_loss: 0.5289 - val_acc: 0.7552\n",
      "Epoch 248/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4625 - acc: 0.7760 - val_loss: 0.5288 - val_acc: 0.7552\n",
      "Epoch 249/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4624 - acc: 0.7760 - val_loss: 0.5287 - val_acc: 0.7552\n",
      "Epoch 250/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4622 - acc: 0.7778 - val_loss: 0.5287 - val_acc: 0.7552\n",
      "Epoch 251/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4621 - acc: 0.7778 - val_loss: 0.5286 - val_acc: 0.7552\n",
      "Epoch 252/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4619 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7552\n",
      "Epoch 253/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4618 - acc: 0.7778 - val_loss: 0.5285 - val_acc: 0.7552\n",
      "Epoch 254/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4617 - acc: 0.7778 - val_loss: 0.5284 - val_acc: 0.7552\n",
      "Epoch 255/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4615 - acc: 0.7778 - val_loss: 0.5284 - val_acc: 0.7552\n",
      "Epoch 256/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4614 - acc: 0.7778 - val_loss: 0.5283 - val_acc: 0.7552\n",
      "Epoch 257/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4613 - acc: 0.7795 - val_loss: 0.5282 - val_acc: 0.7552\n",
      "Epoch 258/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4611 - acc: 0.7778 - val_loss: 0.5282 - val_acc: 0.7552\n",
      "Epoch 259/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4610 - acc: 0.7778 - val_loss: 0.5281 - val_acc: 0.7552\n",
      "Epoch 260/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4609 - acc: 0.7795 - val_loss: 0.5281 - val_acc: 0.7552\n",
      "Epoch 261/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4607 - acc: 0.7795 - val_loss: 0.5280 - val_acc: 0.7552\n",
      "Epoch 262/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4606 - acc: 0.7795 - val_loss: 0.5280 - val_acc: 0.7552\n",
      "Epoch 263/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4605 - acc: 0.7795 - val_loss: 0.5279 - val_acc: 0.7448\n",
      "Epoch 264/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4604 - acc: 0.7795 - val_loss: 0.5279 - val_acc: 0.7448\n",
      "Epoch 265/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4602 - acc: 0.7795 - val_loss: 0.5278 - val_acc: 0.7448\n",
      "Epoch 266/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4601 - acc: 0.7795 - val_loss: 0.5277 - val_acc: 0.7448\n",
      "Epoch 267/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4599 - acc: 0.7795 - val_loss: 0.5277 - val_acc: 0.7448\n",
      "Epoch 268/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4598 - acc: 0.7795 - val_loss: 0.5276 - val_acc: 0.7448\n",
      "Epoch 269/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4597 - acc: 0.7795 - val_loss: 0.5276 - val_acc: 0.7448\n",
      "Epoch 270/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4596 - acc: 0.7778 - val_loss: 0.5275 - val_acc: 0.7448\n",
      "Epoch 271/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4594 - acc: 0.7778 - val_loss: 0.5275 - val_acc: 0.7448\n",
      "Epoch 272/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4593 - acc: 0.7778 - val_loss: 0.5274 - val_acc: 0.7448\n",
      "Epoch 273/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4592 - acc: 0.7778 - val_loss: 0.5274 - val_acc: 0.7448\n",
      "Epoch 274/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4591 - acc: 0.7778 - val_loss: 0.5273 - val_acc: 0.7448\n",
      "Epoch 275/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4590 - acc: 0.7778 - val_loss: 0.5272 - val_acc: 0.7448\n",
      "Epoch 276/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4588 - acc: 0.7778 - val_loss: 0.5272 - val_acc: 0.7448\n",
      "Epoch 277/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4587 - acc: 0.7778 - val_loss: 0.5271 - val_acc: 0.7448\n",
      "Epoch 278/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4586 - acc: 0.7778 - val_loss: 0.5271 - val_acc: 0.7448\n",
      "Epoch 279/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4585 - acc: 0.7778 - val_loss: 0.5270 - val_acc: 0.7448\n",
      "Epoch 280/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4583 - acc: 0.7778 - val_loss: 0.5270 - val_acc: 0.7448\n",
      "Epoch 281/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3077 - acc: 0.875 - 0s 31us/step - loss: 0.4582 - acc: 0.7778 - val_loss: 0.5269 - val_acc: 0.7448\n",
      "Epoch 282/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4581 - acc: 0.7778 - val_loss: 0.5268 - val_acc: 0.7448\n",
      "Epoch 283/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4580 - acc: 0.7778 - val_loss: 0.5268 - val_acc: 0.7448\n",
      "Epoch 284/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4579 - acc: 0.7778 - val_loss: 0.5267 - val_acc: 0.7448\n",
      "Epoch 285/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4578 - acc: 0.7778 - val_loss: 0.5267 - val_acc: 0.7448\n",
      "Epoch 286/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4576 - acc: 0.7778 - val_loss: 0.5266 - val_acc: 0.7448\n",
      "Epoch 287/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4576 - acc: 0.7778 - val_loss: 0.5266 - val_acc: 0.7448\n",
      "Epoch 288/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4574 - acc: 0.7778 - val_loss: 0.5265 - val_acc: 0.7448\n",
      "Epoch 289/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4573 - acc: 0.7778 - val_loss: 0.5265 - val_acc: 0.7448\n",
      "Epoch 290/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4572 - acc: 0.7778 - val_loss: 0.5264 - val_acc: 0.7448\n",
      "Epoch 291/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4571 - acc: 0.7795 - val_loss: 0.5264 - val_acc: 0.7448\n",
      "Epoch 292/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4570 - acc: 0.7795 - val_loss: 0.5263 - val_acc: 0.7448\n",
      "Epoch 293/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4569 - acc: 0.7795 - val_loss: 0.5263 - val_acc: 0.7448\n",
      "Epoch 294/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4568 - acc: 0.7795 - val_loss: 0.5262 - val_acc: 0.7448\n",
      "Epoch 295/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4567 - acc: 0.7778 - val_loss: 0.5262 - val_acc: 0.7448\n",
      "Epoch 296/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4566 - acc: 0.7778 - val_loss: 0.5261 - val_acc: 0.7448\n",
      "Epoch 297/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4565 - acc: 0.7778 - val_loss: 0.5261 - val_acc: 0.7448\n",
      "Epoch 298/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4564 - acc: 0.7778 - val_loss: 0.5260 - val_acc: 0.7448\n",
      "Epoch 299/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4563 - acc: 0.7778 - val_loss: 0.5260 - val_acc: 0.7448\n",
      "Epoch 300/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4562 - acc: 0.7760 - val_loss: 0.5259 - val_acc: 0.7448\n",
      "Epoch 301/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4561 - acc: 0.7760 - val_loss: 0.5259 - val_acc: 0.7448\n",
      "Epoch 302/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4560 - acc: 0.7760 - val_loss: 0.5258 - val_acc: 0.7448\n",
      "Epoch 303/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4559 - acc: 0.7760 - val_loss: 0.5258 - val_acc: 0.7448\n",
      "Epoch 304/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4558 - acc: 0.7760 - val_loss: 0.5257 - val_acc: 0.7448\n",
      "Epoch 305/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4557 - acc: 0.7760 - val_loss: 0.5257 - val_acc: 0.7448\n",
      "Epoch 306/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4556 - acc: 0.7760 - val_loss: 0.5256 - val_acc: 0.7448\n",
      "Epoch 307/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4555 - acc: 0.7760 - val_loss: 0.5256 - val_acc: 0.7396\n",
      "Epoch 308/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4554 - acc: 0.7760 - val_loss: 0.5255 - val_acc: 0.7396\n",
      "Epoch 309/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4552 - acc: 0.7760 - val_loss: 0.5255 - val_acc: 0.7396\n",
      "Epoch 310/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4552 - acc: 0.7760 - val_loss: 0.5254 - val_acc: 0.7396\n",
      "Epoch 311/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4551 - acc: 0.7760 - val_loss: 0.5254 - val_acc: 0.7396\n",
      "Epoch 312/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4550 - acc: 0.7760 - val_loss: 0.5253 - val_acc: 0.7396\n",
      "Epoch 313/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4549 - acc: 0.7760 - val_loss: 0.5253 - val_acc: 0.7396\n",
      "Epoch 314/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4548 - acc: 0.7760 - val_loss: 0.5252 - val_acc: 0.7396\n",
      "Epoch 315/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4547 - acc: 0.7760 - val_loss: 0.5252 - val_acc: 0.7448\n",
      "Epoch 316/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4546 - acc: 0.7760 - val_loss: 0.5251 - val_acc: 0.7448\n",
      "Epoch 317/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4545 - acc: 0.7760 - val_loss: 0.5251 - val_acc: 0.7448\n",
      "Epoch 318/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4544 - acc: 0.7760 - val_loss: 0.5250 - val_acc: 0.7448\n",
      "Epoch 319/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4543 - acc: 0.7760 - val_loss: 0.5250 - val_acc: 0.7448\n",
      "Epoch 320/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4542 - acc: 0.7760 - val_loss: 0.5249 - val_acc: 0.7448\n",
      "Epoch 321/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4541 - acc: 0.7743 - val_loss: 0.5249 - val_acc: 0.7448\n",
      "Epoch 322/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4540 - acc: 0.7760 - val_loss: 0.5248 - val_acc: 0.7448\n",
      "Epoch 323/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4540 - acc: 0.7743 - val_loss: 0.5248 - val_acc: 0.7448\n",
      "Epoch 324/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4538 - acc: 0.7743 - val_loss: 0.5247 - val_acc: 0.7448\n",
      "Epoch 325/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4537 - acc: 0.7743 - val_loss: 0.5247 - val_acc: 0.7448\n",
      "Epoch 326/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4537 - acc: 0.7743 - val_loss: 0.5246 - val_acc: 0.7448\n",
      "Epoch 327/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4535 - acc: 0.7743 - val_loss: 0.5246 - val_acc: 0.7448\n",
      "Epoch 328/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4535 - acc: 0.7743 - val_loss: 0.5246 - val_acc: 0.7448\n",
      "Epoch 329/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4534 - acc: 0.7743 - val_loss: 0.5245 - val_acc: 0.7448\n",
      "Epoch 330/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4533 - acc: 0.7743 - val_loss: 0.5245 - val_acc: 0.7448\n",
      "Epoch 331/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4532 - acc: 0.7743 - val_loss: 0.5244 - val_acc: 0.7448\n",
      "Epoch 332/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4531 - acc: 0.7743 - val_loss: 0.5244 - val_acc: 0.7448\n",
      "Epoch 333/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4530 - acc: 0.7743 - val_loss: 0.5243 - val_acc: 0.7448\n",
      "Epoch 334/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4529 - acc: 0.7743 - val_loss: 0.5243 - val_acc: 0.7448\n",
      "Epoch 335/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4528 - acc: 0.7743 - val_loss: 0.5243 - val_acc: 0.7448\n",
      "Epoch 336/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4527 - acc: 0.7743 - val_loss: 0.5242 - val_acc: 0.7448\n",
      "Epoch 337/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4527 - acc: 0.7743 - val_loss: 0.5242 - val_acc: 0.7448\n",
      "Epoch 338/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4526 - acc: 0.7743 - val_loss: 0.5241 - val_acc: 0.7448\n",
      "Epoch 339/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4525 - acc: 0.7743 - val_loss: 0.5241 - val_acc: 0.7448\n",
      "Epoch 340/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4524 - acc: 0.7743 - val_loss: 0.5241 - val_acc: 0.7448\n",
      "Epoch 341/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4523 - acc: 0.7743 - val_loss: 0.5240 - val_acc: 0.7448\n",
      "Epoch 342/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4522 - acc: 0.7726 - val_loss: 0.5240 - val_acc: 0.7448\n",
      "Epoch 343/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4521 - acc: 0.7726 - val_loss: 0.5240 - val_acc: 0.7448\n",
      "Epoch 344/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4520 - acc: 0.7726 - val_loss: 0.5239 - val_acc: 0.7448\n",
      "Epoch 345/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4519 - acc: 0.7743 - val_loss: 0.5239 - val_acc: 0.7448\n",
      "Epoch 346/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4518 - acc: 0.7743 - val_loss: 0.5239 - val_acc: 0.7448\n",
      "Epoch 347/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4518 - acc: 0.7743 - val_loss: 0.5238 - val_acc: 0.7448\n",
      "Epoch 348/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4516 - acc: 0.7743 - val_loss: 0.5238 - val_acc: 0.7448\n",
      "Epoch 349/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4516 - acc: 0.7743 - val_loss: 0.5238 - val_acc: 0.7448\n",
      "Epoch 350/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4515 - acc: 0.7743 - val_loss: 0.5237 - val_acc: 0.7448\n",
      "Epoch 351/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4514 - acc: 0.7743 - val_loss: 0.5237 - val_acc: 0.7448\n",
      "Epoch 352/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4513 - acc: 0.7743 - val_loss: 0.5237 - val_acc: 0.7448\n",
      "Epoch 353/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4512 - acc: 0.7743 - val_loss: 0.5236 - val_acc: 0.7448\n",
      "Epoch 354/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4511 - acc: 0.7743 - val_loss: 0.5236 - val_acc: 0.7448\n",
      "Epoch 355/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4510 - acc: 0.7743 - val_loss: 0.5236 - val_acc: 0.7448\n",
      "Epoch 356/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4509 - acc: 0.7743 - val_loss: 0.5235 - val_acc: 0.7448\n",
      "Epoch 357/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4508 - acc: 0.7743 - val_loss: 0.5235 - val_acc: 0.7448\n",
      "Epoch 358/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4508 - acc: 0.7743 - val_loss: 0.5235 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 359/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4507 - acc: 0.7743 - val_loss: 0.5235 - val_acc: 0.7448\n",
      "Epoch 360/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4506 - acc: 0.7743 - val_loss: 0.5234 - val_acc: 0.7448\n",
      "Epoch 361/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4505 - acc: 0.7743 - val_loss: 0.5234 - val_acc: 0.7448\n",
      "Epoch 362/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4504 - acc: 0.7760 - val_loss: 0.5234 - val_acc: 0.7448\n",
      "Epoch 363/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4503 - acc: 0.7760 - val_loss: 0.5233 - val_acc: 0.7448\n",
      "Epoch 364/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4502 - acc: 0.7760 - val_loss: 0.5233 - val_acc: 0.7448\n",
      "Epoch 365/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4502 - acc: 0.7760 - val_loss: 0.5233 - val_acc: 0.7448\n",
      "Epoch 366/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4501 - acc: 0.7778 - val_loss: 0.5232 - val_acc: 0.7448\n",
      "Epoch 367/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4500 - acc: 0.7795 - val_loss: 0.5232 - val_acc: 0.7448\n",
      "Epoch 368/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4499 - acc: 0.7778 - val_loss: 0.5232 - val_acc: 0.7448\n",
      "Epoch 369/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4498 - acc: 0.7760 - val_loss: 0.5232 - val_acc: 0.7448\n",
      "Epoch 370/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4498 - acc: 0.7778 - val_loss: 0.5231 - val_acc: 0.7448\n",
      "Epoch 371/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4497 - acc: 0.7760 - val_loss: 0.5231 - val_acc: 0.7448\n",
      "Epoch 372/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4496 - acc: 0.7778 - val_loss: 0.5231 - val_acc: 0.7448\n",
      "Epoch 373/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4495 - acc: 0.7795 - val_loss: 0.5231 - val_acc: 0.7448\n",
      "Epoch 374/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4494 - acc: 0.7778 - val_loss: 0.5230 - val_acc: 0.7448\n",
      "Epoch 375/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4493 - acc: 0.7760 - val_loss: 0.5230 - val_acc: 0.7448\n",
      "Epoch 376/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4492 - acc: 0.7778 - val_loss: 0.5230 - val_acc: 0.7448\n",
      "Epoch 377/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4492 - acc: 0.7778 - val_loss: 0.5230 - val_acc: 0.7448\n",
      "Epoch 378/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4491 - acc: 0.7778 - val_loss: 0.5229 - val_acc: 0.7448\n",
      "Epoch 379/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4490 - acc: 0.7778 - val_loss: 0.5229 - val_acc: 0.7448\n",
      "Epoch 380/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4490 - acc: 0.7760 - val_loss: 0.5229 - val_acc: 0.7448\n",
      "Epoch 381/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4488 - acc: 0.7795 - val_loss: 0.5229 - val_acc: 0.7448\n",
      "Epoch 382/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4488 - acc: 0.7760 - val_loss: 0.5228 - val_acc: 0.7448\n",
      "Epoch 383/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4487 - acc: 0.7778 - val_loss: 0.5228 - val_acc: 0.7448\n",
      "Epoch 384/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4486 - acc: 0.7778 - val_loss: 0.5228 - val_acc: 0.7448\n",
      "Epoch 385/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4486 - acc: 0.7795 - val_loss: 0.5228 - val_acc: 0.7448\n",
      "Epoch 386/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4485 - acc: 0.7760 - val_loss: 0.5228 - val_acc: 0.7448\n",
      "Epoch 387/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4485 - acc: 0.7760 - val_loss: 0.5228 - val_acc: 0.7448\n",
      "Epoch 388/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4484 - acc: 0.7760 - val_loss: 0.5228 - val_acc: 0.7448\n",
      "Epoch 389/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4483 - acc: 0.7778 - val_loss: 0.5228 - val_acc: 0.7448\n",
      "Epoch 390/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4482 - acc: 0.7760 - val_loss: 0.5228 - val_acc: 0.7448\n",
      "Epoch 391/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4481 - acc: 0.7760 - val_loss: 0.5228 - val_acc: 0.7500\n",
      "Epoch 392/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4481 - acc: 0.7760 - val_loss: 0.5228 - val_acc: 0.7500\n",
      "Epoch 393/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4480 - acc: 0.7795 - val_loss: 0.5228 - val_acc: 0.7500\n",
      "Epoch 394/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4480 - acc: 0.7778 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 395/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4479 - acc: 0.7760 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 396/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4478 - acc: 0.7847 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 397/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4477 - acc: 0.7865 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 398/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4476 - acc: 0.7865 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 399/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4476 - acc: 0.7847 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 400/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4476 - acc: 0.7865 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 401/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4475 - acc: 0.7847 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 402/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4474 - acc: 0.7847 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 403/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4474 - acc: 0.7847 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 404/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4473 - acc: 0.7865 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 405/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4472 - acc: 0.7847 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 406/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4472 - acc: 0.7847 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 407/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4471 - acc: 0.7847 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 408/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4470 - acc: 0.7847 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 409/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4469 - acc: 0.7847 - val_loss: 0.5225 - val_acc: 0.7500\n",
      "Epoch 410/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4469 - acc: 0.7847 - val_loss: 0.5225 - val_acc: 0.7500\n",
      "Epoch 411/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4468 - acc: 0.7847 - val_loss: 0.5225 - val_acc: 0.7500\n",
      "Epoch 412/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4467 - acc: 0.7847 - val_loss: 0.5225 - val_acc: 0.7500\n",
      "Epoch 413/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4467 - acc: 0.7812 - val_loss: 0.5225 - val_acc: 0.7500\n",
      "Epoch 414/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4466 - acc: 0.7830 - val_loss: 0.5224 - val_acc: 0.7500\n",
      "Epoch 415/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4465 - acc: 0.7830 - val_loss: 0.5224 - val_acc: 0.7500\n",
      "Epoch 416/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4465 - acc: 0.7812 - val_loss: 0.5224 - val_acc: 0.7500\n",
      "Epoch 417/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4464 - acc: 0.7812 - val_loss: 0.5224 - val_acc: 0.7500\n",
      "Epoch 418/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4464 - acc: 0.7812 - val_loss: 0.5224 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 419/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4463 - acc: 0.7812 - val_loss: 0.5223 - val_acc: 0.7500\n",
      "Epoch 420/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4462 - acc: 0.7830 - val_loss: 0.5223 - val_acc: 0.7500\n",
      "Epoch 421/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4461 - acc: 0.7830 - val_loss: 0.5223 - val_acc: 0.7500\n",
      "Epoch 422/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4461 - acc: 0.7830 - val_loss: 0.5223 - val_acc: 0.7500\n",
      "Epoch 423/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4460 - acc: 0.7830 - val_loss: 0.5223 - val_acc: 0.7500\n",
      "Epoch 424/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4459 - acc: 0.7830 - val_loss: 0.5222 - val_acc: 0.7500\n",
      "Epoch 425/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4459 - acc: 0.7830 - val_loss: 0.5222 - val_acc: 0.7500\n",
      "Epoch 426/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4459 - acc: 0.7830 - val_loss: 0.5222 - val_acc: 0.7500\n",
      "Epoch 427/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4458 - acc: 0.7830 - val_loss: 0.5222 - val_acc: 0.7500\n",
      "Epoch 428/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4457 - acc: 0.7830 - val_loss: 0.5221 - val_acc: 0.7500\n",
      "Epoch 429/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4456 - acc: 0.7830 - val_loss: 0.5221 - val_acc: 0.7500\n",
      "Epoch 430/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4456 - acc: 0.7830 - val_loss: 0.5221 - val_acc: 0.7500\n",
      "Epoch 431/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4455 - acc: 0.7830 - val_loss: 0.5221 - val_acc: 0.7500\n",
      "Epoch 432/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4454 - acc: 0.7830 - val_loss: 0.5221 - val_acc: 0.7500\n",
      "Epoch 433/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4454 - acc: 0.7830 - val_loss: 0.5220 - val_acc: 0.7500\n",
      "Epoch 434/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4453 - acc: 0.7830 - val_loss: 0.5220 - val_acc: 0.7448\n",
      "Epoch 435/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4453 - acc: 0.7830 - val_loss: 0.5220 - val_acc: 0.7448\n",
      "Epoch 436/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4452 - acc: 0.7847 - val_loss: 0.5220 - val_acc: 0.7448\n",
      "Epoch 437/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4452 - acc: 0.7830 - val_loss: 0.5220 - val_acc: 0.7448\n",
      "Epoch 438/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4451 - acc: 0.7830 - val_loss: 0.5220 - val_acc: 0.7448\n",
      "Epoch 439/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4450 - acc: 0.7830 - val_loss: 0.5219 - val_acc: 0.7448\n",
      "Epoch 440/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4450 - acc: 0.7830 - val_loss: 0.5219 - val_acc: 0.7448\n",
      "Epoch 441/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4450 - acc: 0.7847 - val_loss: 0.5219 - val_acc: 0.7448\n",
      "Epoch 442/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4449 - acc: 0.7847 - val_loss: 0.5219 - val_acc: 0.7448\n",
      "Epoch 443/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4448 - acc: 0.7847 - val_loss: 0.5219 - val_acc: 0.7448\n",
      "Epoch 444/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4448 - acc: 0.7865 - val_loss: 0.5219 - val_acc: 0.7448\n",
      "Epoch 445/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4447 - acc: 0.7830 - val_loss: 0.5219 - val_acc: 0.7448\n",
      "Epoch 446/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4447 - acc: 0.7865 - val_loss: 0.5219 - val_acc: 0.7448\n",
      "Epoch 447/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4446 - acc: 0.7865 - val_loss: 0.5219 - val_acc: 0.7448\n",
      "Epoch 448/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4446 - acc: 0.7865 - val_loss: 0.5218 - val_acc: 0.7448\n",
      "Epoch 449/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4445 - acc: 0.7865 - val_loss: 0.5218 - val_acc: 0.7448\n",
      "Epoch 450/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4444 - acc: 0.7865 - val_loss: 0.5218 - val_acc: 0.7448\n",
      "Epoch 451/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4444 - acc: 0.7847 - val_loss: 0.5218 - val_acc: 0.7448\n",
      "Epoch 452/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4443 - acc: 0.7865 - val_loss: 0.5218 - val_acc: 0.7448\n",
      "Epoch 453/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4443 - acc: 0.7865 - val_loss: 0.5218 - val_acc: 0.7448\n",
      "Epoch 454/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4442 - acc: 0.7865 - val_loss: 0.5217 - val_acc: 0.7448\n",
      "Epoch 455/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4442 - acc: 0.7865 - val_loss: 0.5217 - val_acc: 0.7448\n",
      "Epoch 456/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4442 - acc: 0.7865 - val_loss: 0.5217 - val_acc: 0.7448\n",
      "Epoch 457/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4441 - acc: 0.7865 - val_loss: 0.5217 - val_acc: 0.7448\n",
      "Epoch 458/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4440 - acc: 0.7865 - val_loss: 0.5217 - val_acc: 0.7396\n",
      "Epoch 459/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4439 - acc: 0.7865 - val_loss: 0.5217 - val_acc: 0.7396\n",
      "Epoch 460/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4439 - acc: 0.7865 - val_loss: 0.5216 - val_acc: 0.7396\n",
      "Epoch 461/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4438 - acc: 0.7865 - val_loss: 0.5216 - val_acc: 0.7396\n",
      "Epoch 462/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4438 - acc: 0.7847 - val_loss: 0.5216 - val_acc: 0.7396\n",
      "Epoch 463/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4438 - acc: 0.7865 - val_loss: 0.5216 - val_acc: 0.7396\n",
      "Epoch 464/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4437 - acc: 0.7865 - val_loss: 0.5216 - val_acc: 0.7448\n",
      "Epoch 465/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4436 - acc: 0.7865 - val_loss: 0.5216 - val_acc: 0.7448\n",
      "Epoch 466/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4436 - acc: 0.7865 - val_loss: 0.5215 - val_acc: 0.7448\n",
      "Epoch 467/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4435 - acc: 0.7865 - val_loss: 0.5215 - val_acc: 0.7448\n",
      "Epoch 468/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4435 - acc: 0.7865 - val_loss: 0.5215 - val_acc: 0.7448\n",
      "Epoch 469/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4434 - acc: 0.7865 - val_loss: 0.5215 - val_acc: 0.7448\n",
      "Epoch 470/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4434 - acc: 0.7847 - val_loss: 0.5215 - val_acc: 0.7448\n",
      "Epoch 471/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4433 - acc: 0.7865 - val_loss: 0.5215 - val_acc: 0.7448\n",
      "Epoch 472/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4433 - acc: 0.7865 - val_loss: 0.5214 - val_acc: 0.7448\n",
      "Epoch 473/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4432 - acc: 0.7865 - val_loss: 0.5214 - val_acc: 0.7448\n",
      "Epoch 474/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4432 - acc: 0.7865 - val_loss: 0.5214 - val_acc: 0.7500\n",
      "Epoch 475/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4431 - acc: 0.7865 - val_loss: 0.5214 - val_acc: 0.7500\n",
      "Epoch 476/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4430 - acc: 0.7882 - val_loss: 0.5214 - val_acc: 0.7500\n",
      "Epoch 477/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4430 - acc: 0.7865 - val_loss: 0.5213 - val_acc: 0.7500\n",
      "Epoch 478/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4430 - acc: 0.7865 - val_loss: 0.5213 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 479/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4429 - acc: 0.7882 - val_loss: 0.5213 - val_acc: 0.7500\n",
      "Epoch 480/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4429 - acc: 0.7865 - val_loss: 0.5213 - val_acc: 0.7500\n",
      "Epoch 481/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4428 - acc: 0.7865 - val_loss: 0.5213 - val_acc: 0.7500\n",
      "Epoch 482/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4428 - acc: 0.7882 - val_loss: 0.5213 - val_acc: 0.7500\n",
      "Epoch 483/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4427 - acc: 0.7847 - val_loss: 0.5212 - val_acc: 0.7500\n",
      "Epoch 484/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4427 - acc: 0.7847 - val_loss: 0.5212 - val_acc: 0.7500\n",
      "Epoch 485/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4426 - acc: 0.7847 - val_loss: 0.5212 - val_acc: 0.7500\n",
      "Epoch 486/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4426 - acc: 0.7865 - val_loss: 0.5212 - val_acc: 0.7500\n",
      "Epoch 487/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4425 - acc: 0.7865 - val_loss: 0.5212 - val_acc: 0.7500\n",
      "Epoch 488/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4425 - acc: 0.7865 - val_loss: 0.5212 - val_acc: 0.7500\n",
      "Epoch 489/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4424 - acc: 0.7865 - val_loss: 0.5212 - val_acc: 0.7500\n",
      "Epoch 490/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4423 - acc: 0.7882 - val_loss: 0.5211 - val_acc: 0.7500\n",
      "Epoch 491/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4423 - acc: 0.7865 - val_loss: 0.5211 - val_acc: 0.7500\n",
      "Epoch 492/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4423 - acc: 0.7865 - val_loss: 0.5211 - val_acc: 0.7500\n",
      "Epoch 493/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4423 - acc: 0.7865 - val_loss: 0.5211 - val_acc: 0.7500\n",
      "Epoch 494/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4422 - acc: 0.7865 - val_loss: 0.5211 - val_acc: 0.7500\n",
      "Epoch 495/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4421 - acc: 0.7865 - val_loss: 0.5211 - val_acc: 0.7500\n",
      "Epoch 496/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4421 - acc: 0.7865 - val_loss: 0.5211 - val_acc: 0.7500\n",
      "Epoch 497/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4420 - acc: 0.7882 - val_loss: 0.5211 - val_acc: 0.7500\n",
      "Epoch 498/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4420 - acc: 0.7865 - val_loss: 0.5211 - val_acc: 0.7500\n",
      "Epoch 499/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4419 - acc: 0.7865 - val_loss: 0.5210 - val_acc: 0.7500\n",
      "Epoch 500/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4419 - acc: 0.7865 - val_loss: 0.5210 - val_acc: 0.7500\n",
      "Epoch 501/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4418 - acc: 0.7865 - val_loss: 0.5210 - val_acc: 0.7500\n",
      "Epoch 502/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4418 - acc: 0.7865 - val_loss: 0.5210 - val_acc: 0.7500\n",
      "Epoch 503/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4417 - acc: 0.7865 - val_loss: 0.5210 - val_acc: 0.7500\n",
      "Epoch 504/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4417 - acc: 0.7865 - val_loss: 0.5210 - val_acc: 0.7500\n",
      "Epoch 505/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4416 - acc: 0.7847 - val_loss: 0.5210 - val_acc: 0.7500\n",
      "Epoch 506/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4416 - acc: 0.7847 - val_loss: 0.5210 - val_acc: 0.7500\n",
      "Epoch 507/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4415 - acc: 0.7847 - val_loss: 0.5209 - val_acc: 0.7500\n",
      "Epoch 508/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4415 - acc: 0.7865 - val_loss: 0.5209 - val_acc: 0.7500\n",
      "Epoch 509/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4414 - acc: 0.7847 - val_loss: 0.5209 - val_acc: 0.7500\n",
      "Epoch 510/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4414 - acc: 0.7865 - val_loss: 0.5209 - val_acc: 0.7500\n",
      "Epoch 511/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4413 - acc: 0.7865 - val_loss: 0.5209 - val_acc: 0.7500\n",
      "Epoch 512/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4413 - acc: 0.7882 - val_loss: 0.5209 - val_acc: 0.7448\n",
      "Epoch 513/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4412 - acc: 0.7847 - val_loss: 0.5208 - val_acc: 0.7448\n",
      "Epoch 514/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4411 - acc: 0.7865 - val_loss: 0.5208 - val_acc: 0.7448\n",
      "Epoch 515/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4411 - acc: 0.7847 - val_loss: 0.5208 - val_acc: 0.7448\n",
      "Epoch 516/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4410 - acc: 0.7847 - val_loss: 0.5208 - val_acc: 0.7448\n",
      "Epoch 517/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4410 - acc: 0.7865 - val_loss: 0.5208 - val_acc: 0.7448\n",
      "Epoch 518/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4409 - acc: 0.7865 - val_loss: 0.5207 - val_acc: 0.7448\n",
      "Epoch 519/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4409 - acc: 0.7865 - val_loss: 0.5207 - val_acc: 0.7448\n",
      "Epoch 520/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4408 - acc: 0.7865 - val_loss: 0.5207 - val_acc: 0.7448\n",
      "Epoch 521/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4408 - acc: 0.7847 - val_loss: 0.5207 - val_acc: 0.7448\n",
      "Epoch 522/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4407 - acc: 0.7865 - val_loss: 0.5207 - val_acc: 0.7448\n",
      "Epoch 523/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4407 - acc: 0.7865 - val_loss: 0.5207 - val_acc: 0.7448\n",
      "Epoch 524/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4406 - acc: 0.7847 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 525/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4406 - acc: 0.7865 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 526/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4405 - acc: 0.7882 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 527/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4405 - acc: 0.7865 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 528/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4404 - acc: 0.7865 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 529/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4404 - acc: 0.7899 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 530/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4403 - acc: 0.7882 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 531/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4403 - acc: 0.7865 - val_loss: 0.5205 - val_acc: 0.7500\n",
      "Epoch 532/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4402 - acc: 0.7865 - val_loss: 0.5205 - val_acc: 0.7500\n",
      "Epoch 533/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4402 - acc: 0.7882 - val_loss: 0.5205 - val_acc: 0.7500\n",
      "Epoch 534/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4401 - acc: 0.7865 - val_loss: 0.5205 - val_acc: 0.7500\n",
      "Epoch 535/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4401 - acc: 0.7882 - val_loss: 0.5205 - val_acc: 0.7500\n",
      "Epoch 536/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4400 - acc: 0.7882 - val_loss: 0.5205 - val_acc: 0.7500\n",
      "Epoch 537/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4400 - acc: 0.7865 - val_loss: 0.5205 - val_acc: 0.7500\n",
      "Epoch 538/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4400 - acc: 0.7882 - val_loss: 0.5205 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 539/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4399 - acc: 0.7865 - val_loss: 0.5205 - val_acc: 0.7500\n",
      "Epoch 540/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4398 - acc: 0.7882 - val_loss: 0.5204 - val_acc: 0.7500\n",
      "Epoch 541/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4398 - acc: 0.7882 - val_loss: 0.5204 - val_acc: 0.7500\n",
      "Epoch 542/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4397 - acc: 0.7882 - val_loss: 0.5204 - val_acc: 0.7500\n",
      "Epoch 543/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4397 - acc: 0.7899 - val_loss: 0.5204 - val_acc: 0.7500\n",
      "Epoch 544/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4396 - acc: 0.7882 - val_loss: 0.5204 - val_acc: 0.7500\n",
      "Epoch 545/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4396 - acc: 0.7899 - val_loss: 0.5204 - val_acc: 0.7500\n",
      "Epoch 546/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4396 - acc: 0.7899 - val_loss: 0.5204 - val_acc: 0.7500\n",
      "Epoch 547/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4395 - acc: 0.7882 - val_loss: 0.5204 - val_acc: 0.7500\n",
      "Epoch 548/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4394 - acc: 0.7882 - val_loss: 0.5204 - val_acc: 0.7500\n",
      "Epoch 549/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4394 - acc: 0.7899 - val_loss: 0.5203 - val_acc: 0.7500\n",
      "Epoch 550/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4394 - acc: 0.7899 - val_loss: 0.5203 - val_acc: 0.7500\n",
      "Epoch 551/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4394 - acc: 0.7899 - val_loss: 0.5203 - val_acc: 0.7500\n",
      "Epoch 552/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4393 - acc: 0.7882 - val_loss: 0.5203 - val_acc: 0.7500\n",
      "Epoch 553/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4392 - acc: 0.7899 - val_loss: 0.5203 - val_acc: 0.7500\n",
      "Epoch 554/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4392 - acc: 0.7899 - val_loss: 0.5203 - val_acc: 0.7500\n",
      "Epoch 555/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4391 - acc: 0.7899 - val_loss: 0.5203 - val_acc: 0.7500\n",
      "Epoch 556/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4391 - acc: 0.7882 - val_loss: 0.5202 - val_acc: 0.7500\n",
      "Epoch 557/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4390 - acc: 0.7899 - val_loss: 0.5202 - val_acc: 0.7500\n",
      "Epoch 558/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4390 - acc: 0.7899 - val_loss: 0.5202 - val_acc: 0.7500\n",
      "Epoch 559/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4389 - acc: 0.7899 - val_loss: 0.5202 - val_acc: 0.7500\n",
      "Epoch 560/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4389 - acc: 0.7899 - val_loss: 0.5202 - val_acc: 0.7500\n",
      "Epoch 561/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4388 - acc: 0.7899 - val_loss: 0.5202 - val_acc: 0.7500\n",
      "Epoch 562/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4388 - acc: 0.7882 - val_loss: 0.5202 - val_acc: 0.7500\n",
      "Epoch 563/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4387 - acc: 0.7917 - val_loss: 0.5202 - val_acc: 0.7500\n",
      "Epoch 564/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4387 - acc: 0.7882 - val_loss: 0.5202 - val_acc: 0.7500\n",
      "Epoch 565/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4386 - acc: 0.7899 - val_loss: 0.5202 - val_acc: 0.7500\n",
      "Epoch 566/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4386 - acc: 0.7917 - val_loss: 0.5202 - val_acc: 0.7500\n",
      "Epoch 567/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4386 - acc: 0.7917 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 568/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4385 - acc: 0.7917 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 569/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4385 - acc: 0.7917 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 570/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4384 - acc: 0.7917 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 571/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4384 - acc: 0.7917 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 572/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4383 - acc: 0.7934 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 573/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4383 - acc: 0.7934 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 574/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4382 - acc: 0.7969 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 575/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4382 - acc: 0.7934 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 576/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4381 - acc: 0.7951 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 577/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4381 - acc: 0.7969 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 578/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4381 - acc: 0.7934 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 579/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4380 - acc: 0.7986 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 580/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4379 - acc: 0.7969 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 581/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4379 - acc: 0.7969 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 582/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4379 - acc: 0.7969 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 583/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4378 - acc: 0.7969 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 584/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4378 - acc: 0.7969 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 585/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4934 - acc: 0.750 - 0s 29us/step - loss: 0.4377 - acc: 0.7969 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 586/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4377 - acc: 0.7986 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 587/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4377 - acc: 0.7986 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 588/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4376 - acc: 0.7969 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 589/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4376 - acc: 0.7969 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 590/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4375 - acc: 0.7969 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 591/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4375 - acc: 0.7986 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 592/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4374 - acc: 0.7969 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 593/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4374 - acc: 0.7986 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 594/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4373 - acc: 0.7986 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 595/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4373 - acc: 0.7986 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 596/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4373 - acc: 0.8003 - val_loss: 0.5201 - val_acc: 0.7500\n",
      "Epoch 597/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4372 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 598/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4372 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 599/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4371 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 600/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4370 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 601/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4370 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 602/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4370 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 603/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4370 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 604/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4369 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 605/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4368 - acc: 0.8021 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 606/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4368 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 607/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4368 - acc: 0.8021 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 608/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4367 - acc: 0.8021 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 609/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4367 - acc: 0.8021 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 610/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4366 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 611/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4366 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 612/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4366 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 613/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4366 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 614/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4364 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 615/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4364 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 616/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4364 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 617/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4363 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 618/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4363 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 619/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4362 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 620/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4362 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 621/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4362 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 622/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4361 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 623/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4360 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 624/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4360 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 625/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4360 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 626/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4360 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 627/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4359 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 628/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4359 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 629/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4358 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 630/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4358 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 631/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4357 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 632/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4357 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 633/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4357 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 634/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4356 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 635/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4356 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 636/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4355 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 637/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4355 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 638/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4355 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 639/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4354 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 640/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4354 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 641/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4354 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 642/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4353 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 643/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4353 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 644/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4353 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 645/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4352 - acc: 0.7986 - val_loss: 0.5200 - val_acc: 0.7448\n",
      "Epoch 646/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4352 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7448\n",
      "Epoch 647/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4351 - acc: 0.8003 - val_loss: 0.5200 - val_acc: 0.7448\n",
      "Epoch 648/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4351 - acc: 0.7986 - val_loss: 0.5200 - val_acc: 0.7448\n",
      "Epoch 649/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4351 - acc: 0.7986 - val_loss: 0.5200 - val_acc: 0.7448\n",
      "Epoch 650/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4350 - acc: 0.7986 - val_loss: 0.5200 - val_acc: 0.7448\n",
      "Epoch 651/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4350 - acc: 0.8003 - val_loss: 0.5201 - val_acc: 0.7448\n",
      "Epoch 652/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4350 - acc: 0.8003 - val_loss: 0.5201 - val_acc: 0.7448\n",
      "Epoch 653/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4350 - acc: 0.8003 - val_loss: 0.5201 - val_acc: 0.7448\n",
      "Epoch 654/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4349 - acc: 0.8003 - val_loss: 0.5201 - val_acc: 0.7448\n",
      "Epoch 655/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4349 - acc: 0.8003 - val_loss: 0.5201 - val_acc: 0.7448\n",
      "Epoch 656/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4348 - acc: 0.7986 - val_loss: 0.5201 - val_acc: 0.7448\n",
      "Epoch 657/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4348 - acc: 0.7969 - val_loss: 0.5201 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 658/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4348 - acc: 0.7986 - val_loss: 0.5201 - val_acc: 0.7448\n",
      "Epoch 659/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4347 - acc: 0.7986 - val_loss: 0.5201 - val_acc: 0.7448\n",
      "Epoch 660/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4347 - acc: 0.7986 - val_loss: 0.5201 - val_acc: 0.7448\n",
      "Epoch 661/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4347 - acc: 0.7986 - val_loss: 0.5201 - val_acc: 0.7448\n",
      "Epoch 662/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4347 - acc: 0.7986 - val_loss: 0.5201 - val_acc: 0.7448\n",
      "Epoch 663/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4346 - acc: 0.8003 - val_loss: 0.5201 - val_acc: 0.7448\n",
      "Epoch 664/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4346 - acc: 0.7986 - val_loss: 0.5201 - val_acc: 0.7448\n",
      "Epoch 665/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4346 - acc: 0.7986 - val_loss: 0.5201 - val_acc: 0.7448\n",
      "Epoch 666/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4345 - acc: 0.7969 - val_loss: 0.5202 - val_acc: 0.7448\n",
      "Epoch 667/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4345 - acc: 0.7986 - val_loss: 0.5202 - val_acc: 0.7448\n",
      "Epoch 668/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4345 - acc: 0.7986 - val_loss: 0.5202 - val_acc: 0.7448\n",
      "Epoch 669/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4345 - acc: 0.7986 - val_loss: 0.5202 - val_acc: 0.7448\n",
      "Epoch 670/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4344 - acc: 0.7969 - val_loss: 0.5202 - val_acc: 0.7448\n",
      "Epoch 671/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4343 - acc: 0.7986 - val_loss: 0.5202 - val_acc: 0.7448\n",
      "Epoch 672/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4343 - acc: 0.7986 - val_loss: 0.5202 - val_acc: 0.7448\n",
      "Epoch 673/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4343 - acc: 0.7986 - val_loss: 0.5202 - val_acc: 0.7448\n",
      "Epoch 674/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4343 - acc: 0.7986 - val_loss: 0.5202 - val_acc: 0.7448\n",
      "Epoch 675/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4342 - acc: 0.7986 - val_loss: 0.5202 - val_acc: 0.7448\n",
      "Epoch 676/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4342 - acc: 0.7986 - val_loss: 0.5202 - val_acc: 0.7448\n",
      "Epoch 677/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4342 - acc: 0.7986 - val_loss: 0.5202 - val_acc: 0.7448\n",
      "Epoch 678/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4342 - acc: 0.7969 - val_loss: 0.5202 - val_acc: 0.7448\n",
      "Epoch 679/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4341 - acc: 0.7986 - val_loss: 0.5202 - val_acc: 0.7448\n",
      "Epoch 680/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4341 - acc: 0.7986 - val_loss: 0.5202 - val_acc: 0.7448\n",
      "Epoch 681/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4340 - acc: 0.7986 - val_loss: 0.5203 - val_acc: 0.7448\n",
      "Epoch 682/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4340 - acc: 0.7986 - val_loss: 0.5203 - val_acc: 0.7448\n",
      "Epoch 683/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4340 - acc: 0.7986 - val_loss: 0.5203 - val_acc: 0.7448\n",
      "Epoch 684/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4340 - acc: 0.7986 - val_loss: 0.5203 - val_acc: 0.7448\n",
      "Epoch 685/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4339 - acc: 0.7986 - val_loss: 0.5203 - val_acc: 0.7448\n",
      "Epoch 686/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4339 - acc: 0.7969 - val_loss: 0.5203 - val_acc: 0.7448\n",
      "Epoch 687/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4149 - acc: 0.843 - 0s 31us/step - loss: 0.4339 - acc: 0.7986 - val_loss: 0.5203 - val_acc: 0.7448\n",
      "Epoch 688/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4338 - acc: 0.7986 - val_loss: 0.5203 - val_acc: 0.7448\n",
      "Epoch 689/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4338 - acc: 0.7986 - val_loss: 0.5203 - val_acc: 0.7448\n",
      "Epoch 690/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4338 - acc: 0.7986 - val_loss: 0.5203 - val_acc: 0.7448\n",
      "Epoch 691/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4338 - acc: 0.7986 - val_loss: 0.5203 - val_acc: 0.7448\n",
      "Epoch 692/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4337 - acc: 0.7986 - val_loss: 0.5203 - val_acc: 0.7448\n",
      "Epoch 693/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4337 - acc: 0.7986 - val_loss: 0.5203 - val_acc: 0.7448\n",
      "Epoch 694/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4336 - acc: 0.7986 - val_loss: 0.5203 - val_acc: 0.7448\n",
      "Epoch 695/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4336 - acc: 0.7986 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 696/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4336 - acc: 0.7986 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 697/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4336 - acc: 0.7986 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 698/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4335 - acc: 0.7986 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 699/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4335 - acc: 0.8003 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 700/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4335 - acc: 0.8003 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 701/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4335 - acc: 0.8003 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 702/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4334 - acc: 0.8003 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 703/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4334 - acc: 0.8003 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 704/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4334 - acc: 0.8003 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 705/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4334 - acc: 0.8003 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 706/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4334 - acc: 0.8003 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 707/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4333 - acc: 0.8003 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 708/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4333 - acc: 0.8003 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 709/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4332 - acc: 0.8003 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 710/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4332 - acc: 0.8003 - val_loss: 0.5204 - val_acc: 0.7448\n",
      "Epoch 711/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4332 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 712/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4332 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 713/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4331 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 714/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4331 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 715/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4330 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 716/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4330 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 717/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4330 - acc: 0.8021 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 718/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4330 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 719/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4329 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 720/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4329 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 721/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4329 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 722/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4328 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 723/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4328 - acc: 0.8021 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 724/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4328 - acc: 0.8021 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 725/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4327 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 726/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4327 - acc: 0.8021 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 727/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4327 - acc: 0.8021 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 728/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4327 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 729/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4326 - acc: 0.8021 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 730/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4326 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 731/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4325 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 732/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4326 - acc: 0.8021 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 733/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4325 - acc: 0.8021 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 734/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4325 - acc: 0.8021 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 735/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4324 - acc: 0.8003 - val_loss: 0.5205 - val_acc: 0.7448\n",
      "Epoch 736/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4324 - acc: 0.8003 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 737/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4324 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 738/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4324 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 739/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4323 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 740/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4323 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 741/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4323 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 742/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4322 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 743/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4322 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 744/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4321 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 745/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4322 - acc: 0.8038 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 746/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4321 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 747/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4321 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 748/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4321 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 749/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4320 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 750/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4320 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 751/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4320 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 752/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4319 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 753/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4319 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 754/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4319 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 755/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4318 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 756/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4318 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 757/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4318 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 758/1500\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.4318 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 759/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4317 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 760/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4317 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 761/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4317 - acc: 0.8038 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 762/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4316 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 763/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4316 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 764/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4316 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 765/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4316 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 766/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4315 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 767/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4315 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 768/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4315 - acc: 0.8038 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 769/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4314 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 770/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4314 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 771/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4314 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 772/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4313 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 773/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4313 - acc: 0.8021 - val_loss: 0.5206 - val_acc: 0.7448\n",
      "Epoch 774/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4313 - acc: 0.8021 - val_loss: 0.5207 - val_acc: 0.7448\n",
      "Epoch 775/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4313 - acc: 0.8021 - val_loss: 0.5207 - val_acc: 0.7448\n",
      "Epoch 776/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4312 - acc: 0.8038 - val_loss: 0.5207 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 777/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4312 - acc: 0.8021 - val_loss: 0.5207 - val_acc: 0.7448\n",
      "Epoch 778/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4311 - acc: 0.8021 - val_loss: 0.5207 - val_acc: 0.7448\n",
      "Epoch 779/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4311 - acc: 0.8021 - val_loss: 0.5207 - val_acc: 0.7448\n",
      "Epoch 780/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4311 - acc: 0.8021 - val_loss: 0.5207 - val_acc: 0.7448\n",
      "Epoch 781/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4311 - acc: 0.8038 - val_loss: 0.5207 - val_acc: 0.7448\n",
      "Epoch 782/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4311 - acc: 0.8021 - val_loss: 0.5208 - val_acc: 0.7448\n",
      "Epoch 783/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4310 - acc: 0.8021 - val_loss: 0.5208 - val_acc: 0.7448\n",
      "Epoch 784/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4310 - acc: 0.8038 - val_loss: 0.5208 - val_acc: 0.7448\n",
      "Epoch 785/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4309 - acc: 0.8038 - val_loss: 0.5208 - val_acc: 0.7448\n",
      "Epoch 786/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4309 - acc: 0.8021 - val_loss: 0.5208 - val_acc: 0.7448\n",
      "Epoch 787/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4309 - acc: 0.8021 - val_loss: 0.5208 - val_acc: 0.7448\n",
      "Epoch 788/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4308 - acc: 0.8038 - val_loss: 0.5208 - val_acc: 0.7448\n",
      "Epoch 789/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4308 - acc: 0.8038 - val_loss: 0.5208 - val_acc: 0.7448\n",
      "Epoch 790/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4308 - acc: 0.8021 - val_loss: 0.5209 - val_acc: 0.7448\n",
      "Epoch 791/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4308 - acc: 0.8021 - val_loss: 0.5209 - val_acc: 0.7448\n",
      "Epoch 792/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4308 - acc: 0.8038 - val_loss: 0.5209 - val_acc: 0.7448\n",
      "Epoch 793/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4308 - acc: 0.8038 - val_loss: 0.5209 - val_acc: 0.7448\n",
      "Epoch 794/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4307 - acc: 0.8021 - val_loss: 0.5209 - val_acc: 0.7448\n",
      "Epoch 795/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4307 - acc: 0.8038 - val_loss: 0.5209 - val_acc: 0.7448\n",
      "Epoch 796/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4307 - acc: 0.8038 - val_loss: 0.5209 - val_acc: 0.7448\n",
      "Epoch 797/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4306 - acc: 0.8038 - val_loss: 0.5209 - val_acc: 0.7448\n",
      "Epoch 798/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4305 - acc: 0.8038 - val_loss: 0.5210 - val_acc: 0.7448\n",
      "Epoch 799/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4305 - acc: 0.8038 - val_loss: 0.5210 - val_acc: 0.7448\n",
      "Epoch 800/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4305 - acc: 0.8038 - val_loss: 0.5210 - val_acc: 0.7448\n",
      "Epoch 801/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4305 - acc: 0.8021 - val_loss: 0.5210 - val_acc: 0.7448\n",
      "Epoch 802/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4305 - acc: 0.8038 - val_loss: 0.5210 - val_acc: 0.7448\n",
      "Epoch 803/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4304 - acc: 0.8038 - val_loss: 0.5210 - val_acc: 0.7448\n",
      "Epoch 804/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4304 - acc: 0.8038 - val_loss: 0.5211 - val_acc: 0.7448\n",
      "Epoch 805/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4304 - acc: 0.8038 - val_loss: 0.5211 - val_acc: 0.7448\n",
      "Epoch 806/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4304 - acc: 0.8038 - val_loss: 0.5211 - val_acc: 0.7448\n",
      "Epoch 807/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4303 - acc: 0.8038 - val_loss: 0.5211 - val_acc: 0.7448\n",
      "Epoch 808/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4302 - acc: 0.8038 - val_loss: 0.5212 - val_acc: 0.7448\n",
      "Epoch 809/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4302 - acc: 0.8038 - val_loss: 0.5212 - val_acc: 0.7448\n",
      "Epoch 810/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4302 - acc: 0.8038 - val_loss: 0.5212 - val_acc: 0.7448\n",
      "Epoch 811/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4302 - acc: 0.8038 - val_loss: 0.5212 - val_acc: 0.7448\n",
      "Epoch 812/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4302 - acc: 0.8038 - val_loss: 0.5212 - val_acc: 0.7448\n",
      "Epoch 813/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4301 - acc: 0.8038 - val_loss: 0.5213 - val_acc: 0.7448\n",
      "Epoch 814/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4301 - acc: 0.8038 - val_loss: 0.5213 - val_acc: 0.7448\n",
      "Epoch 815/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4301 - acc: 0.8038 - val_loss: 0.5213 - val_acc: 0.7448\n",
      "Epoch 816/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4301 - acc: 0.8038 - val_loss: 0.5213 - val_acc: 0.7448\n",
      "Epoch 817/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4300 - acc: 0.8038 - val_loss: 0.5213 - val_acc: 0.7448\n",
      "Epoch 818/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4300 - acc: 0.8038 - val_loss: 0.5214 - val_acc: 0.7448\n",
      "Epoch 819/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4300 - acc: 0.8038 - val_loss: 0.5214 - val_acc: 0.7448\n",
      "Epoch 820/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4300 - acc: 0.8038 - val_loss: 0.5214 - val_acc: 0.7448\n",
      "Epoch 821/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4299 - acc: 0.8038 - val_loss: 0.5214 - val_acc: 0.7448\n",
      "Epoch 822/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4299 - acc: 0.8038 - val_loss: 0.5214 - val_acc: 0.7448\n",
      "Epoch 823/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4298 - acc: 0.8038 - val_loss: 0.5215 - val_acc: 0.7448\n",
      "Epoch 824/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4298 - acc: 0.8038 - val_loss: 0.5215 - val_acc: 0.7448\n",
      "Epoch 825/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4298 - acc: 0.8038 - val_loss: 0.5215 - val_acc: 0.7448\n",
      "Epoch 826/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4298 - acc: 0.8038 - val_loss: 0.5215 - val_acc: 0.7448\n",
      "Epoch 827/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4297 - acc: 0.8038 - val_loss: 0.5215 - val_acc: 0.7448\n",
      "Epoch 828/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4297 - acc: 0.8038 - val_loss: 0.5215 - val_acc: 0.7448\n",
      "Epoch 829/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4297 - acc: 0.8038 - val_loss: 0.5216 - val_acc: 0.7448\n",
      "Epoch 830/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4296 - acc: 0.8038 - val_loss: 0.5216 - val_acc: 0.7448\n",
      "Epoch 831/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4296 - acc: 0.8038 - val_loss: 0.5216 - val_acc: 0.7448\n",
      "Epoch 832/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4296 - acc: 0.8038 - val_loss: 0.5216 - val_acc: 0.7448\n",
      "Epoch 833/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4296 - acc: 0.8038 - val_loss: 0.5216 - val_acc: 0.7448\n",
      "Epoch 834/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4295 - acc: 0.8038 - val_loss: 0.5216 - val_acc: 0.7448\n",
      "Epoch 835/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4295 - acc: 0.8038 - val_loss: 0.5216 - val_acc: 0.7500\n",
      "Epoch 836/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4295 - acc: 0.8038 - val_loss: 0.5217 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 837/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4295 - acc: 0.8038 - val_loss: 0.5217 - val_acc: 0.7500\n",
      "Epoch 838/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4294 - acc: 0.8038 - val_loss: 0.5217 - val_acc: 0.7500\n",
      "Epoch 839/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4294 - acc: 0.8038 - val_loss: 0.5217 - val_acc: 0.7500\n",
      "Epoch 840/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4294 - acc: 0.8038 - val_loss: 0.5217 - val_acc: 0.7500\n",
      "Epoch 841/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4293 - acc: 0.8038 - val_loss: 0.5217 - val_acc: 0.7500\n",
      "Epoch 842/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4293 - acc: 0.8038 - val_loss: 0.5217 - val_acc: 0.7500\n",
      "Epoch 843/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4293 - acc: 0.8038 - val_loss: 0.5218 - val_acc: 0.7500\n",
      "Epoch 844/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4292 - acc: 0.8038 - val_loss: 0.5218 - val_acc: 0.7500\n",
      "Epoch 845/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4293 - acc: 0.8038 - val_loss: 0.5218 - val_acc: 0.7500\n",
      "Epoch 846/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4292 - acc: 0.8038 - val_loss: 0.5218 - val_acc: 0.7500\n",
      "Epoch 847/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4292 - acc: 0.8038 - val_loss: 0.5218 - val_acc: 0.7500\n",
      "Epoch 848/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4291 - acc: 0.8038 - val_loss: 0.5219 - val_acc: 0.7500\n",
      "Epoch 849/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4291 - acc: 0.8038 - val_loss: 0.5219 - val_acc: 0.7500\n",
      "Epoch 850/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4291 - acc: 0.8038 - val_loss: 0.5219 - val_acc: 0.7500\n",
      "Epoch 851/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4291 - acc: 0.8038 - val_loss: 0.5219 - val_acc: 0.7500\n",
      "Epoch 852/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4290 - acc: 0.8038 - val_loss: 0.5219 - val_acc: 0.7500\n",
      "Epoch 853/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4290 - acc: 0.8038 - val_loss: 0.5220 - val_acc: 0.7500\n",
      "Epoch 854/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4290 - acc: 0.8038 - val_loss: 0.5220 - val_acc: 0.7500\n",
      "Epoch 855/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4290 - acc: 0.8038 - val_loss: 0.5220 - val_acc: 0.7500\n",
      "Epoch 856/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4289 - acc: 0.8038 - val_loss: 0.5220 - val_acc: 0.7500\n",
      "Epoch 857/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4289 - acc: 0.8038 - val_loss: 0.5220 - val_acc: 0.7500\n",
      "Epoch 858/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4289 - acc: 0.8038 - val_loss: 0.5220 - val_acc: 0.7500\n",
      "Epoch 859/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4288 - acc: 0.8038 - val_loss: 0.5221 - val_acc: 0.7500\n",
      "Epoch 860/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4288 - acc: 0.8038 - val_loss: 0.5221 - val_acc: 0.7500\n",
      "Epoch 861/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4288 - acc: 0.8038 - val_loss: 0.5221 - val_acc: 0.7500\n",
      "Epoch 862/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4288 - acc: 0.8038 - val_loss: 0.5221 - val_acc: 0.7500\n",
      "Epoch 863/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4287 - acc: 0.8038 - val_loss: 0.5221 - val_acc: 0.7500\n",
      "Epoch 864/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4288 - acc: 0.8038 - val_loss: 0.5221 - val_acc: 0.7500\n",
      "Epoch 865/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4287 - acc: 0.8038 - val_loss: 0.5222 - val_acc: 0.7500\n",
      "Epoch 866/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4286 - acc: 0.8038 - val_loss: 0.5222 - val_acc: 0.7500\n",
      "Epoch 867/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4286 - acc: 0.8038 - val_loss: 0.5222 - val_acc: 0.7500\n",
      "Epoch 868/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4286 - acc: 0.8038 - val_loss: 0.5222 - val_acc: 0.7500\n",
      "Epoch 869/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4286 - acc: 0.8038 - val_loss: 0.5222 - val_acc: 0.7500\n",
      "Epoch 870/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4286 - acc: 0.8038 - val_loss: 0.5222 - val_acc: 0.7500\n",
      "Epoch 871/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4286 - acc: 0.8038 - val_loss: 0.5223 - val_acc: 0.7500\n",
      "Epoch 872/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4285 - acc: 0.8038 - val_loss: 0.5223 - val_acc: 0.7500\n",
      "Epoch 873/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4285 - acc: 0.8038 - val_loss: 0.5223 - val_acc: 0.7500\n",
      "Epoch 874/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4285 - acc: 0.8038 - val_loss: 0.5223 - val_acc: 0.7500\n",
      "Epoch 875/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4285 - acc: 0.8038 - val_loss: 0.5223 - val_acc: 0.7500\n",
      "Epoch 876/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4284 - acc: 0.8038 - val_loss: 0.5224 - val_acc: 0.7500\n",
      "Epoch 877/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4284 - acc: 0.8038 - val_loss: 0.5224 - val_acc: 0.7500\n",
      "Epoch 878/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4284 - acc: 0.8038 - val_loss: 0.5224 - val_acc: 0.7500\n",
      "Epoch 879/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4283 - acc: 0.8038 - val_loss: 0.5224 - val_acc: 0.7500\n",
      "Epoch 880/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4283 - acc: 0.8038 - val_loss: 0.5224 - val_acc: 0.7500\n",
      "Epoch 881/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4283 - acc: 0.8038 - val_loss: 0.5225 - val_acc: 0.7500\n",
      "Epoch 882/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4282 - acc: 0.8038 - val_loss: 0.5225 - val_acc: 0.7500\n",
      "Epoch 883/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4282 - acc: 0.8038 - val_loss: 0.5225 - val_acc: 0.7500\n",
      "Epoch 884/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4282 - acc: 0.8038 - val_loss: 0.5225 - val_acc: 0.7500\n",
      "Epoch 885/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4281 - acc: 0.8038 - val_loss: 0.5225 - val_acc: 0.7500\n",
      "Epoch 886/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4281 - acc: 0.8038 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 887/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4281 - acc: 0.8038 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 888/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4281 - acc: 0.8038 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 889/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4281 - acc: 0.8038 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 890/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4280 - acc: 0.8056 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 891/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4280 - acc: 0.8038 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 892/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4279 - acc: 0.8056 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 893/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4280 - acc: 0.8056 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 894/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4279 - acc: 0.8038 - val_loss: 0.5226 - val_acc: 0.7500\n",
      "Epoch 895/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4279 - acc: 0.8056 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 896/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4279 - acc: 0.8056 - val_loss: 0.5227 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 897/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4278 - acc: 0.8056 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 898/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4278 - acc: 0.8056 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 899/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4278 - acc: 0.8056 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 900/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4278 - acc: 0.8056 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 901/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3787 - acc: 0.781 - 0s 28us/step - loss: 0.4277 - acc: 0.8056 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 902/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4277 - acc: 0.8056 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 903/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4277 - acc: 0.8056 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 904/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4277 - acc: 0.8038 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 905/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4276 - acc: 0.8056 - val_loss: 0.5228 - val_acc: 0.7500\n",
      "Epoch 906/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4276 - acc: 0.8056 - val_loss: 0.5228 - val_acc: 0.7500\n",
      "Epoch 907/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4276 - acc: 0.8056 - val_loss: 0.5228 - val_acc: 0.7500\n",
      "Epoch 908/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4275 - acc: 0.8056 - val_loss: 0.5228 - val_acc: 0.7500\n",
      "Epoch 909/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4275 - acc: 0.8056 - val_loss: 0.5228 - val_acc: 0.7500\n",
      "Epoch 910/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4275 - acc: 0.8056 - val_loss: 0.5228 - val_acc: 0.7500\n",
      "Epoch 911/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4275 - acc: 0.8056 - val_loss: 0.5228 - val_acc: 0.7500\n",
      "Epoch 912/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4274 - acc: 0.8056 - val_loss: 0.5228 - val_acc: 0.7500\n",
      "Epoch 913/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4274 - acc: 0.8056 - val_loss: 0.5228 - val_acc: 0.7500\n",
      "Epoch 914/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4274 - acc: 0.8056 - val_loss: 0.5229 - val_acc: 0.7500\n",
      "Epoch 915/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4274 - acc: 0.8056 - val_loss: 0.5229 - val_acc: 0.7500\n",
      "Epoch 916/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4273 - acc: 0.8056 - val_loss: 0.5229 - val_acc: 0.7500\n",
      "Epoch 917/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4274 - acc: 0.8056 - val_loss: 0.5229 - val_acc: 0.7552\n",
      "Epoch 918/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4273 - acc: 0.8056 - val_loss: 0.5229 - val_acc: 0.7552\n",
      "Epoch 919/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4273 - acc: 0.8056 - val_loss: 0.5229 - val_acc: 0.7552\n",
      "Epoch 920/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4272 - acc: 0.8056 - val_loss: 0.5229 - val_acc: 0.7552\n",
      "Epoch 921/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4272 - acc: 0.8056 - val_loss: 0.5229 - val_acc: 0.7552\n",
      "Epoch 922/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4272 - acc: 0.8056 - val_loss: 0.5229 - val_acc: 0.7552\n",
      "Epoch 923/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4272 - acc: 0.8056 - val_loss: 0.5229 - val_acc: 0.7552\n",
      "Epoch 924/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4272 - acc: 0.8056 - val_loss: 0.5230 - val_acc: 0.7552\n",
      "Epoch 925/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4271 - acc: 0.8056 - val_loss: 0.5230 - val_acc: 0.7552\n",
      "Epoch 926/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4271 - acc: 0.8056 - val_loss: 0.5230 - val_acc: 0.7552\n",
      "Epoch 927/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4270 - acc: 0.8056 - val_loss: 0.5230 - val_acc: 0.7552\n",
      "Epoch 928/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4271 - acc: 0.8056 - val_loss: 0.5230 - val_acc: 0.7552\n",
      "Epoch 929/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4270 - acc: 0.8056 - val_loss: 0.5230 - val_acc: 0.7552\n",
      "Epoch 930/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4270 - acc: 0.8056 - val_loss: 0.5230 - val_acc: 0.7552\n",
      "Epoch 931/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4270 - acc: 0.8056 - val_loss: 0.5230 - val_acc: 0.7552\n",
      "Epoch 932/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4269 - acc: 0.8056 - val_loss: 0.5230 - val_acc: 0.7552\n",
      "Epoch 933/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4269 - acc: 0.8056 - val_loss: 0.5231 - val_acc: 0.7552\n",
      "Epoch 934/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4269 - acc: 0.8056 - val_loss: 0.5231 - val_acc: 0.7552\n",
      "Epoch 935/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4269 - acc: 0.8056 - val_loss: 0.5231 - val_acc: 0.7552\n",
      "Epoch 936/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4269 - acc: 0.8056 - val_loss: 0.5231 - val_acc: 0.7552\n",
      "Epoch 937/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4268 - acc: 0.8056 - val_loss: 0.5231 - val_acc: 0.7552\n",
      "Epoch 938/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4268 - acc: 0.8056 - val_loss: 0.5231 - val_acc: 0.7552\n",
      "Epoch 939/1500\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.4268 - acc: 0.8056 - val_loss: 0.5231 - val_acc: 0.7552\n",
      "Epoch 940/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4267 - acc: 0.8038 - val_loss: 0.5231 - val_acc: 0.7552\n",
      "Epoch 941/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4267 - acc: 0.8056 - val_loss: 0.5231 - val_acc: 0.7552\n",
      "Epoch 942/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4267 - acc: 0.8056 - val_loss: 0.5232 - val_acc: 0.7552\n",
      "Epoch 943/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4267 - acc: 0.8056 - val_loss: 0.5232 - val_acc: 0.7552\n",
      "Epoch 944/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4266 - acc: 0.8056 - val_loss: 0.5232 - val_acc: 0.7552\n",
      "Epoch 945/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4266 - acc: 0.8038 - val_loss: 0.5232 - val_acc: 0.7552\n",
      "Epoch 946/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4266 - acc: 0.8056 - val_loss: 0.5232 - val_acc: 0.7552\n",
      "Epoch 947/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4266 - acc: 0.8056 - val_loss: 0.5232 - val_acc: 0.7552\n",
      "Epoch 948/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4265 - acc: 0.8056 - val_loss: 0.5232 - val_acc: 0.7552\n",
      "Epoch 949/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4265 - acc: 0.8021 - val_loss: 0.5232 - val_acc: 0.7552\n",
      "Epoch 950/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4265 - acc: 0.8038 - val_loss: 0.5232 - val_acc: 0.7552\n",
      "Epoch 951/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4265 - acc: 0.8056 - val_loss: 0.5232 - val_acc: 0.7552\n",
      "Epoch 952/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4265 - acc: 0.8073 - val_loss: 0.5233 - val_acc: 0.7552\n",
      "Epoch 953/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4265 - acc: 0.8073 - val_loss: 0.5233 - val_acc: 0.7552\n",
      "Epoch 954/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4264 - acc: 0.8056 - val_loss: 0.5233 - val_acc: 0.7552\n",
      "Epoch 955/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4264 - acc: 0.8056 - val_loss: 0.5233 - val_acc: 0.7552\n",
      "Epoch 956/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4264 - acc: 0.8056 - val_loss: 0.5233 - val_acc: 0.7552\n",
      "Epoch 957/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4264 - acc: 0.8073 - val_loss: 0.5233 - val_acc: 0.7552\n",
      "Epoch 958/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4263 - acc: 0.8073 - val_loss: 0.5233 - val_acc: 0.7552\n",
      "Epoch 959/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4263 - acc: 0.8056 - val_loss: 0.5233 - val_acc: 0.7552\n",
      "Epoch 960/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4263 - acc: 0.8038 - val_loss: 0.5233 - val_acc: 0.7552\n",
      "Epoch 961/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4263 - acc: 0.8073 - val_loss: 0.5233 - val_acc: 0.7552\n",
      "Epoch 962/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4262 - acc: 0.8073 - val_loss: 0.5233 - val_acc: 0.7552\n",
      "Epoch 963/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4262 - acc: 0.8056 - val_loss: 0.5233 - val_acc: 0.7552\n",
      "Epoch 964/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4262 - acc: 0.8038 - val_loss: 0.5233 - val_acc: 0.7552\n",
      "Epoch 965/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4261 - acc: 0.8056 - val_loss: 0.5233 - val_acc: 0.7552\n",
      "Epoch 966/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4261 - acc: 0.8056 - val_loss: 0.5233 - val_acc: 0.7552\n",
      "Epoch 967/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4261 - acc: 0.8056 - val_loss: 0.5233 - val_acc: 0.7552\n",
      "Epoch 968/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4261 - acc: 0.8056 - val_loss: 0.5234 - val_acc: 0.7552\n",
      "Epoch 969/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4260 - acc: 0.8056 - val_loss: 0.5234 - val_acc: 0.7552\n",
      "Epoch 970/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4260 - acc: 0.8056 - val_loss: 0.5234 - val_acc: 0.7552\n",
      "Epoch 971/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4260 - acc: 0.8056 - val_loss: 0.5234 - val_acc: 0.7552\n",
      "Epoch 972/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4260 - acc: 0.8056 - val_loss: 0.5234 - val_acc: 0.7552\n",
      "Epoch 973/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4259 - acc: 0.8056 - val_loss: 0.5234 - val_acc: 0.7552\n",
      "Epoch 974/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4259 - acc: 0.8056 - val_loss: 0.5234 - val_acc: 0.7552\n",
      "Epoch 975/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4259 - acc: 0.8056 - val_loss: 0.5234 - val_acc: 0.7552\n",
      "Epoch 976/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4259 - acc: 0.8056 - val_loss: 0.5234 - val_acc: 0.7552\n",
      "Epoch 977/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4259 - acc: 0.8056 - val_loss: 0.5234 - val_acc: 0.7552\n",
      "Epoch 978/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4258 - acc: 0.8056 - val_loss: 0.5234 - val_acc: 0.7552\n",
      "Epoch 979/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4258 - acc: 0.8056 - val_loss: 0.5234 - val_acc: 0.7552\n",
      "Epoch 980/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4257 - acc: 0.8056 - val_loss: 0.5234 - val_acc: 0.7552\n",
      "Epoch 981/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4258 - acc: 0.8056 - val_loss: 0.5235 - val_acc: 0.7552\n",
      "Epoch 982/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4257 - acc: 0.8056 - val_loss: 0.5235 - val_acc: 0.7552\n",
      "Epoch 983/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4257 - acc: 0.8056 - val_loss: 0.5235 - val_acc: 0.7552\n",
      "Epoch 984/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4256 - acc: 0.8056 - val_loss: 0.5235 - val_acc: 0.7552\n",
      "Epoch 985/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4257 - acc: 0.8056 - val_loss: 0.5235 - val_acc: 0.7552\n",
      "Epoch 986/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4256 - acc: 0.8038 - val_loss: 0.5235 - val_acc: 0.7552\n",
      "Epoch 987/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4256 - acc: 0.8056 - val_loss: 0.5235 - val_acc: 0.7552\n",
      "Epoch 988/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4256 - acc: 0.8056 - val_loss: 0.5235 - val_acc: 0.7552\n",
      "Epoch 989/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4256 - acc: 0.8056 - val_loss: 0.5235 - val_acc: 0.7552\n",
      "Epoch 990/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4255 - acc: 0.8056 - val_loss: 0.5235 - val_acc: 0.7552\n",
      "Epoch 991/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4255 - acc: 0.8038 - val_loss: 0.5235 - val_acc: 0.7552\n",
      "Epoch 992/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4255 - acc: 0.8056 - val_loss: 0.5235 - val_acc: 0.7552\n",
      "Epoch 993/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4254 - acc: 0.8056 - val_loss: 0.5235 - val_acc: 0.7552\n",
      "Epoch 994/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4254 - acc: 0.8038 - val_loss: 0.5236 - val_acc: 0.7552\n",
      "Epoch 995/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4254 - acc: 0.8038 - val_loss: 0.5236 - val_acc: 0.7552\n",
      "Epoch 996/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4253 - acc: 0.8038 - val_loss: 0.5236 - val_acc: 0.7552\n",
      "Epoch 997/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4253 - acc: 0.8038 - val_loss: 0.5236 - val_acc: 0.7552\n",
      "Epoch 998/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4253 - acc: 0.8056 - val_loss: 0.5236 - val_acc: 0.7552\n",
      "Epoch 999/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4253 - acc: 0.8038 - val_loss: 0.5236 - val_acc: 0.7552\n",
      "Epoch 1000/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4252 - acc: 0.8038 - val_loss: 0.5236 - val_acc: 0.7552\n",
      "Epoch 1001/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4252 - acc: 0.8038 - val_loss: 0.5236 - val_acc: 0.7552\n",
      "Epoch 1002/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4252 - acc: 0.8038 - val_loss: 0.5236 - val_acc: 0.7552\n",
      "Epoch 1003/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4252 - acc: 0.8056 - val_loss: 0.5236 - val_acc: 0.7552\n",
      "Epoch 1004/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4251 - acc: 0.8038 - val_loss: 0.5236 - val_acc: 0.7552\n",
      "Epoch 1005/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4251 - acc: 0.8038 - val_loss: 0.5236 - val_acc: 0.7552\n",
      "Epoch 1006/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4252 - acc: 0.8038 - val_loss: 0.5236 - val_acc: 0.7552\n",
      "Epoch 1007/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4250 - acc: 0.8038 - val_loss: 0.5236 - val_acc: 0.7552\n",
      "Epoch 1008/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4250 - acc: 0.8038 - val_loss: 0.5237 - val_acc: 0.7552\n",
      "Epoch 1009/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4250 - acc: 0.8038 - val_loss: 0.5237 - val_acc: 0.7552\n",
      "Epoch 1010/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4250 - acc: 0.8038 - val_loss: 0.5237 - val_acc: 0.7552\n",
      "Epoch 1011/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4250 - acc: 0.8038 - val_loss: 0.5237 - val_acc: 0.7552\n",
      "Epoch 1012/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4250 - acc: 0.8038 - val_loss: 0.5237 - val_acc: 0.7552\n",
      "Epoch 1013/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4250 - acc: 0.8038 - val_loss: 0.5237 - val_acc: 0.7552\n",
      "Epoch 1014/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4249 - acc: 0.8038 - val_loss: 0.5237 - val_acc: 0.7552\n",
      "Epoch 1015/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4249 - acc: 0.8038 - val_loss: 0.5237 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1016/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4248 - acc: 0.8038 - val_loss: 0.5237 - val_acc: 0.7552\n",
      "Epoch 1017/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4248 - acc: 0.8038 - val_loss: 0.5237 - val_acc: 0.7552\n",
      "Epoch 1018/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4248 - acc: 0.8038 - val_loss: 0.5237 - val_acc: 0.7552\n",
      "Epoch 1019/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4248 - acc: 0.8056 - val_loss: 0.5237 - val_acc: 0.7552\n",
      "Epoch 1020/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4247 - acc: 0.8038 - val_loss: 0.5237 - val_acc: 0.7552\n",
      "Epoch 1021/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4247 - acc: 0.8038 - val_loss: 0.5237 - val_acc: 0.7552\n",
      "Epoch 1022/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4247 - acc: 0.8038 - val_loss: 0.5238 - val_acc: 0.7552\n",
      "Epoch 1023/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4247 - acc: 0.8038 - val_loss: 0.5238 - val_acc: 0.7552\n",
      "Epoch 1024/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4246 - acc: 0.8038 - val_loss: 0.5238 - val_acc: 0.7552\n",
      "Epoch 1025/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4246 - acc: 0.8038 - val_loss: 0.5238 - val_acc: 0.7552\n",
      "Epoch 1026/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4246 - acc: 0.8038 - val_loss: 0.5238 - val_acc: 0.7552\n",
      "Epoch 1027/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4245 - acc: 0.8038 - val_loss: 0.5238 - val_acc: 0.7552\n",
      "Epoch 1028/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4245 - acc: 0.8038 - val_loss: 0.5238 - val_acc: 0.7552\n",
      "Epoch 1029/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4245 - acc: 0.8038 - val_loss: 0.5239 - val_acc: 0.7552\n",
      "Epoch 1030/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4245 - acc: 0.8056 - val_loss: 0.5239 - val_acc: 0.7552\n",
      "Epoch 1031/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4244 - acc: 0.8038 - val_loss: 0.5239 - val_acc: 0.7552\n",
      "Epoch 1032/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4244 - acc: 0.8038 - val_loss: 0.5239 - val_acc: 0.7552\n",
      "Epoch 1033/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4244 - acc: 0.8038 - val_loss: 0.5239 - val_acc: 0.7552\n",
      "Epoch 1034/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4244 - acc: 0.8038 - val_loss: 0.5239 - val_acc: 0.7552\n",
      "Epoch 1035/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5239 - val_acc: 0.7552\n",
      "Epoch 1036/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5240 - val_acc: 0.7552\n",
      "Epoch 1037/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4243 - acc: 0.8038 - val_loss: 0.5240 - val_acc: 0.7552\n",
      "Epoch 1038/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4242 - acc: 0.8038 - val_loss: 0.5240 - val_acc: 0.7552\n",
      "Epoch 1039/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4242 - acc: 0.8038 - val_loss: 0.5240 - val_acc: 0.7552\n",
      "Epoch 1040/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4242 - acc: 0.8038 - val_loss: 0.5240 - val_acc: 0.7552\n",
      "Epoch 1041/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4241 - acc: 0.8038 - val_loss: 0.5240 - val_acc: 0.7552\n",
      "Epoch 1042/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4241 - acc: 0.8038 - val_loss: 0.5240 - val_acc: 0.7552\n",
      "Epoch 1043/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4241 - acc: 0.8038 - val_loss: 0.5241 - val_acc: 0.7552\n",
      "Epoch 1044/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4240 - acc: 0.8038 - val_loss: 0.5241 - val_acc: 0.7552\n",
      "Epoch 1045/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4240 - acc: 0.8038 - val_loss: 0.5241 - val_acc: 0.7552\n",
      "Epoch 1046/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4240 - acc: 0.8038 - val_loss: 0.5241 - val_acc: 0.7552\n",
      "Epoch 1047/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4240 - acc: 0.8038 - val_loss: 0.5241 - val_acc: 0.7552\n",
      "Epoch 1048/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4240 - acc: 0.8038 - val_loss: 0.5241 - val_acc: 0.7500\n",
      "Epoch 1049/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4240 - acc: 0.8038 - val_loss: 0.5241 - val_acc: 0.7500\n",
      "Epoch 1050/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4239 - acc: 0.8038 - val_loss: 0.5241 - val_acc: 0.7500\n",
      "Epoch 1051/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4239 - acc: 0.8038 - val_loss: 0.5241 - val_acc: 0.7500\n",
      "Epoch 1052/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4238 - acc: 0.8038 - val_loss: 0.5242 - val_acc: 0.7500\n",
      "Epoch 1053/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4238 - acc: 0.8038 - val_loss: 0.5242 - val_acc: 0.7500\n",
      "Epoch 1054/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4238 - acc: 0.8038 - val_loss: 0.5242 - val_acc: 0.7500\n",
      "Epoch 1055/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4238 - acc: 0.8038 - val_loss: 0.5242 - val_acc: 0.7500\n",
      "Epoch 1056/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4237 - acc: 0.8038 - val_loss: 0.5242 - val_acc: 0.7500\n",
      "Epoch 1057/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4237 - acc: 0.8038 - val_loss: 0.5242 - val_acc: 0.7500\n",
      "Epoch 1058/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4237 - acc: 0.8038 - val_loss: 0.5242 - val_acc: 0.7500\n",
      "Epoch 1059/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4236 - acc: 0.8038 - val_loss: 0.5243 - val_acc: 0.7500\n",
      "Epoch 1060/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4237 - acc: 0.8038 - val_loss: 0.5243 - val_acc: 0.7500\n",
      "Epoch 1061/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4236 - acc: 0.8038 - val_loss: 0.5243 - val_acc: 0.7500\n",
      "Epoch 1062/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4236 - acc: 0.8038 - val_loss: 0.5243 - val_acc: 0.7500\n",
      "Epoch 1063/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4236 - acc: 0.8038 - val_loss: 0.5243 - val_acc: 0.7500\n",
      "Epoch 1064/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4235 - acc: 0.8038 - val_loss: 0.5243 - val_acc: 0.7500\n",
      "Epoch 1065/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4235 - acc: 0.8038 - val_loss: 0.5243 - val_acc: 0.7500\n",
      "Epoch 1066/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4235 - acc: 0.8038 - val_loss: 0.5243 - val_acc: 0.7500\n",
      "Epoch 1067/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4235 - acc: 0.8038 - val_loss: 0.5243 - val_acc: 0.7500\n",
      "Epoch 1068/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4234 - acc: 0.8038 - val_loss: 0.5243 - val_acc: 0.7500\n",
      "Epoch 1069/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4234 - acc: 0.8038 - val_loss: 0.5243 - val_acc: 0.7500\n",
      "Epoch 1070/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4234 - acc: 0.8038 - val_loss: 0.5243 - val_acc: 0.7500\n",
      "Epoch 1071/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4233 - acc: 0.8038 - val_loss: 0.5243 - val_acc: 0.7500\n",
      "Epoch 1072/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4233 - acc: 0.8038 - val_loss: 0.5244 - val_acc: 0.7500\n",
      "Epoch 1073/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4233 - acc: 0.8038 - val_loss: 0.5244 - val_acc: 0.7500\n",
      "Epoch 1074/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4233 - acc: 0.8038 - val_loss: 0.5244 - val_acc: 0.7500\n",
      "Epoch 1075/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4233 - acc: 0.8038 - val_loss: 0.5244 - val_acc: 0.7500\n",
      "Epoch 1076/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4232 - acc: 0.8038 - val_loss: 0.5244 - val_acc: 0.7500\n",
      "Epoch 1077/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4232 - acc: 0.8038 - val_loss: 0.5244 - val_acc: 0.7500\n",
      "Epoch 1078/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4231 - acc: 0.8038 - val_loss: 0.5245 - val_acc: 0.7500\n",
      "Epoch 1079/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4231 - acc: 0.8038 - val_loss: 0.5245 - val_acc: 0.7500\n",
      "Epoch 1080/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4231 - acc: 0.8038 - val_loss: 0.5245 - val_acc: 0.7500\n",
      "Epoch 1081/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4231 - acc: 0.8038 - val_loss: 0.5245 - val_acc: 0.7500\n",
      "Epoch 1082/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4230 - acc: 0.8038 - val_loss: 0.5245 - val_acc: 0.7500\n",
      "Epoch 1083/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4230 - acc: 0.8038 - val_loss: 0.5245 - val_acc: 0.7500\n",
      "Epoch 1084/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4229 - acc: 0.8038 - val_loss: 0.5246 - val_acc: 0.7500\n",
      "Epoch 1085/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4229 - acc: 0.8038 - val_loss: 0.5246 - val_acc: 0.7500\n",
      "Epoch 1086/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3813 - acc: 0.781 - 0s 28us/step - loss: 0.4229 - acc: 0.8038 - val_loss: 0.5246 - val_acc: 0.7500\n",
      "Epoch 1087/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4228 - acc: 0.8038 - val_loss: 0.5246 - val_acc: 0.7500\n",
      "Epoch 1088/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4228 - acc: 0.8038 - val_loss: 0.5246 - val_acc: 0.7500\n",
      "Epoch 1089/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4228 - acc: 0.8038 - val_loss: 0.5246 - val_acc: 0.7500\n",
      "Epoch 1090/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4227 - acc: 0.8038 - val_loss: 0.5246 - val_acc: 0.7500\n",
      "Epoch 1091/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4227 - acc: 0.8038 - val_loss: 0.5246 - val_acc: 0.7500\n",
      "Epoch 1092/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4227 - acc: 0.8038 - val_loss: 0.5246 - val_acc: 0.7500\n",
      "Epoch 1093/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4227 - acc: 0.8038 - val_loss: 0.5246 - val_acc: 0.7500\n",
      "Epoch 1094/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4226 - acc: 0.8038 - val_loss: 0.5246 - val_acc: 0.7500\n",
      "Epoch 1095/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4226 - acc: 0.8038 - val_loss: 0.5247 - val_acc: 0.7500\n",
      "Epoch 1096/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4226 - acc: 0.8038 - val_loss: 0.5247 - val_acc: 0.7500\n",
      "Epoch 1097/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4225 - acc: 0.8038 - val_loss: 0.5247 - val_acc: 0.7500\n",
      "Epoch 1098/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4225 - acc: 0.8038 - val_loss: 0.5247 - val_acc: 0.7500\n",
      "Epoch 1099/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4224 - acc: 0.8038 - val_loss: 0.5247 - val_acc: 0.7500\n",
      "Epoch 1100/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4225 - acc: 0.8038 - val_loss: 0.5248 - val_acc: 0.7500\n",
      "Epoch 1101/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4224 - acc: 0.8038 - val_loss: 0.5248 - val_acc: 0.7500\n",
      "Epoch 1102/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4224 - acc: 0.8038 - val_loss: 0.5248 - val_acc: 0.7500\n",
      "Epoch 1103/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4224 - acc: 0.8038 - val_loss: 0.5248 - val_acc: 0.7500\n",
      "Epoch 1104/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4223 - acc: 0.8038 - val_loss: 0.5248 - val_acc: 0.7500\n",
      "Epoch 1105/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4223 - acc: 0.8038 - val_loss: 0.5249 - val_acc: 0.7500\n",
      "Epoch 1106/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4223 - acc: 0.8038 - val_loss: 0.5249 - val_acc: 0.7500\n",
      "Epoch 1107/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4222 - acc: 0.8038 - val_loss: 0.5249 - val_acc: 0.7500\n",
      "Epoch 1108/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4222 - acc: 0.8038 - val_loss: 0.5249 - val_acc: 0.7500\n",
      "Epoch 1109/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4222 - acc: 0.8038 - val_loss: 0.5249 - val_acc: 0.7500\n",
      "Epoch 1110/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4222 - acc: 0.8038 - val_loss: 0.5249 - val_acc: 0.7500\n",
      "Epoch 1111/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4221 - acc: 0.8038 - val_loss: 0.5250 - val_acc: 0.7500\n",
      "Epoch 1112/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4221 - acc: 0.8038 - val_loss: 0.5250 - val_acc: 0.7500\n",
      "Epoch 1113/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4220 - acc: 0.8038 - val_loss: 0.5250 - val_acc: 0.7500\n",
      "Epoch 1114/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4220 - acc: 0.8038 - val_loss: 0.5250 - val_acc: 0.7500\n",
      "Epoch 1115/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4220 - acc: 0.8038 - val_loss: 0.5251 - val_acc: 0.7500\n",
      "Epoch 1116/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4220 - acc: 0.8038 - val_loss: 0.5251 - val_acc: 0.7500\n",
      "Epoch 1117/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4220 - acc: 0.8038 - val_loss: 0.5251 - val_acc: 0.7500\n",
      "Epoch 1118/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4219 - acc: 0.8038 - val_loss: 0.5251 - val_acc: 0.7500\n",
      "Epoch 1119/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4219 - acc: 0.8038 - val_loss: 0.5251 - val_acc: 0.7500\n",
      "Epoch 1120/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4219 - acc: 0.8038 - val_loss: 0.5251 - val_acc: 0.7500\n",
      "Epoch 1121/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4218 - acc: 0.8038 - val_loss: 0.5252 - val_acc: 0.7500\n",
      "Epoch 1122/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4218 - acc: 0.8038 - val_loss: 0.5252 - val_acc: 0.7500\n",
      "Epoch 1123/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4218 - acc: 0.8038 - val_loss: 0.5252 - val_acc: 0.7500\n",
      "Epoch 1124/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4218 - acc: 0.8038 - val_loss: 0.5252 - val_acc: 0.7500\n",
      "Epoch 1125/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4217 - acc: 0.8038 - val_loss: 0.5252 - val_acc: 0.7500\n",
      "Epoch 1126/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4217 - acc: 0.8038 - val_loss: 0.5253 - val_acc: 0.7500\n",
      "Epoch 1127/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4217 - acc: 0.8038 - val_loss: 0.5253 - val_acc: 0.7500\n",
      "Epoch 1128/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4216 - acc: 0.8038 - val_loss: 0.5253 - val_acc: 0.7500\n",
      "Epoch 1129/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4216 - acc: 0.8038 - val_loss: 0.5253 - val_acc: 0.7500\n",
      "Epoch 1130/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4216 - acc: 0.8038 - val_loss: 0.5254 - val_acc: 0.7500\n",
      "Epoch 1131/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4215 - acc: 0.8038 - val_loss: 0.5254 - val_acc: 0.7500\n",
      "Epoch 1132/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4215 - acc: 0.8038 - val_loss: 0.5254 - val_acc: 0.7500\n",
      "Epoch 1133/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4215 - acc: 0.8038 - val_loss: 0.5254 - val_acc: 0.7500\n",
      "Epoch 1134/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 28us/step - loss: 0.4215 - acc: 0.8038 - val_loss: 0.5254 - val_acc: 0.7500\n",
      "Epoch 1135/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4215 - acc: 0.8038 - val_loss: 0.5255 - val_acc: 0.7500\n",
      "Epoch 1136/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4215 - acc: 0.8038 - val_loss: 0.5255 - val_acc: 0.7500\n",
      "Epoch 1137/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4214 - acc: 0.8038 - val_loss: 0.5255 - val_acc: 0.7500\n",
      "Epoch 1138/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4214 - acc: 0.8038 - val_loss: 0.5255 - val_acc: 0.7500\n",
      "Epoch 1139/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4214 - acc: 0.8038 - val_loss: 0.5255 - val_acc: 0.7500\n",
      "Epoch 1140/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4213 - acc: 0.8038 - val_loss: 0.5255 - val_acc: 0.7500\n",
      "Epoch 1141/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4213 - acc: 0.8038 - val_loss: 0.5256 - val_acc: 0.7500\n",
      "Epoch 1142/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4213 - acc: 0.8056 - val_loss: 0.5256 - val_acc: 0.7500\n",
      "Epoch 1143/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4213 - acc: 0.8056 - val_loss: 0.5256 - val_acc: 0.7500\n",
      "Epoch 1144/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4212 - acc: 0.8056 - val_loss: 0.5256 - val_acc: 0.7500\n",
      "Epoch 1145/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4212 - acc: 0.8056 - val_loss: 0.5256 - val_acc: 0.7500\n",
      "Epoch 1146/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4212 - acc: 0.8056 - val_loss: 0.5256 - val_acc: 0.7500\n",
      "Epoch 1147/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4212 - acc: 0.8056 - val_loss: 0.5256 - val_acc: 0.7500\n",
      "Epoch 1148/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4211 - acc: 0.8056 - val_loss: 0.5257 - val_acc: 0.7500\n",
      "Epoch 1149/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4211 - acc: 0.8056 - val_loss: 0.5257 - val_acc: 0.7500\n",
      "Epoch 1150/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4211 - acc: 0.8056 - val_loss: 0.5257 - val_acc: 0.7500\n",
      "Epoch 1151/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4211 - acc: 0.8056 - val_loss: 0.5257 - val_acc: 0.7500\n",
      "Epoch 1152/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4210 - acc: 0.8056 - val_loss: 0.5257 - val_acc: 0.7500\n",
      "Epoch 1153/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4211 - acc: 0.8056 - val_loss: 0.5257 - val_acc: 0.7500\n",
      "Epoch 1154/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4210 - acc: 0.8056 - val_loss: 0.5258 - val_acc: 0.7500\n",
      "Epoch 1155/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4210 - acc: 0.8056 - val_loss: 0.5258 - val_acc: 0.7500\n",
      "Epoch 1156/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4209 - acc: 0.8056 - val_loss: 0.5258 - val_acc: 0.7500\n",
      "Epoch 1157/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4209 - acc: 0.8056 - val_loss: 0.5258 - val_acc: 0.7500\n",
      "Epoch 1158/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4209 - acc: 0.8056 - val_loss: 0.5258 - val_acc: 0.7500\n",
      "Epoch 1159/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4209 - acc: 0.8056 - val_loss: 0.5258 - val_acc: 0.7500\n",
      "Epoch 1160/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4209 - acc: 0.8056 - val_loss: 0.5258 - val_acc: 0.7500\n",
      "Epoch 1161/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4208 - acc: 0.8056 - val_loss: 0.5258 - val_acc: 0.7500\n",
      "Epoch 1162/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4208 - acc: 0.8056 - val_loss: 0.5259 - val_acc: 0.7500\n",
      "Epoch 1163/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4208 - acc: 0.8056 - val_loss: 0.5259 - val_acc: 0.7500\n",
      "Epoch 1164/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4208 - acc: 0.8056 - val_loss: 0.5259 - val_acc: 0.7500\n",
      "Epoch 1165/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4207 - acc: 0.8056 - val_loss: 0.5259 - val_acc: 0.7500\n",
      "Epoch 1166/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4207 - acc: 0.8056 - val_loss: 0.5259 - val_acc: 0.7500\n",
      "Epoch 1167/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4207 - acc: 0.8056 - val_loss: 0.5260 - val_acc: 0.7500\n",
      "Epoch 1168/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4207 - acc: 0.8056 - val_loss: 0.5260 - val_acc: 0.7500\n",
      "Epoch 1169/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4207 - acc: 0.8056 - val_loss: 0.5260 - val_acc: 0.7500\n",
      "Epoch 1170/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4206 - acc: 0.8056 - val_loss: 0.5260 - val_acc: 0.7500\n",
      "Epoch 1171/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4206 - acc: 0.8056 - val_loss: 0.5260 - val_acc: 0.7500\n",
      "Epoch 1172/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4206 - acc: 0.8073 - val_loss: 0.5261 - val_acc: 0.7500\n",
      "Epoch 1173/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4205 - acc: 0.8056 - val_loss: 0.5261 - val_acc: 0.7500\n",
      "Epoch 1174/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4205 - acc: 0.8056 - val_loss: 0.5261 - val_acc: 0.7500\n",
      "Epoch 1175/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4206 - acc: 0.8056 - val_loss: 0.5261 - val_acc: 0.7500\n",
      "Epoch 1176/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4205 - acc: 0.8056 - val_loss: 0.5261 - val_acc: 0.7500\n",
      "Epoch 1177/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4205 - acc: 0.8056 - val_loss: 0.5262 - val_acc: 0.7500\n",
      "Epoch 1178/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4205 - acc: 0.8056 - val_loss: 0.5262 - val_acc: 0.7500\n",
      "Epoch 1179/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4204 - acc: 0.8056 - val_loss: 0.5262 - val_acc: 0.7500\n",
      "Epoch 1180/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4204 - acc: 0.8056 - val_loss: 0.5262 - val_acc: 0.7500\n",
      "Epoch 1181/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4204 - acc: 0.8073 - val_loss: 0.5262 - val_acc: 0.7500\n",
      "Epoch 1182/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4204 - acc: 0.8073 - val_loss: 0.5262 - val_acc: 0.7500\n",
      "Epoch 1183/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4203 - acc: 0.8056 - val_loss: 0.5263 - val_acc: 0.7500\n",
      "Epoch 1184/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4203 - acc: 0.8056 - val_loss: 0.5263 - val_acc: 0.7500\n",
      "Epoch 1185/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4203 - acc: 0.8056 - val_loss: 0.5263 - val_acc: 0.7500\n",
      "Epoch 1186/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.6228 - acc: 0.781 - 0s 28us/step - loss: 0.4203 - acc: 0.8056 - val_loss: 0.5263 - val_acc: 0.7500\n",
      "Epoch 1187/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4203 - acc: 0.8056 - val_loss: 0.5263 - val_acc: 0.7500\n",
      "Epoch 1188/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4202 - acc: 0.8056 - val_loss: 0.5264 - val_acc: 0.7500\n",
      "Epoch 1189/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4203 - acc: 0.8056 - val_loss: 0.5264 - val_acc: 0.7500\n",
      "Epoch 1190/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4202 - acc: 0.8056 - val_loss: 0.5264 - val_acc: 0.7500\n",
      "Epoch 1191/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4202 - acc: 0.8056 - val_loss: 0.5264 - val_acc: 0.7500\n",
      "Epoch 1192/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4202 - acc: 0.8056 - val_loss: 0.5264 - val_acc: 0.7500\n",
      "Epoch 1193/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4202 - acc: 0.8056 - val_loss: 0.5265 - val_acc: 0.7500\n",
      "Epoch 1194/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4201 - acc: 0.8073 - val_loss: 0.5265 - val_acc: 0.7500\n",
      "Epoch 1195/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4201 - acc: 0.8056 - val_loss: 0.5265 - val_acc: 0.7500\n",
      "Epoch 1196/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4201 - acc: 0.8056 - val_loss: 0.5265 - val_acc: 0.7500\n",
      "Epoch 1197/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4201 - acc: 0.8056 - val_loss: 0.5265 - val_acc: 0.7500\n",
      "Epoch 1198/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4200 - acc: 0.8056 - val_loss: 0.5266 - val_acc: 0.7500\n",
      "Epoch 1199/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4200 - acc: 0.8056 - val_loss: 0.5266 - val_acc: 0.7500\n",
      "Epoch 1200/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4200 - acc: 0.8073 - val_loss: 0.5266 - val_acc: 0.7500\n",
      "Epoch 1201/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4200 - acc: 0.8073 - val_loss: 0.5266 - val_acc: 0.7500\n",
      "Epoch 1202/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4200 - acc: 0.8073 - val_loss: 0.5267 - val_acc: 0.7500\n",
      "Epoch 1203/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4199 - acc: 0.8056 - val_loss: 0.5267 - val_acc: 0.7500\n",
      "Epoch 1204/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4199 - acc: 0.8056 - val_loss: 0.5267 - val_acc: 0.7500\n",
      "Epoch 1205/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4199 - acc: 0.8056 - val_loss: 0.5267 - val_acc: 0.7500\n",
      "Epoch 1206/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4199 - acc: 0.8073 - val_loss: 0.5267 - val_acc: 0.7500\n",
      "Epoch 1207/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4198 - acc: 0.8056 - val_loss: 0.5267 - val_acc: 0.7500\n",
      "Epoch 1208/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4199 - acc: 0.8073 - val_loss: 0.5267 - val_acc: 0.7500\n",
      "Epoch 1209/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4198 - acc: 0.8073 - val_loss: 0.5267 - val_acc: 0.7500\n",
      "Epoch 1210/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4198 - acc: 0.8056 - val_loss: 0.5268 - val_acc: 0.7500\n",
      "Epoch 1211/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4197 - acc: 0.8073 - val_loss: 0.5268 - val_acc: 0.7500\n",
      "Epoch 1212/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4198 - acc: 0.8056 - val_loss: 0.5268 - val_acc: 0.7500\n",
      "Epoch 1213/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4197 - acc: 0.8073 - val_loss: 0.5268 - val_acc: 0.7500\n",
      "Epoch 1214/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4197 - acc: 0.8090 - val_loss: 0.5268 - val_acc: 0.7500\n",
      "Epoch 1215/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4197 - acc: 0.8056 - val_loss: 0.5268 - val_acc: 0.7500\n",
      "Epoch 1216/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4197 - acc: 0.8073 - val_loss: 0.5268 - val_acc: 0.7500\n",
      "Epoch 1217/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4196 - acc: 0.8073 - val_loss: 0.5268 - val_acc: 0.7500\n",
      "Epoch 1218/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4196 - acc: 0.8090 - val_loss: 0.5269 - val_acc: 0.7500\n",
      "Epoch 1219/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4196 - acc: 0.8073 - val_loss: 0.5269 - val_acc: 0.7500\n",
      "Epoch 1220/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4196 - acc: 0.8090 - val_loss: 0.5269 - val_acc: 0.7500\n",
      "Epoch 1221/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4196 - acc: 0.8090 - val_loss: 0.5269 - val_acc: 0.7500\n",
      "Epoch 1222/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4195 - acc: 0.8056 - val_loss: 0.5269 - val_acc: 0.7500\n",
      "Epoch 1223/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4195 - acc: 0.8090 - val_loss: 0.5269 - val_acc: 0.7500\n",
      "Epoch 1224/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4195 - acc: 0.8108 - val_loss: 0.5270 - val_acc: 0.7500\n",
      "Epoch 1225/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4195 - acc: 0.8090 - val_loss: 0.5270 - val_acc: 0.7500\n",
      "Epoch 1226/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4195 - acc: 0.8090 - val_loss: 0.5270 - val_acc: 0.7500\n",
      "Epoch 1227/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4194 - acc: 0.8090 - val_loss: 0.5270 - val_acc: 0.7500\n",
      "Epoch 1228/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4194 - acc: 0.8090 - val_loss: 0.5270 - val_acc: 0.7500\n",
      "Epoch 1229/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4194 - acc: 0.8090 - val_loss: 0.5271 - val_acc: 0.7500\n",
      "Epoch 1230/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4194 - acc: 0.8073 - val_loss: 0.5271 - val_acc: 0.7500\n",
      "Epoch 1231/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4194 - acc: 0.8090 - val_loss: 0.5271 - val_acc: 0.7500\n",
      "Epoch 1232/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4194 - acc: 0.8090 - val_loss: 0.5271 - val_acc: 0.7500\n",
      "Epoch 1233/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4194 - acc: 0.8108 - val_loss: 0.5271 - val_acc: 0.7500\n",
      "Epoch 1234/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4193 - acc: 0.8090 - val_loss: 0.5271 - val_acc: 0.7500\n",
      "Epoch 1235/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4193 - acc: 0.8108 - val_loss: 0.5272 - val_acc: 0.7500\n",
      "Epoch 1236/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4193 - acc: 0.8090 - val_loss: 0.5272 - val_acc: 0.7500\n",
      "Epoch 1237/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4192 - acc: 0.8108 - val_loss: 0.5272 - val_acc: 0.7500\n",
      "Epoch 1238/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4192 - acc: 0.8108 - val_loss: 0.5272 - val_acc: 0.7500\n",
      "Epoch 1239/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4192 - acc: 0.8108 - val_loss: 0.5273 - val_acc: 0.7500\n",
      "Epoch 1240/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4192 - acc: 0.8108 - val_loss: 0.5273 - val_acc: 0.7500\n",
      "Epoch 1241/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4191 - acc: 0.8090 - val_loss: 0.5273 - val_acc: 0.7500\n",
      "Epoch 1242/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4192 - acc: 0.8108 - val_loss: 0.5273 - val_acc: 0.7500\n",
      "Epoch 1243/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4191 - acc: 0.8108 - val_loss: 0.5273 - val_acc: 0.7500\n",
      "Epoch 1244/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4191 - acc: 0.8108 - val_loss: 0.5274 - val_acc: 0.7500\n",
      "Epoch 1245/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4190 - acc: 0.8090 - val_loss: 0.5274 - val_acc: 0.7500\n",
      "Epoch 1246/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4190 - acc: 0.8090 - val_loss: 0.5274 - val_acc: 0.7500\n",
      "Epoch 1247/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4191 - acc: 0.8090 - val_loss: 0.5274 - val_acc: 0.7500\n",
      "Epoch 1248/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4190 - acc: 0.8108 - val_loss: 0.5274 - val_acc: 0.7500\n",
      "Epoch 1249/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4190 - acc: 0.8090 - val_loss: 0.5274 - val_acc: 0.7500\n",
      "Epoch 1250/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4190 - acc: 0.8056 - val_loss: 0.5274 - val_acc: 0.7500\n",
      "Epoch 1251/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4190 - acc: 0.8090 - val_loss: 0.5274 - val_acc: 0.7500\n",
      "Epoch 1252/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 28us/step - loss: 0.4189 - acc: 0.8073 - val_loss: 0.5275 - val_acc: 0.7500\n",
      "Epoch 1253/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4189 - acc: 0.8108 - val_loss: 0.5275 - val_acc: 0.7500\n",
      "Epoch 1254/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4189 - acc: 0.8090 - val_loss: 0.5275 - val_acc: 0.7500\n",
      "Epoch 1255/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4188 - acc: 0.8108 - val_loss: 0.5275 - val_acc: 0.7500\n",
      "Epoch 1256/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4188 - acc: 0.8108 - val_loss: 0.5275 - val_acc: 0.7500\n",
      "Epoch 1257/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4188 - acc: 0.8090 - val_loss: 0.5275 - val_acc: 0.7500\n",
      "Epoch 1258/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4188 - acc: 0.8090 - val_loss: 0.5275 - val_acc: 0.7500\n",
      "Epoch 1259/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4187 - acc: 0.8090 - val_loss: 0.5275 - val_acc: 0.7500\n",
      "Epoch 1260/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4188 - acc: 0.8090 - val_loss: 0.5276 - val_acc: 0.7500\n",
      "Epoch 1261/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4187 - acc: 0.8090 - val_loss: 0.5276 - val_acc: 0.7500\n",
      "Epoch 1262/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4187 - acc: 0.8108 - val_loss: 0.5276 - val_acc: 0.7500\n",
      "Epoch 1263/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4187 - acc: 0.8108 - val_loss: 0.5276 - val_acc: 0.7500\n",
      "Epoch 1264/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4187 - acc: 0.8073 - val_loss: 0.5276 - val_acc: 0.7500\n",
      "Epoch 1265/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4186 - acc: 0.8108 - val_loss: 0.5277 - val_acc: 0.7500\n",
      "Epoch 1266/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4186 - acc: 0.8090 - val_loss: 0.5277 - val_acc: 0.7500\n",
      "Epoch 1267/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4186 - acc: 0.8090 - val_loss: 0.5277 - val_acc: 0.7500\n",
      "Epoch 1268/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4186 - acc: 0.8073 - val_loss: 0.5277 - val_acc: 0.7500\n",
      "Epoch 1269/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4186 - acc: 0.8056 - val_loss: 0.5277 - val_acc: 0.7500\n",
      "Epoch 1270/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4186 - acc: 0.8090 - val_loss: 0.5277 - val_acc: 0.7500\n",
      "Epoch 1271/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4185 - acc: 0.8073 - val_loss: 0.5277 - val_acc: 0.7500\n",
      "Epoch 1272/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4185 - acc: 0.8090 - val_loss: 0.5277 - val_acc: 0.7500\n",
      "Epoch 1273/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4185 - acc: 0.8090 - val_loss: 0.5278 - val_acc: 0.7500\n",
      "Epoch 1274/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4185 - acc: 0.8090 - val_loss: 0.5278 - val_acc: 0.7500\n",
      "Epoch 1275/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4185 - acc: 0.8090 - val_loss: 0.5278 - val_acc: 0.7500\n",
      "Epoch 1276/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4184 - acc: 0.8090 - val_loss: 0.5278 - val_acc: 0.7500\n",
      "Epoch 1277/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4184 - acc: 0.8056 - val_loss: 0.5278 - val_acc: 0.7500\n",
      "Epoch 1278/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4184 - acc: 0.8090 - val_loss: 0.5278 - val_acc: 0.7500\n",
      "Epoch 1279/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4184 - acc: 0.8090 - val_loss: 0.5278 - val_acc: 0.7500\n",
      "Epoch 1280/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4183 - acc: 0.8090 - val_loss: 0.5279 - val_acc: 0.7500\n",
      "Epoch 1281/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4183 - acc: 0.8090 - val_loss: 0.5279 - val_acc: 0.7500\n",
      "Epoch 1282/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4183 - acc: 0.8073 - val_loss: 0.5279 - val_acc: 0.7500\n",
      "Epoch 1283/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4182 - acc: 0.8073 - val_loss: 0.5279 - val_acc: 0.7500\n",
      "Epoch 1284/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4183 - acc: 0.8056 - val_loss: 0.5279 - val_acc: 0.7500\n",
      "Epoch 1285/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4183 - acc: 0.8056 - val_loss: 0.5279 - val_acc: 0.7500\n",
      "Epoch 1286/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4182 - acc: 0.8090 - val_loss: 0.5280 - val_acc: 0.7500\n",
      "Epoch 1287/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4182 - acc: 0.8090 - val_loss: 0.5280 - val_acc: 0.7500\n",
      "Epoch 1288/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4182 - acc: 0.8090 - val_loss: 0.5280 - val_acc: 0.7500\n",
      "Epoch 1289/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4181 - acc: 0.8108 - val_loss: 0.5280 - val_acc: 0.7500\n",
      "Epoch 1290/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4181 - acc: 0.8073 - val_loss: 0.5280 - val_acc: 0.7500\n",
      "Epoch 1291/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4181 - acc: 0.8073 - val_loss: 0.5281 - val_acc: 0.7500\n",
      "Epoch 1292/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4181 - acc: 0.8073 - val_loss: 0.5281 - val_acc: 0.7500\n",
      "Epoch 1293/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4181 - acc: 0.8073 - val_loss: 0.5281 - val_acc: 0.7500\n",
      "Epoch 1294/1500\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.4181 - acc: 0.8108 - val_loss: 0.5281 - val_acc: 0.7500\n",
      "Epoch 1295/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4180 - acc: 0.8090 - val_loss: 0.5281 - val_acc: 0.7500\n",
      "Epoch 1296/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4180 - acc: 0.8090 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 1297/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4180 - acc: 0.8090 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 1298/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4180 - acc: 0.8090 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 1299/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4180 - acc: 0.8090 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 1300/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4180 - acc: 0.8090 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 1301/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4180 - acc: 0.8073 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 1302/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4179 - acc: 0.8090 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 1303/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4179 - acc: 0.8090 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 1304/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4179 - acc: 0.8090 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 1305/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4179 - acc: 0.8073 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 1306/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4178 - acc: 0.8090 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 1307/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4178 - acc: 0.8090 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 1308/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4177 - acc: 0.8090 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 1309/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4178 - acc: 0.8090 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 1310/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4177 - acc: 0.8090 - val_loss: 0.5284 - val_acc: 0.7500\n",
      "Epoch 1311/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4177 - acc: 0.8073 - val_loss: 0.5284 - val_acc: 0.7500\n",
      "Epoch 1312/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4177 - acc: 0.8090 - val_loss: 0.5284 - val_acc: 0.7500\n",
      "Epoch 1313/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4177 - acc: 0.8073 - val_loss: 0.5284 - val_acc: 0.7500\n",
      "Epoch 1314/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4177 - acc: 0.8090 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 1315/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4177 - acc: 0.8090 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 1316/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4176 - acc: 0.8090 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 1317/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4176 - acc: 0.8090 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 1318/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4176 - acc: 0.8090 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 1319/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4175 - acc: 0.8073 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 1320/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4176 - acc: 0.8073 - val_loss: 0.5285 - val_acc: 0.7500\n",
      "Epoch 1321/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4175 - acc: 0.8073 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 1322/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4175 - acc: 0.8073 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 1323/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4175 - acc: 0.8073 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 1324/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4175 - acc: 0.8073 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 1325/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4174 - acc: 0.8073 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 1326/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4174 - acc: 0.8073 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 1327/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4174 - acc: 0.8073 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 1328/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4174 - acc: 0.8073 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 1329/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4174 - acc: 0.8073 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 1330/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4174 - acc: 0.8073 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 1331/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4173 - acc: 0.8073 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 1332/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4173 - acc: 0.8073 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 1333/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4173 - acc: 0.8073 - val_loss: 0.5287 - val_acc: 0.7500\n",
      "Epoch 1334/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4173 - acc: 0.8073 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 1335/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4173 - acc: 0.8073 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 1336/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4173 - acc: 0.8090 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 1337/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4173 - acc: 0.8090 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 1338/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4172 - acc: 0.8073 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 1339/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4172 - acc: 0.8073 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 1340/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4172 - acc: 0.8073 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 1341/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4171 - acc: 0.8073 - val_loss: 0.5289 - val_acc: 0.7500\n",
      "Epoch 1342/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4172 - acc: 0.8073 - val_loss: 0.5289 - val_acc: 0.7500\n",
      "Epoch 1343/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4171 - acc: 0.8073 - val_loss: 0.5289 - val_acc: 0.7500\n",
      "Epoch 1344/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4171 - acc: 0.8090 - val_loss: 0.5289 - val_acc: 0.7500\n",
      "Epoch 1345/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4171 - acc: 0.8073 - val_loss: 0.5289 - val_acc: 0.7500\n",
      "Epoch 1346/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4171 - acc: 0.8073 - val_loss: 0.5289 - val_acc: 0.7500\n",
      "Epoch 1347/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4170 - acc: 0.8073 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 1348/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4170 - acc: 0.8073 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 1349/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4170 - acc: 0.8073 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 1350/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4170 - acc: 0.8073 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 1351/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4170 - acc: 0.8073 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 1352/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4169 - acc: 0.8073 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 1353/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4169 - acc: 0.8073 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 1354/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4169 - acc: 0.8073 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 1355/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4169 - acc: 0.8073 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 1356/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4169 - acc: 0.8073 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 1357/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4169 - acc: 0.8090 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 1358/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4168 - acc: 0.8090 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 1359/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4168 - acc: 0.8073 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 1360/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4168 - acc: 0.8090 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 1361/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4168 - acc: 0.8073 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 1362/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4167 - acc: 0.8090 - val_loss: 0.5292 - val_acc: 0.7500\n",
      "Epoch 1363/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4168 - acc: 0.8090 - val_loss: 0.5292 - val_acc: 0.7500\n",
      "Epoch 1364/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4167 - acc: 0.8073 - val_loss: 0.5292 - val_acc: 0.7500\n",
      "Epoch 1365/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4167 - acc: 0.8090 - val_loss: 0.5292 - val_acc: 0.7500\n",
      "Epoch 1366/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4167 - acc: 0.8073 - val_loss: 0.5292 - val_acc: 0.7500\n",
      "Epoch 1367/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4167 - acc: 0.8090 - val_loss: 0.5292 - val_acc: 0.7500\n",
      "Epoch 1368/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4166 - acc: 0.8073 - val_loss: 0.5293 - val_acc: 0.7500\n",
      "Epoch 1369/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4167 - acc: 0.8073 - val_loss: 0.5293 - val_acc: 0.7500\n",
      "Epoch 1370/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 28us/step - loss: 0.4166 - acc: 0.8073 - val_loss: 0.5293 - val_acc: 0.7500\n",
      "Epoch 1371/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4166 - acc: 0.8073 - val_loss: 0.5293 - val_acc: 0.7500\n",
      "Epoch 1372/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4166 - acc: 0.8073 - val_loss: 0.5293 - val_acc: 0.7500\n",
      "Epoch 1373/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4165 - acc: 0.8090 - val_loss: 0.5293 - val_acc: 0.7500\n",
      "Epoch 1374/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4166 - acc: 0.8073 - val_loss: 0.5294 - val_acc: 0.7500\n",
      "Epoch 1375/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4165 - acc: 0.8073 - val_loss: 0.5294 - val_acc: 0.7500\n",
      "Epoch 1376/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4165 - acc: 0.8073 - val_loss: 0.5294 - val_acc: 0.7500\n",
      "Epoch 1377/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4165 - acc: 0.8090 - val_loss: 0.5294 - val_acc: 0.7500\n",
      "Epoch 1378/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4164 - acc: 0.8090 - val_loss: 0.5294 - val_acc: 0.7500\n",
      "Epoch 1379/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4164 - acc: 0.8090 - val_loss: 0.5294 - val_acc: 0.7500\n",
      "Epoch 1380/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4164 - acc: 0.8090 - val_loss: 0.5294 - val_acc: 0.7500\n",
      "Epoch 1381/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4164 - acc: 0.8090 - val_loss: 0.5295 - val_acc: 0.7500\n",
      "Epoch 1382/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4164 - acc: 0.8090 - val_loss: 0.5295 - val_acc: 0.7500\n",
      "Epoch 1383/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4163 - acc: 0.8090 - val_loss: 0.5295 - val_acc: 0.7552\n",
      "Epoch 1384/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4164 - acc: 0.8090 - val_loss: 0.5295 - val_acc: 0.7552\n",
      "Epoch 1385/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4163 - acc: 0.8108 - val_loss: 0.5295 - val_acc: 0.7552\n",
      "Epoch 1386/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4163 - acc: 0.8090 - val_loss: 0.5296 - val_acc: 0.7552\n",
      "Epoch 1387/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4163 - acc: 0.8090 - val_loss: 0.5296 - val_acc: 0.7552\n",
      "Epoch 1388/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4162 - acc: 0.8090 - val_loss: 0.5296 - val_acc: 0.7552\n",
      "Epoch 1389/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4163 - acc: 0.8108 - val_loss: 0.5296 - val_acc: 0.7552\n",
      "Epoch 1390/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4162 - acc: 0.8090 - val_loss: 0.5296 - val_acc: 0.7552\n",
      "Epoch 1391/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4162 - acc: 0.8090 - val_loss: 0.5296 - val_acc: 0.7552\n",
      "Epoch 1392/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4162 - acc: 0.8090 - val_loss: 0.5296 - val_acc: 0.7552\n",
      "Epoch 1393/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4161 - acc: 0.8090 - val_loss: 0.5297 - val_acc: 0.7552\n",
      "Epoch 1394/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4162 - acc: 0.8090 - val_loss: 0.5297 - val_acc: 0.7552\n",
      "Epoch 1395/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4161 - acc: 0.8090 - val_loss: 0.5297 - val_acc: 0.7552\n",
      "Epoch 1396/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4161 - acc: 0.8108 - val_loss: 0.5297 - val_acc: 0.7552\n",
      "Epoch 1397/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4161 - acc: 0.8125 - val_loss: 0.5297 - val_acc: 0.7552\n",
      "Epoch 1398/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4161 - acc: 0.8108 - val_loss: 0.5297 - val_acc: 0.7552\n",
      "Epoch 1399/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4160 - acc: 0.8108 - val_loss: 0.5298 - val_acc: 0.7552\n",
      "Epoch 1400/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4160 - acc: 0.8108 - val_loss: 0.5298 - val_acc: 0.7552\n",
      "Epoch 1401/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4160 - acc: 0.8090 - val_loss: 0.5298 - val_acc: 0.7552\n",
      "Epoch 1402/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4160 - acc: 0.8125 - val_loss: 0.5298 - val_acc: 0.7552\n",
      "Epoch 1403/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4159 - acc: 0.8125 - val_loss: 0.5298 - val_acc: 0.7552\n",
      "Epoch 1404/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4160 - acc: 0.8142 - val_loss: 0.5298 - val_acc: 0.7552\n",
      "Epoch 1405/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4159 - acc: 0.8125 - val_loss: 0.5298 - val_acc: 0.7552\n",
      "Epoch 1406/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4159 - acc: 0.8125 - val_loss: 0.5298 - val_acc: 0.7552\n",
      "Epoch 1407/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4159 - acc: 0.8142 - val_loss: 0.5299 - val_acc: 0.7552\n",
      "Epoch 1408/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4159 - acc: 0.8125 - val_loss: 0.5299 - val_acc: 0.7552\n",
      "Epoch 1409/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4158 - acc: 0.8142 - val_loss: 0.5299 - val_acc: 0.7552\n",
      "Epoch 1410/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4158 - acc: 0.8142 - val_loss: 0.5299 - val_acc: 0.7552\n",
      "Epoch 1411/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4158 - acc: 0.8142 - val_loss: 0.5299 - val_acc: 0.7552\n",
      "Epoch 1412/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4158 - acc: 0.8125 - val_loss: 0.5299 - val_acc: 0.7552\n",
      "Epoch 1413/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4158 - acc: 0.8125 - val_loss: 0.5299 - val_acc: 0.7552\n",
      "Epoch 1414/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4157 - acc: 0.8142 - val_loss: 0.5300 - val_acc: 0.7552\n",
      "Epoch 1415/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4157 - acc: 0.8142 - val_loss: 0.5300 - val_acc: 0.7552\n",
      "Epoch 1416/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4157 - acc: 0.8142 - val_loss: 0.5300 - val_acc: 0.7552\n",
      "Epoch 1417/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4157 - acc: 0.8142 - val_loss: 0.5300 - val_acc: 0.7552\n",
      "Epoch 1418/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4157 - acc: 0.8142 - val_loss: 0.5300 - val_acc: 0.7552\n",
      "Epoch 1419/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4156 - acc: 0.8142 - val_loss: 0.5300 - val_acc: 0.7552\n",
      "Epoch 1420/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4156 - acc: 0.8160 - val_loss: 0.5301 - val_acc: 0.7552\n",
      "Epoch 1421/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4156 - acc: 0.8142 - val_loss: 0.5301 - val_acc: 0.7552\n",
      "Epoch 1422/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4156 - acc: 0.8142 - val_loss: 0.5301 - val_acc: 0.7552\n",
      "Epoch 1423/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4156 - acc: 0.8142 - val_loss: 0.5301 - val_acc: 0.7552\n",
      "Epoch 1424/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4155 - acc: 0.8142 - val_loss: 0.5301 - val_acc: 0.7552\n",
      "Epoch 1425/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4156 - acc: 0.8142 - val_loss: 0.5301 - val_acc: 0.7552\n",
      "Epoch 1426/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4155 - acc: 0.8142 - val_loss: 0.5301 - val_acc: 0.7552\n",
      "Epoch 1427/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4155 - acc: 0.8160 - val_loss: 0.5301 - val_acc: 0.7552\n",
      "Epoch 1428/1500\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.4155 - acc: 0.8142 - val_loss: 0.5301 - val_acc: 0.7552\n",
      "Epoch 1429/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4154 - acc: 0.8142 - val_loss: 0.5301 - val_acc: 0.7552\n",
      "Epoch 1430/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4154 - acc: 0.8142 - val_loss: 0.5301 - val_acc: 0.7552\n",
      "Epoch 1431/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4154 - acc: 0.8142 - val_loss: 0.5301 - val_acc: 0.7552\n",
      "Epoch 1432/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4154 - acc: 0.8142 - val_loss: 0.5302 - val_acc: 0.7552\n",
      "Epoch 1433/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4154 - acc: 0.8125 - val_loss: 0.5302 - val_acc: 0.7552\n",
      "Epoch 1434/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4628 - acc: 0.812 - 0s 28us/step - loss: 0.4154 - acc: 0.8142 - val_loss: 0.5302 - val_acc: 0.7552\n",
      "Epoch 1435/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4153 - acc: 0.8160 - val_loss: 0.5302 - val_acc: 0.7552\n",
      "Epoch 1436/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4152 - acc: 0.8125 - val_loss: 0.5302 - val_acc: 0.7552\n",
      "Epoch 1437/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4153 - acc: 0.8125 - val_loss: 0.5303 - val_acc: 0.7552\n",
      "Epoch 1438/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4152 - acc: 0.8125 - val_loss: 0.5303 - val_acc: 0.7552\n",
      "Epoch 1439/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4152 - acc: 0.8160 - val_loss: 0.5303 - val_acc: 0.7552\n",
      "Epoch 1440/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4151 - acc: 0.8125 - val_loss: 0.5303 - val_acc: 0.7552\n",
      "Epoch 1441/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4152 - acc: 0.8125 - val_loss: 0.5303 - val_acc: 0.7552\n",
      "Epoch 1442/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4151 - acc: 0.8125 - val_loss: 0.5304 - val_acc: 0.7552\n",
      "Epoch 1443/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4151 - acc: 0.8125 - val_loss: 0.5304 - val_acc: 0.7552\n",
      "Epoch 1444/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4151 - acc: 0.8125 - val_loss: 0.5304 - val_acc: 0.7552\n",
      "Epoch 1445/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4150 - acc: 0.8125 - val_loss: 0.5304 - val_acc: 0.7552\n",
      "Epoch 1446/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4151 - acc: 0.8142 - val_loss: 0.5305 - val_acc: 0.7552\n",
      "Epoch 1447/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4150 - acc: 0.8125 - val_loss: 0.5305 - val_acc: 0.7552\n",
      "Epoch 1448/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4150 - acc: 0.8125 - val_loss: 0.5305 - val_acc: 0.7552\n",
      "Epoch 1449/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4150 - acc: 0.8125 - val_loss: 0.5305 - val_acc: 0.7552\n",
      "Epoch 1450/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4149 - acc: 0.8125 - val_loss: 0.5306 - val_acc: 0.7552\n",
      "Epoch 1451/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4149 - acc: 0.8125 - val_loss: 0.5306 - val_acc: 0.7552\n",
      "Epoch 1452/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4149 - acc: 0.8142 - val_loss: 0.5306 - val_acc: 0.7552\n",
      "Epoch 1453/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4149 - acc: 0.8142 - val_loss: 0.5306 - val_acc: 0.7552\n",
      "Epoch 1454/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4149 - acc: 0.8125 - val_loss: 0.5306 - val_acc: 0.7552\n",
      "Epoch 1455/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4149 - acc: 0.8125 - val_loss: 0.5306 - val_acc: 0.7552\n",
      "Epoch 1456/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4148 - acc: 0.8125 - val_loss: 0.5307 - val_acc: 0.7552\n",
      "Epoch 1457/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4148 - acc: 0.8125 - val_loss: 0.5307 - val_acc: 0.7552\n",
      "Epoch 1458/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4148 - acc: 0.8125 - val_loss: 0.5307 - val_acc: 0.7552\n",
      "Epoch 1459/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4147 - acc: 0.8125 - val_loss: 0.5307 - val_acc: 0.7552\n",
      "Epoch 1460/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4147 - acc: 0.8125 - val_loss: 0.5307 - val_acc: 0.7552\n",
      "Epoch 1461/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4147 - acc: 0.8125 - val_loss: 0.5307 - val_acc: 0.7552\n",
      "Epoch 1462/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4146 - acc: 0.8125 - val_loss: 0.5307 - val_acc: 0.7552\n",
      "Epoch 1463/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4147 - acc: 0.8125 - val_loss: 0.5308 - val_acc: 0.7552\n",
      "Epoch 1464/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4146 - acc: 0.8125 - val_loss: 0.5308 - val_acc: 0.7552\n",
      "Epoch 1465/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4146 - acc: 0.8142 - val_loss: 0.5308 - val_acc: 0.7552\n",
      "Epoch 1466/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4146 - acc: 0.8125 - val_loss: 0.5308 - val_acc: 0.7552\n",
      "Epoch 1467/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4146 - acc: 0.8125 - val_loss: 0.5308 - val_acc: 0.7552\n",
      "Epoch 1468/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4145 - acc: 0.8142 - val_loss: 0.5308 - val_acc: 0.7552\n",
      "Epoch 1469/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4145 - acc: 0.8125 - val_loss: 0.5308 - val_acc: 0.7552\n",
      "Epoch 1470/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4145 - acc: 0.8125 - val_loss: 0.5308 - val_acc: 0.7552\n",
      "Epoch 1471/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4145 - acc: 0.8125 - val_loss: 0.5308 - val_acc: 0.7552\n",
      "Epoch 1472/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4145 - acc: 0.8125 - val_loss: 0.5309 - val_acc: 0.7552\n",
      "Epoch 1473/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4144 - acc: 0.8142 - val_loss: 0.5309 - val_acc: 0.7552\n",
      "Epoch 1474/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4145 - acc: 0.8125 - val_loss: 0.5309 - val_acc: 0.7552\n",
      "Epoch 1475/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4144 - acc: 0.8125 - val_loss: 0.5309 - val_acc: 0.7552\n",
      "Epoch 1476/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4143 - acc: 0.8142 - val_loss: 0.5309 - val_acc: 0.7552\n",
      "Epoch 1477/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4143 - acc: 0.8125 - val_loss: 0.5309 - val_acc: 0.7552\n",
      "Epoch 1478/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4144 - acc: 0.8125 - val_loss: 0.5310 - val_acc: 0.7552\n",
      "Epoch 1479/1500\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.4143 - acc: 0.8142 - val_loss: 0.5310 - val_acc: 0.7552\n",
      "Epoch 1480/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4143 - acc: 0.8142 - val_loss: 0.5310 - val_acc: 0.7552\n",
      "Epoch 1481/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4143 - acc: 0.8125 - val_loss: 0.5310 - val_acc: 0.7552\n",
      "Epoch 1482/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4142 - acc: 0.8142 - val_loss: 0.5310 - val_acc: 0.7552\n",
      "Epoch 1483/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4142 - acc: 0.8142 - val_loss: 0.5311 - val_acc: 0.7552\n",
      "Epoch 1484/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4142 - acc: 0.8142 - val_loss: 0.5311 - val_acc: 0.7552\n",
      "Epoch 1485/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4142 - acc: 0.8142 - val_loss: 0.5311 - val_acc: 0.7552\n",
      "Epoch 1486/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4142 - acc: 0.8125 - val_loss: 0.5311 - val_acc: 0.7552\n",
      "Epoch 1487/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4141 - acc: 0.8125 - val_loss: 0.5311 - val_acc: 0.7552\n",
      "Epoch 1488/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 28us/step - loss: 0.4141 - acc: 0.8125 - val_loss: 0.5311 - val_acc: 0.7552\n",
      "Epoch 1489/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4141 - acc: 0.8142 - val_loss: 0.5312 - val_acc: 0.7552\n",
      "Epoch 1490/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4140 - acc: 0.8142 - val_loss: 0.5312 - val_acc: 0.7552\n",
      "Epoch 1491/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4140 - acc: 0.8160 - val_loss: 0.5312 - val_acc: 0.7552\n",
      "Epoch 1492/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4140 - acc: 0.8142 - val_loss: 0.5312 - val_acc: 0.7552\n",
      "Epoch 1493/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4140 - acc: 0.8160 - val_loss: 0.5312 - val_acc: 0.7552\n",
      "Epoch 1494/1500\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.4140 - acc: 0.8142 - val_loss: 0.5312 - val_acc: 0.7552\n",
      "Epoch 1495/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4139 - acc: 0.8160 - val_loss: 0.5312 - val_acc: 0.7552\n",
      "Epoch 1496/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4139 - acc: 0.8142 - val_loss: 0.5313 - val_acc: 0.7552\n",
      "Epoch 1497/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4139 - acc: 0.8142 - val_loss: 0.5313 - val_acc: 0.7552\n",
      "Epoch 1498/1500\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.4139 - acc: 0.8125 - val_loss: 0.5313 - val_acc: 0.7552\n",
      "Epoch 1499/1500\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.4139 - acc: 0.8125 - val_loss: 0.5313 - val_acc: 0.7552\n",
      "Epoch 1500/1500\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.4138 - acc: 0.8142 - val_loss: 0.5313 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "model_2.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_2 = model_2.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1500);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class_nn_2 = model_2.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_2 = model_2.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.750\n",
      "roc-auc is 0.809\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_2)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_2)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_2, 'NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x285e37d18d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_2.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_2.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
