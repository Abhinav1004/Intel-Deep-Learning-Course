{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skipgrams in Keras\n",
    "\n",
    "- In this lecture, we will implement Skipgrams in `Keras`.\n",
    "\n",
    "#### Loading in and preprocessing data\n",
    "- Load the Alice in Wonderland data in Corpus using Keras utility\n",
    "- `Keras` has some nice text preprocessing features too!\n",
    "- Split the text into sentences.\n",
    "- Use `Keras`' `Tokenizer` to tokenize sentences into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "init_cell": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# Basics\n",
    "from __future__ import print_function, division\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import SVG\n",
    "from scipy.spatial.distance import cosine\n",
    "%matplotlib inline\n",
    "\n",
    "import gensim\n",
    "import os\n",
    "# nltk\n",
    "from nltk import sent_tokenize\n",
    "# Compile the Keras Model\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# keras\n",
    "import keras.backend\n",
    "np.random.seed(13)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Activation,Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot \n",
    "from keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We'll use Alice in Wonderland\n",
    "path = os.path.join(os.getcwd(),\"Alice_in_Wonderland.txt\")\n",
    "corpus = open(path,encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split document into sentences first\n",
    "corpus = corpus[corpus.index('\\n\\n')+2:]  #remove header.\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(corpus) #has some punctuation symbols to be removed in the next step\n",
    "#print(sentences)\n",
    "base_filter='!\"#$%&()*+,-./:;`<=>?@[\\\\]^_{|}~\\t\\n' + \"'\" \n",
    "tokenizer = Tokenizer(filters=base_filter)\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1744 1744\n",
      "163738\n"
     ]
    }
   ],
   "source": [
    "# Convert tokenized sentences to sequence format\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "print(len(sequences), tokenizer.document_count)\n",
    "print(nb_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice guessed in a\n",
      "moment that it was looking for the fan and the pair of white kid gloves,\n",
      "and she very good-naturedly began hunting about for them, but they were\n",
      "nowhere to be seen--everything seemed to have changed since her swim in\n",
      "the pool, and the great hall, with the glass table and the little door,\n",
      "had vanished completely.\n",
      "[12, 1065, 11, 4, 155, 14, 6, 13, 153, 24, 1, 400, 2, 1, 700, 5, 159, 703, 373, 2, 7, 32, 181, 1987, 85, 1066, 42, 24, 49, 26, 30, 54, 1363, 3, 27, 265, 294, 179, 3, 52, 482, 848, 16, 734, 11, 1, 372, 2, 1, 137, 428, 17, 1, 391, 231, 2, 1, 35, 162, 25, 849, 1988]\n"
     ]
    }
   ],
   "source": [
    "# To understand what is happening;\n",
    "\n",
    "print(sentences[324])  # this is a sentence\n",
    "print(sequences[324])  # this is the same sentence where words are encoded as numbers.\n",
    "# print(list(tokenizer.word_index[word.lower().replace('.', '')] for word in sentences[324].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Skipgrams: Generating Input and Output Labels\n",
    "- Now that we have sentences, and word tokenization, we are in good position to create our training set for skipgrams.\n",
    "- Now we need to generate our `X_train` and `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was looking\n",
      "was that\n",
      "was for\n",
      "was it\n"
     ]
    }
   ],
   "source": [
    "# Let's first see how Keras' skipgrams function works.\n",
    "\n",
    "couples, labels = skipgrams(sequences[324], len(tokenizer.word_index) + 1,\n",
    "    window_size=2, negative_samples=0, shuffle=True,\n",
    "    categorical=False, sampling_table=None)\n",
    "\n",
    "# print(couples,labels)\n",
    "index_2_word = {val: key for key, val in tokenizer.word_index.items()}\n",
    "\n",
    "for w1, w2 in couples:\n",
    "    if w1 == 13:\n",
    "        print(index_2_word[w1], index_2_word[w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function to generate the inputs and outputs for all windows\n",
    "\n",
    "# Vocab size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# Dimension to reduce to\n",
    "dim = 100\n",
    "window_size = 2\n",
    "\n",
    "\n",
    "def generate_data(sequences, window_size, vocab_size):\n",
    "    for seq in sequences:\n",
    "        X, y = [], []\n",
    "        couples, _ = skipgrams(\n",
    "            seq, vocab_size,\n",
    "            window_size=window_size, negative_samples=0, shuffle=True,\n",
    "            categorical=False, sampling_table=None)\n",
    "        if not couples:\n",
    "            continue\n",
    "        for in_word, out_word in couples:\n",
    "            X.append(in_word)\n",
    "            y.append(np_utils.to_categorical(out_word, vocab_size))\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        X = X.reshape(len(X), 1)\n",
    "        y = y.reshape(len(X), vocab_size)\n",
    "        yield X, y\n",
    "        \n",
    "data_generator = generate_data(sequences, window_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skipgrams: Creating the Model\n",
    "- Lastly, we create the (shallow) network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"405pt\" viewBox=\"0.00 0.00 325.00 304.00\" width=\"433pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1.3333 1.3333) rotate(0) translate(4 300)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-300 321,-300 321,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 139661745969360 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>139661745969360</title>\n",
       "<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 317,-295.5 317,-249.5 0,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"96.5\" y=\"-268.8\">embedding_1_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"193,-249.5 193,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"193,-272.5 248,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"248,-249.5 248,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"282.5\" y=\"-280.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"248,-272.5 317,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"282.5\" y=\"-257.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 139662614170592 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>139662614170592</title>\n",
       "<polygon fill=\"none\" points=\"2,-166.5 2,-212.5 315,-212.5 315,-166.5 2,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82.5\" y=\"-185.8\">embedding_1: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"163,-166.5 163,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"190.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"163,-189.5 218,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"190.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"218,-166.5 218,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266.5\" y=\"-197.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"218,-189.5 315,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266.5\" y=\"-174.3\">(None, 1, 100)</text>\n",
       "</g>\n",
       "<!-- 139661745969360&#45;&gt;139662614170592 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>139661745969360-&gt;139662614170592</title>\n",
       "<path d=\"M158.5,-249.3799C158.5,-241.1745 158.5,-231.7679 158.5,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"162.0001,-222.784 158.5,-212.784 155.0001,-222.784 162.0001,-222.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139662614169080 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>139662614169080</title>\n",
       "<polygon fill=\"none\" points=\"20,-83.5 20,-129.5 297,-129.5 297,-83.5 20,-83.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82.5\" y=\"-102.8\">reshape_1: Reshape</text>\n",
       "<polyline fill=\"none\" points=\"145,-83.5 145,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"172.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"145,-106.5 200,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"172.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"200,-83.5 200,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"248.5\" y=\"-114.3\">(None, 1, 100)</text>\n",
       "<polyline fill=\"none\" points=\"200,-106.5 297,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"248.5\" y=\"-91.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 139662614170592&#45;&gt;139662614169080 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>139662614170592-&gt;139662614169080</title>\n",
       "<path d=\"M158.5,-166.3799C158.5,-158.1745 158.5,-148.7679 158.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"162.0001,-139.784 158.5,-129.784 155.0001,-139.784 162.0001,-139.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139661480910520 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>139661480910520</title>\n",
       "<polygon fill=\"none\" points=\"35.5,-.5 35.5,-46.5 281.5,-46.5 281.5,-.5 35.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"86.5\" y=\"-19.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"137.5,-.5 137.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"137.5,-23.5 192.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"192.5,-.5 192.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"237\" y=\"-31.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"192.5,-23.5 281.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"237\" y=\"-8.3\">(None, 3043)</text>\n",
       "</g>\n",
       "<!-- 139662614169080&#45;&gt;139661480910520 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>139662614169080-&gt;139661480910520</title>\n",
       "<path d=\"M158.5,-83.3799C158.5,-75.1745 158.5,-65.7679 158.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"162.0001,-56.784 158.5,-46.784 155.0001,-56.784 162.0001,-56.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Keras model and view it \n",
    "skipgram = Sequential()\n",
    "skipgram.add(Embedding(input_dim=vocab_size, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram.add(Reshape((dim,)))\n",
    "skipgram.add(Dense(input_dim=dim, units=vocab_size, activation='softmax'))\n",
    "SVG(model_to_dot(skipgram, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skipgrams: Compiling and Training\n",
    "- Time to compile and train\n",
    "- We use crossentropy, common loss for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/amazon/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "iteration 0, loss is 13123.474479198456\n",
      "iteration 1, loss is 11779.688267469406\n",
      "iteration 2, loss is 11157.769812107086\n",
      "iteration 3, loss is 10855.440164089203\n",
      "iteration 4, loss is 10682.516449809074\n",
      "iteration 5, loss is 10564.241443514824\n",
      "iteration 6, loss is 10472.500138640404\n",
      "iteration 7, loss is 10396.319508552551\n",
      "iteration 8, loss is 10330.800001263618\n",
      "iteration 9, loss is 10272.768285870552\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sgd = SGD(lr=1e-4, decay=1e-6, momentum=0.9)\n",
    "\n",
    "skipgram.compile(loss='categorical_crossentropy', optimizer=\"adadelta\")\n",
    "\n",
    "# Fit the Skipgrams\n",
    "for iteration in range(10):\n",
    "    loss = 0\n",
    "    for x, y in generate_data(sequences, window_size, vocab_size):\n",
    "        loss += skipgram.train_on_batch(x, y)\n",
    "    print('iteration {}, loss is {}'.format(iteration, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgrams: Looking at the vectors\n",
    "\n",
    "To get word_vectors now, we look at the weights of the first layer.\n",
    "\n",
    "Let's also write functions giving us similarity of two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9377803802490234\n",
      "\n",
      "gryphon        0.957120\n",
      "duchess        0.943000\n",
      "king           0.937780\n",
      "dormouse       0.933557\n",
      "hatter         0.923912\n",
      "caterpillar    0.915497\n",
      "first          0.905976\n",
      "cat            0.902722\n",
      "white          0.900410\n",
      "march          0.900139\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "word_vectors = skipgram.get_weights()[0]\n",
    "\n",
    "def get_dist(w1, w2):\n",
    "    i1, i2 = tokenizer.word_index[w1], tokenizer.word_index[w2]\n",
    "    v1, v2 = word_vectors[i1], word_vectors[i2]\n",
    "    return cosine(v1, v2)\n",
    "\n",
    "def get_similarity(w1, w2):\n",
    "    return 1-get_dist(w1, w2)\n",
    "\n",
    "def get_most_similar(w1, n=10):\n",
    "    sims = {word: get_similarity(w1, word) \n",
    "            for word in tokenizer.word_index.keys()\n",
    "            if word != w1}\n",
    "    sims = pd.Series(sims)\n",
    "    sims.sort_values(inplace=True, ascending=False)\n",
    "    return sims.iloc[:n]\n",
    "\n",
    "\n",
    "print(get_similarity('king', 'queen'))\n",
    "print('')\n",
    "print(get_most_similar('queen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=dim, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: keras.backend.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax')) #first input is (vocab_size,1) instead of skip gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 0.0\n",
      "9 0.0\n"
     ]
    }
   ],
   "source": [
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data(corpus, window_size, vocab_size):\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = cbow.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(os.getcwd(),\"vectors.txt\"),binary=True,unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amazon/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py:2372: RuntimeWarning: overflow encountered in square\n",
      "  dist = sqrt((m ** 2).sum(-1))[..., newaxis]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('-0.044734515', 0.6432422399520874),\n",
       " ('843541', 0.6233307719230652),\n",
       " ('94475', 0.5990298986434937),\n",
       " ('78', 0.5961076617240906),\n",
       " ('0.016130604', 0.5802446603775024),\n",
       " ('5223', 0.5669052600860596),\n",
       " ('8586', 0.5543237328529358),\n",
       " ('127', 0.5524085760116577),\n",
       " ('045603838', 0.5436569452285767),\n",
       " ('2206', 0.5420832633972168)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['the'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "livereveal": {
   "height": "100%",
   "margin": 0,
   "maxScale": 1,
   "minScale": 1,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky",
   "transition": "zoom",
   "width": "100%"
  },
  "toc": {
   "nav_menu": {
    "height": "369px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "457px",
    "left": "0px",
    "right": "968px",
    "top": "130px",
    "width": "214px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
