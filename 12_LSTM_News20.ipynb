{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Using LSTMs to Classify the 20 Newsgroups Data Set\n",
    "The 20 Newsgroups data set is a well known classification problem. The goal is to classify which newsgroup a particular post came from.  The 20 possible groups are:\n",
    "\n",
    "`comp.graphics\n",
    "comp.os.ms-windows.misc\n",
    "comp.sys.ibm.pc.hardware\n",
    "comp.sys.mac.hardware\n",
    "comp.windows.x\trec.autos\n",
    "rec.motorcycles\n",
    "rec.sport.baseball\n",
    "rec.sport.hockey\t\n",
    "sci.crypt\n",
    "sci.electronics\n",
    "sci.med\n",
    "sci.space\n",
    "misc.forsale\t\n",
    "talk.politics.misc\n",
    "talk.politics.guns\n",
    "talk.politics.mideast\t\n",
    "talk.religion.misc\n",
    "alt.atheism\n",
    "soc.religion.christian`\n",
    "\n",
    "As you can see, some pairs of groups may be quite similar while others are very different.\n",
    "\n",
    "The data is given as a designated training set of size 11314 and test set of size 7532.  The 20 categories are represented in roughly equal proportions, so the baseline accuracy is around 5%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, review the code below.  This will walk you through the basics of loading in the 20 newsgroups data, loading in the GloVe data, building the word embedding matrix, and building the LSTM model.\n",
    "\n",
    "After we build the first LSTM model, it will be your turn to build one and play with the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import keras\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "seq_length = 30  # How long to make our word sequences\n",
    "batch_size = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the 20 newsgroups data - there is already a designated \"train\" and \"test\" set\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 7532)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroups_train.data), len(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(newsgroups_train.data)\n",
    "sequences_test = tokenizer.texts_to_sequences(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 134142 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(sequences_train, maxlen=seq_length)\n",
    "x_test = pad_sequences(sequences_test, maxlen=seq_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2908,   198,     3, ...,    35,    58,  7860],\n",
       "       [  351,   138,   533, ...,   118,   441,    15],\n",
       "       [    9,    33,     4, ...,   187,    84, 17015],\n",
       "       ...,\n",
       "       [   10,     1,  1787, ...,   349,   383,    31],\n",
       "       [  115,   362,    67, ...,  7772,   486,   492],\n",
       "       [ 4485, 13919,  1031, ...,   200,    38,  3826]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(np.asarray(newsgroups_train.target))\n",
    "y_test = keras.utils.to_categorical(np.asarray(newsgroups_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the Glove pre-trained word vectors.  If you haven't already, please download them using this link:\n",
    "(NOTE: this will start downloading an 822MB file)\n",
    "\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "Then unzip the file and fill your local path to the file in the code cell below.\n",
    "\n",
    "We will use the file `glove.6B.100d.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(os.getcwd(),\"glove.6B.100d.txt\"))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just look at a word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30817  ,  0.30938  ,  0.52803  , -0.92543  , -0.73671  ,\n",
       "        0.63475  ,  0.44197  ,  0.10262  , -0.09142  , -0.56607  ,\n",
       "       -0.5327   ,  0.2013   ,  0.7704   , -0.13983  ,  0.13727  ,\n",
       "        1.1128   ,  0.89301  , -0.17869  , -0.0019722,  0.57289  ,\n",
       "        0.59479  ,  0.50428  , -0.28991  , -1.3491   ,  0.42756  ,\n",
       "        1.2748   , -1.1613   , -0.41084  ,  0.042804 ,  0.54866  ,\n",
       "        0.18897  ,  0.3759   ,  0.58035  ,  0.66975  ,  0.81156  ,\n",
       "        0.93864  , -0.51005  , -0.070079 ,  0.82819  , -0.35346  ,\n",
       "        0.21086  , -0.24412  , -0.16554  , -0.78358  , -0.48482  ,\n",
       "        0.38968  , -0.86356  , -0.016391 ,  0.31984  , -0.49246  ,\n",
       "       -0.069363 ,  0.018869 , -0.098286 ,  1.3126   , -0.12116  ,\n",
       "       -1.2399   , -0.091429 ,  0.35294  ,  0.64645  ,  0.089642 ,\n",
       "        0.70294  ,  1.1244   ,  0.38639  ,  0.52084  ,  0.98787  ,\n",
       "        0.79952  , -0.34625  ,  0.14095  ,  0.80167  ,  0.20987  ,\n",
       "       -0.86007  , -0.15308  ,  0.074523 ,  0.40816  ,  0.019208 ,\n",
       "        0.51587  , -0.34428  , -0.24525  , -0.77984  ,  0.27425  ,\n",
       "        0.22418  ,  0.20164  ,  0.017431 , -0.014697 , -1.0235   ,\n",
       "       -0.39695  , -0.0056188,  0.30569  ,  0.31748  ,  0.021404 ,\n",
       "        0.11837  , -0.11319  ,  0.42456  ,  0.53405  , -0.16717  ,\n",
       "       -0.27185  , -0.6255   ,  0.12883  ,  0.62529  , -0.52086  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_vec = embeddings_index['dog']\n",
    "dog_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This creates a matrix where the $i$th row gives the word embedding for the word represented by integer $i$.\n",
    "## Essentially, these will be the \"weights\" for the Embedding Layer\n",
    "## Rather than learning the weights, we will use these ones and \"freeze\" the layer\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134143, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Layer\n",
    "`keras.layers.recurrent.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)`\n",
    "\n",
    "- Similar in structure to the `SimpleRNN` layer\n",
    "- `units` defines the dimension of the recurrent state\n",
    "- `recurrent_...` refers the recurrent state aspects of the LSTM\n",
    "- `kernel_...` refers to the transformations done on the input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 100)           13414300  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 30)                15720     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                620       \n",
      "=================================================================\n",
      "Total params: 13,430,640\n",
      "Trainable params: 16,340\n",
      "Non-trainable params: 13,414,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_dimension = 100  # This is the dimension of the words we are using from GloVe\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                            word_dimension,  \n",
    "                            weights=[embedding_matrix],  # We set the weights to be the word vectors from GloVe\n",
    "                            input_length=seq_length,\n",
    "                            trainable=False))  # By setting trainable to False, we \"freeze\" the word embeddings.\n",
    "model.add(LSTM(30, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(lr = .002)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/amazon/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/20\n",
      "11314/11314 [==============================] - 9s 804us/step - loss: 2.7668 - accuracy: 0.1380 - val_loss: 2.5094 - val_accuracy: 0.2035\n",
      "Epoch 2/20\n",
      "11314/11314 [==============================] - 6s 567us/step - loss: 2.4140 - accuracy: 0.2504 - val_loss: 2.2940 - val_accuracy: 0.2839\n",
      "Epoch 3/20\n",
      "11314/11314 [==============================] - 7s 576us/step - loss: 2.1951 - accuracy: 0.3259 - val_loss: 2.0765 - val_accuracy: 0.3671\n",
      "Epoch 4/20\n",
      "11314/11314 [==============================] - 7s 582us/step - loss: 2.0452 - accuracy: 0.3694 - val_loss: 1.9883 - val_accuracy: 0.3867\n",
      "Epoch 5/20\n",
      "11314/11314 [==============================] - 7s 593us/step - loss: 1.9449 - accuracy: 0.4022 - val_loss: 1.9262 - val_accuracy: 0.4080\n",
      "Epoch 6/20\n",
      "11314/11314 [==============================] - 7s 580us/step - loss: 1.8535 - accuracy: 0.4304 - val_loss: 1.8756 - val_accuracy: 0.4239\n",
      "Epoch 7/20\n",
      "11314/11314 [==============================] - 6s 566us/step - loss: 1.8005 - accuracy: 0.4421 - val_loss: 1.8404 - val_accuracy: 0.4375\n",
      "Epoch 8/20\n",
      "11314/11314 [==============================] - 6s 573us/step - loss: 1.7471 - accuracy: 0.4613 - val_loss: 1.8153 - val_accuracy: 0.4442\n",
      "Epoch 9/20\n",
      "11314/11314 [==============================] - 7s 581us/step - loss: 1.7096 - accuracy: 0.4715 - val_loss: 1.8021 - val_accuracy: 0.4530\n",
      "Epoch 10/20\n",
      "11314/11314 [==============================] - 7s 578us/step - loss: 1.6633 - accuracy: 0.4909 - val_loss: 1.7915 - val_accuracy: 0.4557\n",
      "Epoch 11/20\n",
      "11314/11314 [==============================] - 6s 569us/step - loss: 1.6368 - accuracy: 0.4952 - val_loss: 1.7900 - val_accuracy: 0.4558\n",
      "Epoch 12/20\n",
      "11314/11314 [==============================] - 7s 608us/step - loss: 1.6140 - accuracy: 0.5067 - val_loss: 1.7677 - val_accuracy: 0.4672\n",
      "Epoch 13/20\n",
      "11314/11314 [==============================] - 6s 573us/step - loss: 1.5786 - accuracy: 0.5107 - val_loss: 1.7752 - val_accuracy: 0.4676\n",
      "Epoch 14/20\n",
      "11314/11314 [==============================] - 7s 598us/step - loss: 1.5518 - accuracy: 0.5228 - val_loss: 1.7573 - val_accuracy: 0.4774\n",
      "Epoch 15/20\n",
      "11314/11314 [==============================] - 6s 543us/step - loss: 1.5363 - accuracy: 0.5241 - val_loss: 1.7602 - val_accuracy: 0.4789\n",
      "Epoch 16/20\n",
      "11314/11314 [==============================] - 7s 595us/step - loss: 1.5326 - accuracy: 0.5240 - val_loss: 1.7496 - val_accuracy: 0.4773\n",
      "Epoch 17/20\n",
      "11314/11314 [==============================] - 7s 630us/step - loss: 1.5123 - accuracy: 0.5324 - val_loss: 1.7415 - val_accuracy: 0.4843\n",
      "Epoch 18/20\n",
      "11314/11314 [==============================] - 7s 587us/step - loss: 1.4878 - accuracy: 0.5410 - val_loss: 1.7587 - val_accuracy: 0.4778\n",
      "Epoch 19/20\n",
      "11314/11314 [==============================] - 7s 580us/step - loss: 1.4784 - accuracy: 0.5448 - val_loss: 1.7458 - val_accuracy: 0.4867\n",
      "Epoch 20/20\n",
      "11314/11314 [==============================] - 7s 641us/step - loss: 1.4613 - accuracy: 0.5473 - val_loss: 1.7499 - val_accuracy: 0.4882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f90a96da400>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/20\n",
      "11314/11314 [==============================] - 6s 560us/step - loss: 1.4524 - accuracy: 0.5529 - val_loss: 1.7413 - val_accuracy: 0.4908\n",
      "Epoch 2/20\n",
      "11314/11314 [==============================] - 7s 582us/step - loss: 1.4439 - accuracy: 0.5547 - val_loss: 1.7575 - val_accuracy: 0.4839\n",
      "Epoch 3/20\n",
      "11314/11314 [==============================] - 7s 638us/step - loss: 1.4361 - accuracy: 0.5521 - val_loss: 1.7395 - val_accuracy: 0.4906\n",
      "Epoch 4/20\n",
      "11314/11314 [==============================] - 6s 566us/step - loss: 1.4307 - accuracy: 0.5594 - val_loss: 1.7509 - val_accuracy: 0.4906\n",
      "Epoch 5/20\n",
      "11314/11314 [==============================] - 6s 557us/step - loss: 1.4191 - accuracy: 0.5602 - val_loss: 1.7302 - val_accuracy: 0.4943\n",
      "Epoch 6/20\n",
      "11314/11314 [==============================] - 7s 589us/step - loss: 1.4074 - accuracy: 0.5671 - val_loss: 1.7460 - val_accuracy: 0.4888\n",
      "Epoch 7/20\n",
      "11314/11314 [==============================] - 7s 623us/step - loss: 1.3926 - accuracy: 0.5658 - val_loss: 1.7484 - val_accuracy: 0.4898\n",
      "Epoch 8/20\n",
      "11314/11314 [==============================] - 6s 567us/step - loss: 1.3883 - accuracy: 0.5698 - val_loss: 1.7519 - val_accuracy: 0.4902\n",
      "Epoch 9/20\n",
      "11314/11314 [==============================] - 6s 569us/step - loss: 1.3942 - accuracy: 0.5699 - val_loss: 1.7474 - val_accuracy: 0.4914\n",
      "Epoch 10/20\n",
      "11314/11314 [==============================] - 7s 583us/step - loss: 1.3836 - accuracy: 0.5674 - val_loss: 1.7610 - val_accuracy: 0.4908\n",
      "Epoch 11/20\n",
      "11314/11314 [==============================] - 6s 568us/step - loss: 1.3846 - accuracy: 0.5712 - val_loss: 1.7450 - val_accuracy: 0.4951\n",
      "Epoch 12/20\n",
      "11314/11314 [==============================] - 6s 555us/step - loss: 1.3671 - accuracy: 0.5765 - val_loss: 1.7564 - val_accuracy: 0.4942\n",
      "Epoch 13/20\n",
      "11314/11314 [==============================] - 6s 541us/step - loss: 1.3629 - accuracy: 0.5823 - val_loss: 1.7573 - val_accuracy: 0.4973\n",
      "Epoch 14/20\n",
      "11314/11314 [==============================] - 6s 554us/step - loss: 1.3636 - accuracy: 0.5784 - val_loss: 1.7667 - val_accuracy: 0.4863\n",
      "Epoch 15/20\n",
      "11314/11314 [==============================] - 7s 577us/step - loss: 1.3550 - accuracy: 0.5804 - val_loss: 1.7708 - val_accuracy: 0.4948\n",
      "Epoch 16/20\n",
      "11314/11314 [==============================] - 6s 560us/step - loss: 1.3597 - accuracy: 0.5764 - val_loss: 1.7614 - val_accuracy: 0.4938\n",
      "Epoch 17/20\n",
      "11314/11314 [==============================] - 7s 584us/step - loss: 1.3439 - accuracy: 0.5838 - val_loss: 1.7698 - val_accuracy: 0.4938\n",
      "Epoch 18/20\n",
      "11314/11314 [==============================] - 6s 561us/step - loss: 1.3412 - accuracy: 0.5887 - val_loss: 1.7714 - val_accuracy: 0.4902\n",
      "Epoch 19/20\n",
      "11314/11314 [==============================] - 6s 552us/step - loss: 1.3493 - accuracy: 0.5800 - val_loss: 1.7712 - val_accuracy: 0.4922\n",
      "Epoch 20/20\n",
      "11314/11314 [==============================] - 6s 549us/step - loss: 1.3317 - accuracy: 0.5845 - val_loss: 1.7611 - val_accuracy: 0.4975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f90a90ec588>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise\n",
    "### Your Turn\n",
    "- Build a neural network with a SimpleRNN instead of an LSTM (with other dimensions and parameters the same). How does the performance compare?\n",
    "- Use the LSTM above without the pretrained word vectors (randomly initialize the weights and have them be learned during the training process).  How does the performance compare?\n",
    "- Try different sequence lengths, and dimensions for the hidden state of the LSTM.  Can you improve the model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please provide your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
